{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca34831",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ0ï¼šè³‡æ–™é è™•ç†èˆ‡é‡æ¡æ¨£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c95a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a05004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š æ­¥éª¤1: è¯»å–åˆ†é’Ÿæ•°æ®\n",
      "============================================================\n",
      "åˆ†é’Ÿæ•°æ®æ€»ç¬”æ•°: 3,463,681\n",
      "æ•°æ®æœŸé—´: 2015-11-01 23:00:00 è‡³ 2025-11-11 02:35:00\n",
      "æœ‰æ•ˆæ•°æ®ç¬”æ•°: 3,463,681\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥éª¤2: è®¡ç®— Dollar Bar (threshold=10,000,000)\n",
      "============================================================\n",
      "\n",
      "Dollar Bar ç»Ÿè®¡:\n",
      "  Dollar Bar æ•°é‡: 189,434\n",
      "  æœŸé—´: 2015-11-03 10:04:00 è‡³ 2025-11-11 02:28:00\n",
      "  å¹³å‡æ¯æ ¹ bar çš„ dollar value: 114,316,430\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥éª¤3: è¯»å–15åˆ†é’Ÿæ•°æ®\n",
      "============================================================\n",
      "15åˆ†é’Ÿæ•°æ®æ€»ç¬”æ•°: 386,924\n",
      "æ•°æ®æœŸé—´: 2008-08-19 09:30:00 è‡³ 2025-09-12 05:45:00\n",
      "ç»˜å›¾æœŸé—´: 2025-10-12 è‡³ 2025-11-11\n",
      "15åˆ†é’Ÿæ•°æ®: 0 ç¬”\n",
      "Dollar Bar: 6,345 æ ¹\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 307\u001b[0m\n\u001b[0;32m    301\u001b[0m dollar_bar_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m db_time \u001b[38;5;129;01min\u001b[39;00m dollar_bars_plot\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;66;03m# æ‰¾åˆ°æœ€æ¥è¿‘çš„15åˆ†é’Ÿbarä½ç½®\u001b[39;00m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;66;03m# å¦‚æœdollar baræ—¶é—´åœ¨15åˆ†é’Ÿbarä¹‹å‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª15åˆ†é’Ÿbarçš„ä½ç½®\u001b[39;00m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;66;03m# å¦‚æœdollar baræ—¶é—´åœ¨15åˆ†é’Ÿbarä¹‹åï¼Œä½¿ç”¨æœ€åä¸€ä¸ª15åˆ†é’Ÿbarçš„ä½ç½®\u001b[39;00m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;66;03m# å¦åˆ™ä½¿ç”¨æœ€æ¥è¿‘çš„15åˆ†é’Ÿbarä½ç½®\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m db_time \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnas100_15min_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m    308\u001b[0m         pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m db_time \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m nas100_15min_plot\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5320\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5318\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5319\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key, warn_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 5320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   5323\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5324\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[0;32m   5325\u001b[0m     result \u001b[38;5;241m=\u001b[39m getitem(key)\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:358\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03mThis getitem defers to the underlying array, which by-definition can\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03monly handle list-likes, slices, and integer scalars\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001b[39;00m\n\u001b[0;32m    357\u001b[0m result \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnion[DatetimeLikeArrayT, DTScalarOrNaT]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m )\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_scalar(result):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:289\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDArrayBackedExtensionArrayT,\n\u001b[0;32m    285\u001b[0m     key: PositionalIndexer2D,\n\u001b[0;32m    286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDArrayBackedExtensionArrayT \u001b[38;5;241m|\u001b[39m Any:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_integer(key):\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;66;03m# fast-path\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    291\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_box_func(result)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# è‡ªå·±å®ç° get_dollar_bars\n",
    "# =====================================================\n",
    "def get_dollar_bars_custom(df, threshold=10_000_000):\n",
    "    \"\"\"\n",
    "    è‡ªå·±å®ç°çš„ dollar bar è®¡ç®—å‡½æ•°\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        è¾“å…¥æ•°æ®ï¼Œå¿…é¡»åŒ…å«åˆ—: date_time, price, volume\n",
    "    threshold : float\n",
    "        Dollar bar çš„é˜ˆå€¼ï¼ˆç´¯ç§¯ç¾å…ƒä»·å€¼ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dollar barsï¼ŒåŒ…å«åˆ—: date_time, open, high, low, close, volume, \n",
    "        cum_buy_volume, cum_ticks, cum_dollar_value\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–å˜é‡\n",
    "    list_bars = []\n",
    "    \n",
    "    # å½“å‰ bar çš„çŠ¶æ€\n",
    "    open_price = None\n",
    "    high_price = -np.inf\n",
    "    low_price = np.inf\n",
    "    close_price = None\n",
    "    \n",
    "    # ç´¯ç§¯ç»Ÿè®¡\n",
    "    cum_ticks = 0\n",
    "    cum_dollar_value = 0\n",
    "    cum_volume = 0\n",
    "    cum_buy_volume = 0\n",
    "    \n",
    "    # Tick rule ç›¸å…³ï¼ˆç”¨äºåˆ¤æ–­ä¹°å–æ–¹å‘ï¼‰\n",
    "    prev_price = None\n",
    "    prev_tick_rule = 0\n",
    "    tick_num = 0\n",
    "    \n",
    "    # éå†æ¯ä¸€è¡Œæ•°æ®\n",
    "    for idx, row in df.iterrows():\n",
    "        date_time = row['date_time']\n",
    "        price = float(row['price'])\n",
    "        volume = float(row['volume'])\n",
    "        \n",
    "        # è·³è¿‡æ— æ•ˆæ•°æ®\n",
    "        if pd.isna(price) or pd.isna(volume) or volume <= 0:\n",
    "            continue\n",
    "        \n",
    "        tick_num += 1\n",
    "        dollar_value = price * volume\n",
    "        \n",
    "        # åˆå§‹åŒ– open_price\n",
    "        if open_price is None:\n",
    "            open_price = price\n",
    "        \n",
    "        # æ›´æ–° high å’Œ low\n",
    "        high_price = max(high_price, price)\n",
    "        low_price = min(low_price, price)\n",
    "        \n",
    "        # åº”ç”¨ tick rule åˆ¤æ–­ä¹°å–æ–¹å‘\n",
    "        if prev_price is not None:\n",
    "            tick_diff = price - prev_price\n",
    "            if tick_diff != 0:\n",
    "                signed_tick = np.sign(tick_diff)\n",
    "                prev_tick_rule = signed_tick\n",
    "            else:\n",
    "                signed_tick = prev_tick_rule\n",
    "        else:\n",
    "            signed_tick = 0\n",
    "        \n",
    "        prev_price = price\n",
    "        \n",
    "        # ç´¯ç§¯ç»Ÿè®¡\n",
    "        cum_ticks += 1\n",
    "        cum_dollar_value += dollar_value\n",
    "        cum_volume += volume\n",
    "        if signed_tick == 1:  # ä¸Šæ¶¨ï¼Œè§†ä¸ºä¹°å…¥\n",
    "            cum_buy_volume += volume\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼\n",
    "        if cum_dollar_value >= threshold:\n",
    "            # ç¡®ä¿ high å’Œ low åŒ…å« open\n",
    "            high_price = max(high_price, open_price)\n",
    "            low_price = min(low_price, open_price)\n",
    "            close_price = price\n",
    "            \n",
    "            # åˆ›å»º bar\n",
    "            list_bars.append([\n",
    "                date_time,\n",
    "                tick_num,\n",
    "                open_price,\n",
    "                high_price,\n",
    "                low_price,\n",
    "                close_price,\n",
    "                cum_volume,\n",
    "                cum_buy_volume,\n",
    "                cum_ticks,\n",
    "                cum_dollar_value\n",
    "            ])\n",
    "            \n",
    "            # é‡ç½®çŠ¶æ€\n",
    "            open_price = None\n",
    "            high_price = -np.inf\n",
    "            low_price = np.inf\n",
    "            cum_ticks = 0\n",
    "            cum_dollar_value = 0\n",
    "            cum_volume = 0\n",
    "            cum_buy_volume = 0\n",
    "    \n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    if list_bars:\n",
    "        bars_df = pd.DataFrame(\n",
    "            list_bars,\n",
    "            columns=['date_time', 'tick_num', 'open', 'high', 'low', 'close', \n",
    "                     'volume', 'cum_buy_volume', 'cum_ticks', 'cum_dollar_value']\n",
    "        )\n",
    "        bars_df['date_time'] = pd.to_datetime(bars_df['date_time'])\n",
    "        bars_df.set_index('date_time', inplace=True)\n",
    "        return bars_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# =====================================================\n",
    "# 1. è¯»å–åˆ†é’Ÿæ•°æ®\n",
    "# =====================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š æ­¥éª¤1: è¯»å–åˆ†é’Ÿæ•°æ®\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¯»å–åˆ†é’Ÿæ•°æ®\n",
    "minute_df = pd.read_csv('minute_prices/NAS100.csv')\n",
    "minute_df['Date'] = pd.to_datetime(minute_df['Date'])\n",
    "minute_df.set_index('Date', inplace=True)\n",
    "\n",
    "print(f\"åˆ†é’Ÿæ•°æ®æ€»ç¬”æ•°: {len(minute_df):,}\")\n",
    "print(f\"æ•°æ®æœŸé—´: {minute_df.index.min()} è‡³ {minute_df.index.max()}\")\n",
    "\n",
    "# å‡†å¤‡ dollar bar è¾“å…¥æ ¼å¼: [date_time, price, volume]\n",
    "dollar_bar_input = pd.DataFrame({\n",
    "    'date_time': minute_df.index,\n",
    "    'price': minute_df['BidClose'],\n",
    "    'volume': minute_df['Volume']\n",
    "})\n",
    "\n",
    "# ç§»é™¤æ— æ•ˆæ•°æ®\n",
    "dollar_bar_input = dollar_bar_input[\n",
    "    (dollar_bar_input['volume'] > 0) & \n",
    "    (dollar_bar_input['volume'].notna()) &\n",
    "    (dollar_bar_input['price'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"æœ‰æ•ˆæ•°æ®ç¬”æ•°: {len(dollar_bar_input):,}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. è®¡ç®— Dollar Bar (æ¯1000ä¸‡ä¸€æ ¹)\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š æ­¥éª¤2: è®¡ç®— Dollar Bar (threshold=10,000,000)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dollar_threshold = 100_000_000  # 1000ä¸‡\n",
    "\n",
    "# ä½¿ç”¨è‡ªå·±å®ç°çš„å‡½æ•°è®¡ç®— dollar bar\n",
    "dollar_bars = get_dollar_bars_custom(dollar_bar_input, threshold=dollar_threshold)\n",
    "\n",
    "print(f\"\\nDollar Bar ç»Ÿè®¡:\")\n",
    "print(f\"  Dollar Bar æ•°é‡: {len(dollar_bars):,}\")\n",
    "if len(dollar_bars) > 0:\n",
    "    print(f\"  æœŸé—´: {dollar_bars.index.min()} è‡³ {dollar_bars.index.max()}\")\n",
    "    print(f\"  å¹³å‡æ¯æ ¹ bar çš„ dollar value: {dollar_bars['cum_dollar_value'].mean():,.0f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. è¯»å–15åˆ†é’Ÿæ•°æ®\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š æ­¥éª¤3: è¯»å–15åˆ†é’Ÿæ•°æ®\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¯»å–15åˆ†é’Ÿæ•°æ®\n",
    "nas100_15min = pd.read_csv('equity_prices/NAS100.csv')\n",
    "nas100_15min['Date'] = pd.to_datetime(nas100_15min['Date'])\n",
    "nas100_15min.set_index('Date', inplace=True)\n",
    "\n",
    "# ä½¿ç”¨ BidClose ä½œä¸ºä¸»è¦ä»·æ ¼\n",
    "nas100_15min['Close'] = nas100_15min['BidClose']\n",
    "nas100_15min['High'] = nas100_15min['BidHigh'] \n",
    "nas100_15min['Low'] = nas100_15min['BidLow']\n",
    "nas100_15min['Open'] = nas100_15min['BidOpen']\n",
    "\n",
    "print(f\"15åˆ†é’Ÿæ•°æ®æ€»ç¬”æ•°: {len(nas100_15min):,}\")\n",
    "print(f\"æ•°æ®æœŸé—´: {nas100_15min.index.min()} è‡³ {nas100_15min.index.max()}\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# Kçº¿å›¾ç»˜åˆ¶å‡½æ•°\n",
    "# =====================================================\n",
    "def plot_candlestick(ax, data, x_positions, width=0.6, color_up='green', color_down='red'):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶Kçº¿å›¾\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib axes\n",
    "        ç»˜å›¾è½´\n",
    "    data : pd.DataFrame\n",
    "        åŒ…å« open, high, low, close åˆ—çš„æ•°æ®\n",
    "    x_positions : array-like\n",
    "        Xè½´ä½ç½®ï¼ˆæ—¶é—´ç´¢å¼•å¯¹åº”çš„æ•°å€¼ä½ç½®ï¼‰\n",
    "    width : float\n",
    "        Kçº¿å®½åº¦\n",
    "    color_up : str\n",
    "        ä¸Šæ¶¨é¢œè‰²\n",
    "    color_down : str\n",
    "        ä¸‹è·Œé¢œè‰²\n",
    "    \"\"\"\n",
    "    for i, (idx, row) in enumerate(data.iterrows()):\n",
    "        x = x_positions[i]\n",
    "        open_price = row['open']\n",
    "        high_price = row['high']\n",
    "        low_price = row['low']\n",
    "        close_price = row['close']\n",
    "        \n",
    "        # åˆ¤æ–­æ¶¨è·Œ\n",
    "        is_up = close_price >= open_price\n",
    "        color = color_up if is_up else color_down\n",
    "        \n",
    "        # ç»˜åˆ¶ä¸Šä¸‹å½±çº¿\n",
    "        ax.plot([x, x], [low_price, high_price], color='black', linewidth=0.5)\n",
    "        \n",
    "        # ç»˜åˆ¶å®ä½“\n",
    "        body_low = min(open_price, close_price)\n",
    "        body_high = max(open_price, close_price)\n",
    "        body_height = body_high - body_low\n",
    "        \n",
    "        # å¦‚æœå®ä½“é«˜åº¦ä¸º0ï¼Œç”»ä¸€æ¡çº¿\n",
    "        if body_height == 0:\n",
    "            ax.plot([x - width/2, x + width/2], [open_price, close_price], \n",
    "                   color=color, linewidth=2)\n",
    "        else:\n",
    "            rect = Rectangle((x - width/2, body_low), width, body_height,\n",
    "                           facecolor=color, edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "# =====================================================\n",
    "# è¯»å–æ•°æ®ï¼ˆå‡è®¾å·²ç»è®¡ç®—å¥½dollar_barså’Œnas100_15minï¼‰\n",
    "# =====================================================\n",
    "# è¿™é‡Œå‡è®¾ä½ å·²ç»æœ‰äº† dollar_bars å’Œ nas100_15min æ•°æ®\n",
    "# å¦‚æœæ²¡æœ‰ï¼Œè¯·å…ˆè¿è¡Œå‰é¢çš„ä»£ç è®¡ç®—\n",
    "\n",
    "# ç¡®å®šæœ€è¿‘30å¤©çš„æ—¥æœŸèŒƒå›´\n",
    "end_date = max(\n",
    "    dollar_bars.index.max() if len(dollar_bars) > 0 else pd.Timestamp('2000-01-01'),\n",
    "    nas100_15min.index.max()\n",
    ")\n",
    "start_date = end_date - timedelta(days=30)\n",
    "\n",
    "# ç­›é€‰æœ€è¿‘30å¤©çš„æ•°æ®\n",
    "dollar_bars_plot = dollar_bars.loc[\n",
    "    (dollar_bars.index >= start_date) & (dollar_bars.index <= end_date)\n",
    "] if len(dollar_bars) > 0 else pd.DataFrame()\n",
    "\n",
    "nas100_15min_plot = nas100_15min.loc[\n",
    "    (nas100_15min.index >= start_date) & (nas100_15min.index <= end_date)\n",
    "]\n",
    "\n",
    "# å‡†å¤‡15åˆ†é’Ÿbarçš„Kçº¿æ•°æ®\n",
    "nas100_15min_ohlc = nas100_15min_plot[['Open', 'High', 'Low', 'Close']].copy()\n",
    "nas100_15min_ohlc.columns = ['open', 'high', 'low', 'close']\n",
    "\n",
    "# å‡†å¤‡dollar barçš„Kçº¿æ•°æ®\n",
    "dollar_bars_ohlc = dollar_bars_plot[['open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "print(f\"ç»˜å›¾æœŸé—´: {start_date.date()} è‡³ {end_date.date()}\")\n",
    "print(f\"15åˆ†é’Ÿæ•°æ®: {len(nas100_15min_plot):,} ç¬”\")\n",
    "print(f\"Dollar Bar: {len(dollar_bars_plot):,} æ ¹\")\n",
    "\n",
    "# =====================================================\n",
    "# åˆ›å»ºæ—¶é—´è½´æ˜ å°„ï¼ˆä»¥15åˆ†é’Ÿbarçš„æ—¶é—´è½´ä¸ºå‡†ï¼‰\n",
    "# =====================================================\n",
    "# å°†æ—¶é—´è½¬æ¢ä¸ºæ•°å€¼ä½ç½®ï¼ˆç”¨äºXè½´ï¼‰\n",
    "time_to_position_15min = {time: i for i, time in enumerate(nas100_15min_plot.index)}\n",
    "\n",
    "# ä¸ºdollar baræ‰¾åˆ°å¯¹åº”çš„Xè½´ä½ç½®\n",
    "# å¦‚æœdollar barçš„æ—¶é—´åœ¨15åˆ†é’Ÿbarä¹‹é—´ï¼Œä½¿ç”¨æœ€è¿‘çš„ä¸‹ä¸€ä¸ª15åˆ†é’Ÿbarä½ç½®\n",
    "dollar_bar_positions = []\n",
    "for db_time in dollar_bars_plot.index:\n",
    "    # æ‰¾åˆ°æœ€æ¥è¿‘çš„15åˆ†é’Ÿbarä½ç½®\n",
    "    # å¦‚æœdollar baræ—¶é—´åœ¨15åˆ†é’Ÿbarä¹‹å‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª15åˆ†é’Ÿbarçš„ä½ç½®\n",
    "    # å¦‚æœdollar baræ—¶é—´åœ¨15åˆ†é’Ÿbarä¹‹åï¼Œä½¿ç”¨æœ€åä¸€ä¸ª15åˆ†é’Ÿbarçš„ä½ç½®\n",
    "    # å¦åˆ™ä½¿ç”¨æœ€æ¥è¿‘çš„15åˆ†é’Ÿbarä½ç½®\n",
    "    if db_time <= nas100_15min_plot.index[0]:\n",
    "        pos = 0\n",
    "    elif db_time >= nas100_15min_plot.index[-1]:\n",
    "        pos = len(nas100_15min_plot) - 1\n",
    "    else:\n",
    "        # æ‰¾åˆ°æœ€æ¥è¿‘çš„15åˆ†é’Ÿbarä½ç½®ï¼ˆä½¿ç”¨ä¸‹ä¸€ä¸ª15åˆ†é’Ÿbarçš„ä½ç½®ï¼‰\n",
    "        idx = nas100_15min_plot.index.searchsorted(db_time, side='right')\n",
    "        pos = min(idx, len(nas100_15min_plot) - 1)\n",
    "    dollar_bar_positions.append(pos)\n",
    "\n",
    "# ä¸ºäº†åœ¨åŒä¸€æ—¶é—´æ®µæ˜¾ç¤ºå¤šæ ¹dollar barï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´ä½ç½®\n",
    "# ç»Ÿè®¡æ¯ä¸ª15åˆ†é’Ÿæ—¶é—´æ®µå†…çš„dollar baræ•°é‡\n",
    "from collections import defaultdict\n",
    "dollar_bars_per_15min = defaultdict(list)\n",
    "for i, db_time in enumerate(dollar_bars_plot.index):\n",
    "    pos = dollar_bar_positions[i]\n",
    "    dollar_bars_per_15min[pos].append(i)\n",
    "\n",
    "# è°ƒæ•´ä½ç½®ï¼šå¦‚æœåŒä¸€æ—¶é—´æ®µæœ‰å¤šæ ¹dollar barï¼Œå°†å®ƒä»¬åˆ†æ•£æ˜¾ç¤º\n",
    "adjusted_dollar_bar_positions = []\n",
    "dollar_bar_widths = []\n",
    "for i, db_time in enumerate(dollar_bars_plot.index):\n",
    "    pos = dollar_bar_positions[i]\n",
    "    bars_in_slot = dollar_bars_per_15min[pos]\n",
    "    if len(bars_in_slot) > 1:\n",
    "        # å¤šæ ¹barï¼Œåˆ†æ•£æ˜¾ç¤º\n",
    "        idx_in_slot = bars_in_slot.index(i)\n",
    "        slot_width = 0.8  # æ¯ä¸ªæ—¶é—´æ®µçš„å®½åº¦\n",
    "        offset = (idx_in_slot - (len(bars_in_slot) - 1) / 2) * (slot_width / len(bars_in_slot))\n",
    "        adjusted_pos = pos + offset\n",
    "        width = 0.8 / len(bars_in_slot) * 0.6  # æ ¹æ®baræ•°é‡è°ƒæ•´å®½åº¦\n",
    "    else:\n",
    "        # å•æ ¹barï¼Œå±…ä¸­æ˜¾ç¤º\n",
    "        adjusted_pos = pos\n",
    "        width = 0.4\n",
    "    adjusted_dollar_bar_positions.append(adjusted_pos)\n",
    "    dollar_bar_widths.append(width)\n",
    "\n",
    "# =====================================================\n",
    "# ç»˜åˆ¶å›¾è¡¨\n",
    "# =====================================================\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "\n",
    "# åˆ›å»ºå­å›¾å¸ƒå±€\n",
    "gs = fig.add_gridspec(3, 1, height_ratios=[2, 2, 1], hspace=0.3)\n",
    "\n",
    "# å­å›¾1: 15åˆ†é’ŸKçº¿å›¾\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "x_positions_15min = np.arange(len(nas100_15min_plot))\n",
    "plot_candlestick(ax1, nas100_15min_ohlc, x_positions_15min, width=0.6)\n",
    "ax1.set_title('15min Bars - Candlestick Chart (Last 30 Days)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Price', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(x_positions_15min[::max(1, len(nas100_15min_plot)//10)])\n",
    "ax1.set_xticklabels([nas100_15min_plot.index[i].strftime('%Y-%m-%d %H:%M') \n",
    "                     for i in range(0, len(nas100_15min_plot), max(1, len(nas100_15min_plot)//10))],\n",
    "                    rotation=45, ha='right')\n",
    "\n",
    "# å­å›¾2: Dollar Bar Kçº¿å›¾ï¼ˆä½¿ç”¨ç›¸åŒçš„Xè½´æ—¶é—´è½´ï¼‰\n",
    "ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "plot_candlestick(ax2, dollar_bars_ohlc, adjusted_dollar_bar_positions, \n",
    "                 width=np.array(dollar_bar_widths))\n",
    "ax2.set_title('Dollar Bars (10M) - Candlestick Chart (Last 30 Days)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Price', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(x_positions_15min[::max(1, len(nas100_15min_plot)//10)])\n",
    "ax2.set_xticklabels([nas100_15min_plot.index[i].strftime('%Y-%m-%d %H:%M') \n",
    "                     for i in range(0, len(nas100_15min_plot), max(1, len(nas100_15min_plot)//10))],\n",
    "                    rotation=45, ha='right')\n",
    "\n",
    "# å­å›¾3: Dollar Value\n",
    "ax3 = fig.add_subplot(gs[2], sharex=ax1)\n",
    "if len(dollar_bars_plot) > 0:\n",
    "    ax3.bar(adjusted_dollar_bar_positions, dollar_bars_plot['cum_dollar_value'], \n",
    "            width=np.array(dollar_bar_widths), alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax3.axhline(y=10_000_000, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Threshold: 10,000,000')\n",
    "ax3.set_title('Dollar Bar: Cumulative Dollar Value per Bar', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Time (15min Bar Timeline)', fontsize=12)\n",
    "ax3.set_ylabel('Cumulative Dollar Value', fontsize=12)\n",
    "ax3.legend(loc='best', fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xticks(x_positions_15min[::max(1, len(nas100_15min_plot)//10)])\n",
    "ax3.set_xticklabels([nas100_15min_plot.index[i].strftime('%Y-%m-%d %H:%M') \n",
    "                     for i in range(0, len(nas100_15min_plot), max(1, len(nas100_15min_plot)//10))],\n",
    "                    rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =====================================================\n",
    "# ç»Ÿè®¡ä¿¡æ¯\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç»Ÿè®¡æ¯ä¸ª15åˆ†é’Ÿæ—¶é—´æ®µå†…çš„dollar baræ•°é‡\n",
    "max_bars_per_15min = max(len(bars) for bars in dollar_bars_per_15min.values()) if dollar_bars_per_15min else 0\n",
    "avg_bars_per_15min = np.mean([len(bars) for bars in dollar_bars_per_15min.values()]) if dollar_bars_per_15min else 0\n",
    "\n",
    "print(f\"\\nDollar Bar åˆ†å¸ƒç»Ÿè®¡:\")\n",
    "print(f\"  æ¯ä¸ª15åˆ†é’Ÿæ—¶é—´æ®µæœ€å¤šæœ‰ {max_bars_per_15min} æ ¹ dollar bar\")\n",
    "print(f\"  æ¯ä¸ª15åˆ†é’Ÿæ—¶é—´æ®µå¹³å‡æœ‰ {avg_bars_per_15min:.2f} æ ¹ dollar bar\")\n",
    "print(f\"  æœ‰ {sum(1 for bars in dollar_bars_per_15min.values() if len(bars) > 1)} ä¸ªæ—¶é—´æ®µæœ‰å¤šæ ¹ dollar bar\")\n",
    "\n",
    "if len(dollar_bars_plot) > 0:\n",
    "    print(f\"\\nDollar Bar ç»Ÿè®¡:\")\n",
    "    print(f\"  æ€» bar æ•°: {len(dollar_bars_plot):,}\")\n",
    "    print(f\"  å¹³å‡ dollar value: {dollar_bars_plot['cum_dollar_value'].mean():,.0f}\")\n",
    "    print(f\"  æœ€å° dollar value: {dollar_bars_plot['cum_dollar_value'].min():,.0f}\")\n",
    "    print(f\"  æœ€å¤§ dollar value: {dollar_bars_plot['cum_dollar_value'].max():,.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1932e",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ1ï¼šè³‡æ–™é›œè¨Šéæ¿¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9270ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1ï¸âƒ£ é›œè¨Šéæ¿¾ Class\n",
    "# =====================================================\n",
    "class NoiseFilter:\n",
    "    \"\"\"\n",
    "    é›œè¨Šéæ¿¾å™¨ï¼šè¨ˆç®—æ³¢å‹•ç‡ã€CUSUM éæ¿¾ç­‰\n",
    "    \n",
    "    CUSUM Filter èªªæ˜ï¼š\n",
    "    ==================\n",
    "    CUSUM (Cumulative Sum) Filter æ˜¯ä¸€ç¨®å“è³ªæ§åˆ¶æ–¹æ³•ï¼Œç”¨æ–¼æª¢æ¸¬æ¸¬é‡å€¼\n",
    "    ç›¸å°æ–¼ç›®æ¨™å€¼çš„å‡å€¼åç§»ã€‚\n",
    "    \n",
    "    ç”¨é€”ï¼š\n",
    "    -----\n",
    "    1. éæ¿¾å¸‚å ´é›œè¨Šï¼Œåªä¿ç•™é¡¯è‘—çš„åƒ¹æ ¼è®Šå‹•äº‹ä»¶\n",
    "    2. é¿å…åœ¨åƒ¹æ ¼åœ¨é–¾å€¼é™„è¿‘å¾˜å¾Šæ™‚è§¸ç™¼å¤šå€‹äº‹ä»¶ï¼ˆé€™æ˜¯ Bollinger Bands ç­‰\n",
    "       å¸¸è¦‹å¸‚å ´ä¿¡è™Ÿçš„ç¼ºé»ï¼‰\n",
    "    3. éœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦ h æ‰æœƒè§¸ç™¼äº‹ä»¶ï¼Œä½¿äº‹ä»¶æ›´å…·æ„ç¾©\n",
    "    \n",
    "    æ•¸å­¸åŸç†ï¼š\n",
    "    ---------\n",
    "    å°æ–¼è§€æ¸¬å€¼ {y_t}_{t=1,...,T}ï¼Œå®šç¾©ç´¯ç©å’Œï¼š\n",
    "    \n",
    "    S^+_t = max{0, S^+_{t-1} + y_t - E_{t-1}[y_t]},  S^+_0 = 0\n",
    "    S^-_t = min{0, S^-_{t-1} + y_t - E_{t-1}[y_t]},  S^-_0 = 0\n",
    "    S_t = max{S^+_t, -S^-_t}\n",
    "    \n",
    "    å…¶ä¸­ E_{t-1}[y_t] = y_{t-1}ï¼ˆä½¿ç”¨å‰ä¸€æœŸå€¼ä½œç‚ºæœŸæœ›å€¼ï¼‰\n",
    "    \n",
    "    ç•¶ S_t â‰¥ hï¼ˆé–¾å€¼ï¼‰æ™‚ï¼Œè§¸ç™¼äº‹ä»¶ä¸¦é‡ç½® S_t = 0ã€‚\n",
    "    \n",
    "    å°ç¨± CUSUM Filter çš„å„ªå‹¢ï¼š\n",
    "    -------------------------\n",
    "    - åŒæ™‚æ•æ‰ä¸Šæ¼²å’Œä¸‹è·Œçš„ç´¯ç©åå·®\n",
    "    - é›¶åº•ç·šæ©Ÿåˆ¶ï¼šç•¶ y_t â‰¤ E_{t-1}[y_t] - S_{t-1} æ™‚ï¼ŒS_t = 0\n",
    "    - é¿å…åœ¨é–¾å€¼é™„è¿‘åè¦†è§¸ç™¼äº‹ä»¶\n",
    "    - éœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦æ‰æœƒè§¸ç™¼ï¼Œä½¿äº‹ä»¶æ›´å…·çµ±è¨ˆæ„ç¾©\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Lam and Yam (1997): æå‡ºäº¤æ›¿è²·è³£ä¿¡è™Ÿçš„æŠ•è³‡ç­–ç•¥\n",
    "    - Fama and Blume (1966): Filter trading strategy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vol_span: int = 100):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        vol_span : int\n",
    "            æ—¥æ³¢å‹•ç‡è¨ˆç®—çš„ EWM span\n",
    "        \"\"\"\n",
    "        self.vol_span = vol_span\n",
    "        self.daily_vol = None\n",
    "        self.total_bars = None\n",
    "        self.filtered_bars = None\n",
    "        \n",
    "    def calculate_daily_vol(self, close: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ—¥æ³¢å‹•ç‡\n",
    "        \n",
    "        ä½¿ç”¨æŒ‡æ•¸åŠ æ¬Šç§»å‹•æ¨™æº–å·® (EWM) è¨ˆç®—æ³¢å‹•ç‡ï¼Œå°è¿‘æœŸæ•¸æ“šçµ¦äºˆæ›´é«˜æ¬Šé‡ã€‚\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        daily_vol : pd.Series\n",
    "            æ—¥æ³¢å‹•ç‡åºåˆ—\n",
    "        \"\"\"\n",
    "        # è¨˜éŒ„ç¸½ bar æ•¸\n",
    "        self.total_bars = len(close)\n",
    "        \n",
    "        # è¨ˆç®—æ—¥æ”¶ç›Šç‡\n",
    "        df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))\n",
    "        df0 = df0[df0 > 0]\n",
    "        df0 = pd.Series(close.index[df0 - 1], \n",
    "                        index=close.index[close.shape[0] - df0.shape[0]:])\n",
    "        df0 = close.loc[df0.index] / close.loc[df0.values].values - 1\n",
    "        \n",
    "        # ä½¿ç”¨ EWM è¨ˆç®—æ¨™æº–å·®ï¼ˆæ³¢å‹•ç‡ï¼‰\n",
    "        df0 = df0.ewm(span=self.vol_span).std()\n",
    "        \n",
    "        self.daily_vol = df0\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_vol_count = df0.notna().sum()\n",
    "        vol_stats = self.get_vol_stats()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ³¢å‹•ç‡è¨ˆç®—çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½ K ç·šæ•¸ (Total Bars): {self.total_bars:,}\")\n",
    "        print(f\"æœ‰æ•ˆæ³¢å‹•ç‡æ•¸æ“šæ•¸: {valid_vol_count:,}\")\n",
    "        print(f\"æ³¢å‹•ç‡è¨ˆç®—æ™‚é–“çª—å£ (vol_span): {self.vol_span} æœŸ\")\n",
    "        print(f\"  (è¨»: EWM ä½¿ç”¨æŒ‡æ•¸åŠ æ¬Šï¼Œä¸æ˜¯å›ºå®šçª—å£)\")\n",
    "        print(f\"\\næ³¢å‹•ç‡çµ±è¨ˆ:\")\n",
    "        if vol_stats:\n",
    "            print(f\"  å¹³å‡å€¼ (Mean): {vol_stats['mean']:.6f}\")\n",
    "            print(f\"  æ¨™æº–å·® (Std): {vol_stats['std']:.6f}\")\n",
    "            print(f\"  æœ€å°å€¼ (Min): {vol_stats['min']:.6f}\")\n",
    "            print(f\"  æœ€å¤§å€¼ (Max): {vol_stats['max']:.6f}\")\n",
    "            print(f\"  ä¸­ä½æ•¸ (Median): {vol_stats['median']:.6f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return df0\n",
    "    \n",
    "    def cusum_filter(self, close: pd.Series, threshold: float) -> pd.DatetimeIndex:\n",
    "        \"\"\"\n",
    "        å°ç¨± CUSUM éæ¿¾å™¨ (Symmetric CUSUM Filter)\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°åƒ¹æ ¼åºåˆ—çš„å·®åˆ†ï¼ˆdiff = y_t - y_{t-1}ï¼‰æ‡‰ç”¨å°ç¨± CUSUM éæ¿¾ï¼š\n",
    "        \n",
    "        S^+_t = max{0, S^+_{t-1} + diff_t}  (ä¸Šæ¼²ç´¯ç©)\n",
    "        S^-_t = min{0, S^-_{t-1} + diff_t}  (ä¸‹è·Œç´¯ç©)\n",
    "        \n",
    "        ç•¶ S^-_t < -h æˆ– S^+_t > h æ™‚ï¼š\n",
    "        - è§¸ç™¼äº‹ä»¶ï¼ˆè¨˜éŒ„æ™‚é–“é»ï¼‰\n",
    "        - é‡ç½®å°æ‡‰çš„ç´¯ç©å’Œç‚º 0\n",
    "        \n",
    "        é€™æ¨£è¨­è¨ˆçš„å„ªå‹¢ï¼š\n",
    "        --------------\n",
    "        1. é¿å…åœ¨é–¾å€¼é™„è¿‘åè¦†è§¸ç™¼ï¼ˆéœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦ hï¼‰\n",
    "        2. åŒæ™‚æ•æ‰ä¸Šæ¼²å’Œä¸‹è·Œçš„ç´¯ç©åå·®\n",
    "        3. é›¶åº•ç·šæ©Ÿåˆ¶ç¢ºä¿åªæ•æ‰å–®æ–¹å‘çš„æŒçºŒåå·®\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        threshold : float\n",
    "            CUSUM é–¾å€¼ (h)\n",
    "            é€šå¸¸ä½¿ç”¨æ³¢å‹•ç‡çš„å¹³å‡å€¼æˆ–æ¨™æº–å·®\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            éæ¿¾å¾Œçš„äº‹ä»¶æ™‚é–“é»\n",
    "            \n",
    "        åƒè€ƒå¯¦ç¾ï¼š\n",
    "        ---------\n",
    "        åŸºæ–¼ Snippet 2.4 (Advances in Financial Machine Learning, Chapter 2)\n",
    "        \"\"\"\n",
    "        # è¨˜éŒ„ç¸½ bar æ•¸\n",
    "        self.total_bars = len(close)\n",
    "        \n",
    "        tEvents = []\n",
    "        sPos, sNeg = 0, 0  # S^+_0 = 0, S^-_0 = 0\n",
    "        diff = close.diff()  # y_t - y_{t-1}\n",
    "        \n",
    "        # éæ­·æ¯å€‹æ™‚é–“é»ï¼ˆå¾ç¬¬äºŒå€‹é–‹å§‹ï¼Œå› ç‚ºç¬¬ä¸€å€‹ diff æ˜¯ NaNï¼‰\n",
    "        for i in diff.index[1:]:\n",
    "            # æ›´æ–°ç´¯ç©å’Œ\n",
    "            # S^+_t = max{0, S^+_{t-1} + diff_t}\n",
    "            sPos = max(0, sPos + diff.loc[i])\n",
    "            # S^-_t = min{0, S^-_{t-1} + diff_t}\n",
    "            sNeg = min(0, sNeg + diff.loc[i])\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦è§¸ç™¼äº‹ä»¶\n",
    "            if sNeg < -threshold:\n",
    "                # ä¸‹è·Œç´¯ç©è¶…éé–¾å€¼ï¼šè§¸ç™¼äº‹ä»¶ä¸¦é‡ç½®\n",
    "                sNeg = 0\n",
    "                tEvents.append(i)\n",
    "            elif sPos > threshold:\n",
    "                # ä¸Šæ¼²ç´¯ç©è¶…éé–¾å€¼ï¼šè§¸ç™¼äº‹ä»¶ä¸¦é‡ç½®\n",
    "                sPos = 0\n",
    "                tEvents.append(i)\n",
    "        \n",
    "        filtered_events = pd.DatetimeIndex(tEvents)\n",
    "        self.filtered_bars = len(filtered_events)\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        filter_rate = self.filtered_bars / self.total_bars if self.total_bars > 0 else 0\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š CUSUM éæ¿¾çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½ K ç·šæ•¸ (Total Bars): {self.total_bars:,}\")\n",
    "        print(f\"ç¯©é¸å¾Œ K ç·šæ•¸ (Filtered Bars): {self.filtered_bars:,}\")\n",
    "        print(f\"éæ¿¾æ¯”ä¾‹: {filter_rate:.4%}\")\n",
    "        print(f\"CUSUM é–¾å€¼ (Threshold h): {threshold:.6f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - åªä¿ç•™åƒ¹æ ¼ç´¯ç©è®Šå‹•è¶…éé–¾å€¼ {threshold:.6f} çš„æ™‚é–“é»\")\n",
    "        print(f\"  - é¿å…åœ¨é–¾å€¼é™„è¿‘åè¦†è§¸ç™¼äº‹ä»¶\")\n",
    "        print(f\"  - éœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦æ‰æœƒè§¸ç™¼ï¼Œä½¿äº‹ä»¶æ›´å…·çµ±è¨ˆæ„ç¾©\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return filtered_events\n",
    "    \n",
    "    def get_vol_stats(self) -> dict:\n",
    "        \"\"\"å–å¾—æ³¢å‹•ç‡çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if self.daily_vol is None:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'mean': self.daily_vol.mean(),\n",
    "            'std': self.daily_vol.std(),\n",
    "            'min': self.daily_vol.min(),\n",
    "            'max': self.daily_vol.max(),\n",
    "            'median': self.daily_vol.median()\n",
    "        }\n",
    "    \n",
    "    def plot_volatility(self):\n",
    "        \"\"\"ç¹ªè£½æ³¢å‹•ç‡åœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.daily_vol is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_daily_vol()\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        self.daily_vol.plot(ax=ax, title='Daily Volatility', linewidth=1)\n",
    "        ax.axhline(self.daily_vol.mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {self.daily_vol.mean():.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. é›œè¨Šéæ¿¾\n",
    "noise_filter = NoiseFilter(vol_span=500)\n",
    "daily_vol = noise_filter.calculate_daily_vol(nas100_raw['Close'])\n",
    "\n",
    "# 2. CUSUM ç¯©é¸æ•¸æ“š\n",
    "# ä½¿ç”¨æ³¢å‹•ç‡å¹³å‡å€¼ä½œç‚ºé–¾å€¼\n",
    "# cusum_threshold = daily_vol.mean()\n",
    "cusum_threshold = 0.1\n",
    "cusum_events = noise_filter.cusum_filter(nas100_raw['Close'], cusum_threshold)\n",
    "df_filtered = nas100_raw.loc[cusum_events].copy()  # âœ… éæ¿¾å¾Œçš„æ•¸æ“š\n",
    "print(f\"éæ¿¾å¾Œçš„è³‡æ–™ç­†æ•¸: {len(df_filtered)}\")\n",
    "noise_filter.plot_volatility()\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a23019",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ2 ï¼šç­–ç•¥è¨Šè™Ÿè¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7792ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2ï¸âƒ£ L1-OLD ç­–ç•¥ Class\n",
    "# =====================================================\n",
    "class L1Strategy:\n",
    "    \"\"\"L1-OLD çªç ´ç­–ç•¥\"\"\"\n",
    "    \n",
    "    def __init__(self, entry_param: float = 0.5, rolling_window: int = 460):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        entry_param : float\n",
    "            çªç ´åƒæ•¸ï¼ˆé è¨­ 0.5ï¼‰\n",
    "        rolling_window : int\n",
    "            è¨ˆç®— HL115 å’Œçªç ´åƒ¹çš„æ»¾å‹•çª—å£ï¼ˆé è¨­ 460ï¼‰\n",
    "        \"\"\"\n",
    "        self.entry_param = entry_param\n",
    "        self.rolling_window = rolling_window\n",
    "        self.signals = None\n",
    "        \n",
    "    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—ç­–ç•¥æŒ‡æ¨™\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            éœ€åŒ…å« 'High', 'Low', 'Open', 'Close' æ¬„ä½\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        df : pd.DataFrame\n",
    "            æ–°å¢ 'HL115', 'çªç ´signalåƒ¹', 'signal', 'side' æ¬„ä½\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # è¨ˆç®— HL115ï¼ˆéå»Næ ¹Kç·šçš„åƒ¹æ ¼ç¯„åœï¼‰\n",
    "        df['HL115'] = (\n",
    "            df['High'].shift(1).rolling(self.rolling_window, \n",
    "                                           min_periods=self.rolling_window).max() - \n",
    "            df['Low'].shift(1).rolling(self.rolling_window, \n",
    "                                         min_periods=self.rolling_window).min()\n",
    "        )\n",
    "        df['HL115%'] = df['HL115'] / df['Low'].shift(self.rolling_window+1)\n",
    "        \n",
    "        # è¨ˆç®—çªç ´signalåƒ¹\n",
    "        df['çªç ´signalåƒ¹'] = (\n",
    "            df['Low'].shift(1).rolling(self.rolling_window).min() + \n",
    "            (self.entry_param * df['HL115'])\n",
    "        )\n",
    "        df['çªç ´signalåƒ¹%'] = df['çªç ´signalåƒ¹']/df['Open']\n",
    "        \n",
    "        # ç”Ÿæˆçªç ´ä¿¡è™Ÿ\n",
    "        df['signal'] = False\n",
    "        df['signal'] = (\n",
    "            (df['signal'].shift(1) == False) & \n",
    "            (df['High'] > df['çªç ´signalåƒ¹']) & \n",
    "            (df['Open'] < df['çªç ´signalåƒ¹'])\n",
    "        )\n",
    "        \n",
    "        # è½‰æ›ç‚º sideï¼ˆåšå¤šæ–¹å‘ï¼‰\n",
    "        df['side'] = np.nan\n",
    "        df.loc[df['signal'] == True, 'side'] = 1.0\n",
    "        df['side'] = df['side'].ffill()\n",
    "        \n",
    "        self.signals = df\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        total_bars = len(df)\n",
    "        signal_count = df['signal'].sum()\n",
    "        signal_rate = signal_count / total_bars if total_bars > 0 else 0\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š L1-OLD ç­–ç•¥çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½ K ç·šæ•¸ (Bars): {total_bars:,}\")\n",
    "        print(f\"ä¿¡è™Ÿæ•¸é‡ (Signals): {signal_count:,}\")\n",
    "        print(f\"ä¿¡è™Ÿæ¯”ä¾‹: {signal_rate:.4%}\")\n",
    "        print(f\"ç­–ç•¥åƒæ•¸: entry_param={self.entry_param}, rolling_window={self.rolling_window}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_signal_events(self) -> pd.DatetimeIndex:\n",
    "        \"\"\"å–å¾—ä¿¡è™Ÿç™¼ç”Ÿçš„æ™‚é–“é»\"\"\"\n",
    "        if self.signals is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_indicators()\")\n",
    "            return None\n",
    "        \n",
    "        return self.signals[self.signals['signal'] == True].index\n",
    "    \n",
    "    def get_signal_stats(self) -> dict:\n",
    "        \"\"\"å–å¾—ä¿¡è™Ÿçµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if self.signals is None:\n",
    "            return None\n",
    "        \n",
    "        signal_count = self.signals['signal'].sum()\n",
    "        total_bars = len(self.signals)\n",
    "        \n",
    "        return {\n",
    "            'total_signals': signal_count,\n",
    "            'signal_rate': signal_count / total_bars,\n",
    "            'avg_HL115': self.signals['HL115'].mean(),\n",
    "            'avg_price_to_signal': (self.signals['Close'] / self.signals['çªç ´signalåƒ¹']).mean()\n",
    "        }\n",
    "    \n",
    "    def plot_strategy(self, start_idx: Optional[int] = None, end_idx: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ç­–ç•¥åœ–\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_idx : int, optional\n",
    "            èµ·å§‹ç´¢å¼•\n",
    "        end_idx : int, optional\n",
    "            çµæŸç´¢å¼•\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.signals is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_indicators()\")\n",
    "            return\n",
    "        \n",
    "        df = self.signals.iloc[start_idx:end_idx] if start_idx or end_idx else self.signals\n",
    "        signal_events = df[df['signal'] == True].index\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # 1. åƒ¹æ ¼èˆ‡çªç ´ä¿¡è™Ÿ\n",
    "        axes[0].plot(df.index, df['Close'], label='Close Price', alpha=0.7, linewidth=1)\n",
    "        axes[0].plot(df.index, df['çªç ´signalåƒ¹'], label='Breakout Signal', \n",
    "                    linestyle='--', alpha=0.7, color='orange')\n",
    "        axes[0].scatter(signal_events, df.loc[signal_events, 'Close'], \n",
    "                       color='red', s=50, label='Entry Signal', zorder=5, marker='^')\n",
    "        axes[0].set_title(f'L1-OLD Strategy (entry_param={self.entry_param})', fontsize=12)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. HL115 æ³¢å‹•æŒ‡æ¨™\n",
    "        axes[1].plot(df.index, df['HL115'], label='HL115', color='orange', linewidth=1)\n",
    "        axes[1].set_title(f'Price Range Indicator (window={self.rolling_window})', fontsize=12)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fc79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. åœ¨ç¯©é¸å¾Œçš„æ•¸æ“šä¸Šè¨ˆç®—ç­–ç•¥ä¿¡è™Ÿ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ˆ åœ¨ç¯©é¸å¾Œçš„æ•¸æ“šä¸Šè¨ˆç®—ç­–ç•¥ä¿¡è™Ÿ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "strategy = L1Strategy(entry_param=0.5, rolling_window=460)\n",
    "df = strategy.calculate_indicators(df_filtered)  # âœ… ä½¿ç”¨ df_filtered\n",
    "tEvents = strategy.get_signal_events()  # ç­–ç•¥ä¿¡è™Ÿçš„äº‹ä»¶é»\n",
    "strategy.plot_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce1d23",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ3ï¼šMeta Labeling ç›®æ¨™è®Šæ•¸ç”Ÿæˆèˆ‡æ­¢ç›ˆæ­¢æè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8780be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3ï¸âƒ£ Meta-Labeling Class\n",
    "# =====================================================\n",
    "class MetaLabeling:\n",
    "    \"\"\"\n",
    "    Meta-Labelingï¼šTriple Barrier å’Œæ¨™ç±¤ç”Ÿæˆ\n",
    "    \n",
    "    Triple Barrier Method èªªæ˜ï¼š\n",
    "    ============================\n",
    "    Triple Barrier Method æ˜¯ä¸€ç¨®æ¨™ç±¤æ–¹æ³•ï¼Œæ ¹æ“šä¸‰å€‹å±éšœä¸­ç¬¬ä¸€å€‹è¢«è§¸åŠçš„\n",
    "    å±éšœä¾†æ¨™è¨˜è§€æ¸¬å€¼ã€‚\n",
    "    \n",
    "    ç”¨é€”ï¼š\n",
    "    -----\n",
    "    1. å‹•æ…‹è¨­å®šæ­¢ç›ˆæ­¢æï¼šæ ¹æ“šä¼°è¨ˆçš„æ³¢å‹•ç‡ï¼ˆå·²å¯¦ç¾æˆ–éš±å«ï¼‰å‹•æ…‹èª¿æ•´\n",
    "    2. é¿å…ç›®æ¨™éé«˜æˆ–éä½ï¼šè€ƒæ…®ç•¶å‰æ³¢å‹•ç‡ï¼Œè¨­å®šåˆç†çš„æ­¢ç›ˆæ­¢ææ°´å¹³\n",
    "    3. è·¯å¾‘ä¾è³´æ¨™ç±¤ï¼šè€ƒæ…®å¾ [t_i,0, t_i,0 + h] çš„å®Œæ•´åƒ¹æ ¼è·¯å¾‘\n",
    "    \n",
    "    ä¸‰å€‹å±éšœï¼š\n",
    "    --------\n",
    "    1. ä¸Šæ–¹æ°´å¹³å±éšœï¼ˆProfit Takingï¼‰ï¼šæ­¢ç›ˆé™åˆ¶\n",
    "    2. ä¸‹æ–¹æ°´å¹³å±éšœï¼ˆStop Lossï¼‰ï¼šæ­¢æé™åˆ¶\n",
    "    3. å‚ç›´å±éšœï¼ˆVertical Barrierï¼‰ï¼šæ™‚é–“åˆ°æœŸé™åˆ¶ï¼ˆæŒæœ‰æœŸï¼‰\n",
    "    \n",
    "    å±éšœé…ç½®ï¼š\n",
    "    --------\n",
    "    ç”¨ä¸‰å…ƒçµ„ [pt, sl, t1] è¡¨ç¤ºï¼Œå…¶ä¸­ï¼š\n",
    "    - 0 è¡¨ç¤ºå±éšœæœªå•Ÿç”¨\n",
    "    - 1 è¡¨ç¤ºå±éšœå•Ÿç”¨\n",
    "    \n",
    "    å¸¸ç”¨é…ç½®ï¼š\n",
    "    - [1,1,1]: æ¨™æº–è¨­å®šï¼ˆæ­¢ç›ˆã€æ­¢æã€æ™‚é–“åˆ°æœŸï¼‰\n",
    "    - [0,1,1]: åªæœ‰æ­¢æå’Œæ™‚é–“åˆ°æœŸ\n",
    "    - [1,1,0]: åªæœ‰æ­¢ç›ˆå’Œæ­¢æï¼ˆç„¡æ™‚é–“é™åˆ¶ï¼‰\n",
    "    \n",
    "    æ•¸å­¸åŸç†ï¼š\n",
    "    ---------\n",
    "    å°æ–¼æ¯å€‹äº‹ä»¶æ™‚é–“é» t_i,0ï¼š\n",
    "    \n",
    "    1. ä¸Šæ–¹å±éšœï¼ˆæ­¢ç›ˆï¼‰ï¼š\n",
    "       PT = t_i,0 + pt Ã— Ïƒ_t_i,0\n",
    "       å…¶ä¸­ pt æ˜¯æ­¢ç›ˆå€æ•¸ï¼ŒÏƒ_t_i,0 æ˜¯ç›®æ¨™æ³¢å‹•ç‡\n",
    "    \n",
    "    2. ä¸‹æ–¹å±éšœï¼ˆæ­¢æï¼‰ï¼š\n",
    "       SL = t_i,0 - sl Ã— Ïƒ_t_i,0\n",
    "       å…¶ä¸­ sl æ˜¯æ­¢æå€æ•¸\n",
    "    \n",
    "    3. å‚ç›´å±éšœï¼ˆæ™‚é–“åˆ°æœŸï¼‰ï¼š\n",
    "       t_i,1 = t_i,0 + h\n",
    "       å…¶ä¸­ h æ˜¯æŒæœ‰æœŸï¼ˆæœŸæ•¸ï¼‰\n",
    "    \n",
    "    4. ç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼š\n",
    "       t_i,1 = min{PT_touch, SL_touch, t_i,0 + h}\n",
    "    \n",
    "    5. æ¨™ç±¤ç”Ÿæˆï¼š\n",
    "       - bin = 1:  è§¸åŠä¸Šæ–¹å±éšœï¼ˆæˆåŠŸï¼‰\n",
    "       - bin = -1: è§¸åŠä¸‹æ–¹å±éšœï¼ˆå¤±æ•—ï¼‰\n",
    "       - bin = 0:  è§¸åŠå‚ç›´å±éšœï¼ˆæ™‚é–“åˆ°æœŸï¼‰\n",
    "    \n",
    "    è·¯å¾‘ä¾è³´æ€§ï¼š\n",
    "    ----------\n",
    "    ç‚ºäº†æ¨™è¨˜è§€æ¸¬å€¼ï¼Œå¿…é ˆè€ƒæ…®å¾ t_i,0 åˆ° t_i,0 + h çš„å®Œæ•´åƒ¹æ ¼è·¯å¾‘ã€‚\n",
    "    é€™ä½¿å¾—æ¨™ç±¤æ˜¯è·¯å¾‘ä¾è³´çš„ï¼Œè€Œéåƒ…ä¾è³´çµ‚é»åƒ¹æ ¼ã€‚\n",
    "    \n",
    "    å‹•æ…‹é–¾å€¼ï¼š\n",
    "    --------\n",
    "    ä½¿ç”¨ getDailyVol() è¨ˆç®—æ—¥æ³¢å‹•ç‡ï¼Œä½œç‚ºå‹•æ…‹èª¿æ•´æ­¢ç›ˆæ­¢æçš„åŸºç¤ï¼š\n",
    "    \n",
    "    Ïƒ_t = EWM_std(returns, span=span0)\n",
    "    \n",
    "    å…¶ä¸­ returns æ˜¯æ—¥æ”¶ç›Šç‡åºåˆ—ã€‚\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Advances in Financial Machine Learning, Chapter 3\n",
    "    - Snippet 3.1: getDailyVol - è¨ˆç®—æ—¥æ³¢å‹•ç‡\n",
    "    - Snippet 3.2: applyPtSlOnT1 - Triple Barrier æ¨™ç±¤æ–¹æ³•\n",
    "    - Snippet 3.3: getEvents - å–å¾—ç¬¬ä¸€å€‹å±éšœè§¸åŠæ™‚é–“\n",
    "    - Snippet 3.4: getVerticalBarriers - è¨­å®šå‚ç›´å±éšœ\n",
    "    - Snippet 3.5: getBins - ç”Ÿæˆæ¨™ç±¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ptSl: List[float] = [1, 2], numPeriods: int = 100, \n",
    "                 minRet: float = 0.01, min_label_pct: float = 0.05):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        ptSl : list\n",
    "            [æ­¢ç›ˆå€æ•¸, æ­¢æå€æ•¸]\n",
    "            - ptSl[0]: ä¸Šæ–¹å±éšœçš„å€æ•¸ï¼ˆProfit Takingï¼‰\n",
    "            - ptSl[1]: ä¸‹æ–¹å±éšœçš„å€æ•¸ï¼ˆStop Lossï¼‰\n",
    "            - å¦‚æœç‚º 0ï¼Œå‰‡è©²å±éšœä¸å•Ÿç”¨\n",
    "        numPeriods : int\n",
    "            å‚ç›´å±éšœæœŸæ•¸ï¼ˆä¾‹å¦‚ï¼š100 æ ¹ K ç·šï¼‰\n",
    "            å°æ‡‰è«–æ–‡ä¸­çš„ hï¼ˆæŒæœ‰æœŸï¼‰\n",
    "        minRet : float\n",
    "            æœ€å°æ³¢å‹•ç‡é–¾å€¼\n",
    "            åªæœ‰ç•¶ trgt > minRet æ™‚æ‰æœƒç”Ÿæˆäº‹ä»¶\n",
    "        min_label_pct : float\n",
    "            æœ€å°æ¨™ç±¤æ¯”ä¾‹ï¼ˆç”¨æ–¼ dropLabelsï¼‰\n",
    "            ä½æ–¼æ­¤æ¯”ä¾‹çš„æ¨™ç±¤æœƒè¢«ç§»é™¤\n",
    "        \"\"\"\n",
    "        self.ptSl = ptSl\n",
    "        self.numPeriods = numPeriods\n",
    "        self.minRet = minRet\n",
    "        self.min_label_pct = min_label_pct\n",
    "        self.events = None\n",
    "        self.bins = None\n",
    "        \n",
    "    def get_vertical_barriers(self, close: pd.Series, tEvents: pd.DatetimeIndex) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨­å®šå‚ç›´æ™‚é–“å±éšœï¼ˆä»¥æœŸæ•¸ç‚ºå–®ä½ï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼ˆåŸºæ–¼ Snippet 3.4ï¼‰ï¼š\n",
    "        ----------------------------\n",
    "        å°æ¯å€‹äº‹ä»¶æ™‚é–“é» t_i,0ï¼Œæ‰¾åˆ° numPeriods æœŸå¾Œçš„æ™‚é–“é»ã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        t_i,1 = close.index[position(t_i,0) + numPeriods]\n",
    "        \n",
    "        å¦‚æœè¶…å‡ºæ•¸æ“šç¯„åœï¼Œå‰‡ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“é»ã€‚\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        t1 : pd.Series\n",
    "            å‚ç›´å±éšœæ™‚é–“é»ï¼ˆindex=äº‹ä»¶èµ·é», value=å±éšœæ™‚é–“é»ï¼‰\n",
    "        \"\"\"\n",
    "        t1 = []\n",
    "        close_index = close.index\n",
    "        \n",
    "        for tEvent in tEvents:\n",
    "            # æ‰¾åˆ° tEvent åœ¨ close ä¸­çš„ä½ç½®\n",
    "            try:\n",
    "                event_pos = close_index.get_loc(tEvent)\n",
    "                # è¨ˆç®— numPeriods æœŸå¾Œçš„ä½ç½®\n",
    "                barrier_pos = event_pos + self.numPeriods\n",
    "                \n",
    "                # ç¢ºä¿ä¸è¶…éæ•¸æ“šç¯„åœ\n",
    "                if barrier_pos < len(close_index):\n",
    "                    t1.append(close_index[barrier_pos])\n",
    "                else:\n",
    "                    # å¦‚æœè¶…å‡ºç¯„åœï¼Œä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“é»\n",
    "                    t1.append(close_index[-1])\n",
    "            except KeyError:\n",
    "                # å¦‚æœæ‰¾ä¸åˆ°è©²æ™‚é–“é»ï¼Œè·³é\n",
    "                continue\n",
    "        \n",
    "        if len(t1) == 0:\n",
    "            return pd.Series(dtype='datetime64[ns]')\n",
    "        \n",
    "        # ç¢ºä¿ t1 çš„ç´¢å¼•èˆ‡ tEvents å°æ‡‰\n",
    "        t1_series = pd.Series(t1, index=tEvents[:len(t1)])\n",
    "        return t1_series\n",
    "    \n",
    "    def apply_pt_sl_on_t1(self, close: pd.Series, events: pd.DataFrame, \n",
    "                          molecule: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        å¥—ç”¨æ­¢ç›ˆæ­¢æï¼ˆåŸºæ–¼ Snippet 3.2ï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ï¼Œè¨ˆç®—å¾äº‹ä»¶é–‹å§‹åˆ°å‚ç›´å±éšœæœŸé–“çš„åƒ¹æ ¼è·¯å¾‘ï¼š\n",
    "        \n",
    "        path_prices = close[loc:t1]\n",
    "        path_returns = (path_prices / close[loc] - 1) Ã— side\n",
    "        \n",
    "        ç„¶å¾Œæ‰¾å‡ºï¼š\n",
    "        - æœ€æ—©è§¸åŠæ­¢ç›ˆçš„æ™‚é–“ï¼špath_returns > pt Ã— trgt\n",
    "        - æœ€æ—©è§¸åŠæ­¢æçš„æ™‚é–“ï¼špath_returns < -sl Ã— trgt\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        - PT_level = pt Ã— trgt\n",
    "        - SL_level = -sl Ã— trgt\n",
    "        - path_returns = (close[t] / close[loc] - 1) Ã— side\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        events : pd.DataFrame\n",
    "            äº‹ä»¶ DataFrameï¼ˆåŒ…å« t1, trgt, sideï¼‰\n",
    "        molecule : pd.DatetimeIndex\n",
    "            è¦è™•ç†çš„äº‹ä»¶å­é›†ï¼ˆç”¨æ–¼ä¸¦è¡Œè¨ˆç®—ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        out : pd.DataFrame\n",
    "            åŒ…å«æ¯å€‹å±éšœè§¸åŠæ™‚é–“çš„ DataFrame\n",
    "        \"\"\"\n",
    "        events_ = events.loc[molecule]\n",
    "        out = events_[['t1']].copy(deep=True)\n",
    "        \n",
    "        # è¨ˆç®—æ­¢ç›ˆæ°´å¹³\n",
    "        if self.ptSl[0] > 0:\n",
    "            pt = self.ptSl[0] * events_['trgt']  # PT = pt Ã— Ïƒ\n",
    "        else:\n",
    "            pt = pd.Series(index=events.index)  # æœªå•Ÿç”¨\n",
    "        \n",
    "        # è¨ˆç®—æ­¢ææ°´å¹³\n",
    "        if self.ptSl[1] > 0:\n",
    "            sl = -self.ptSl[1] * events_['trgt']  # SL = -sl Ã— Ïƒ\n",
    "        else:\n",
    "            sl = pd.Series(index=events.index)  # æœªå•Ÿç”¨\n",
    "        \n",
    "        # å°æ¯å€‹äº‹ä»¶è¨ˆç®—è·¯å¾‘\n",
    "        for loc, t1 in events_['t1'].fillna(close.index[-1]).items():\n",
    "            # å–å¾—åƒ¹æ ¼è·¯å¾‘\n",
    "            df0 = close[loc:t1]\n",
    "            # è¨ˆç®—è·¯å¾‘å ±é…¬ï¼ˆè€ƒæ…®æ–¹å‘ï¼‰\n",
    "            df0 = (df0 / close[loc] - 1) * events_.at[loc, 'side']\n",
    "            \n",
    "            # æ‰¾å‡ºæœ€æ—©è§¸åŠæ­¢æçš„æ™‚é–“\n",
    "            out.loc[loc, 'sl'] = df0[df0 < sl[loc]].index.min()\n",
    "            # æ‰¾å‡ºæœ€æ—©è§¸åŠæ­¢ç›ˆçš„æ™‚é–“\n",
    "            out.loc[loc, 'pt'] = df0[df0 > pt[loc]].index.min()\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_events(self, close: pd.Series, tEvents: pd.DatetimeIndex, \n",
    "                   trgt: pd.Series, side: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆäº‹ä»¶ä¸¦å¥—ç”¨ Triple Barrierï¼ˆåŸºæ–¼ Snippet 3.3ï¼‰\n",
    "        \n",
    "        å¯¦ç¾æµç¨‹ï¼š\n",
    "        ---------\n",
    "        1. éæ¿¾ç›®æ¨™ï¼šåªä¿ç•™ trgt > minRet çš„äº‹ä»¶\n",
    "        2. è¨­å®šå‚ç›´å±éšœï¼šè¨ˆç®—æ¯å€‹äº‹ä»¶çš„æ™‚é–“åˆ°æœŸé»\n",
    "        3. å¥—ç”¨ Triple Barrierï¼šè¨ˆç®—æ­¢ç›ˆæ­¢æè§¸åŠæ™‚é–“\n",
    "        4. æ‰¾å‡ºç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼št1 = min{pt_touch, sl_touch, vertical_barrier}\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»ï¼ˆä¾†è‡ª CUSUM éæ¿¾æˆ–ç­–ç•¥ä¿¡è™Ÿï¼‰\n",
    "        trgt : pd.Series\n",
    "            ç›®æ¨™æ³¢å‹•ç‡ï¼ˆdaily_volï¼Œç”¨æ–¼å‹•æ…‹èª¿æ•´æ­¢ç›ˆæ­¢æï¼‰\n",
    "        side : pd.Series\n",
    "            æ–¹å‘ï¼ˆ1 ç‚ºåšå¤šï¼Œ-1 ç‚ºåšç©ºï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        events : pd.DataFrame\n",
    "            åŒ…å« t1, trgt, side, pt, sl çš„äº‹ä»¶ DataFrame\n",
    "            - t1: ç¬¬ä¸€å€‹å±éšœè§¸åŠæ™‚é–“\n",
    "            - trgt: ç›®æ¨™æ³¢å‹•ç‡\n",
    "            - side: æ–¹å‘\n",
    "            - pt: æ­¢ç›ˆå€æ•¸\n",
    "            - sl: æ­¢æå€æ•¸\n",
    "        \"\"\"\n",
    "        # 1. éæ¿¾ç›®æ¨™ï¼ˆåªä¿ç•™æ³¢å‹•ç‡è¶³å¤ å¤§çš„äº‹ä»¶ï¼‰\n",
    "        trgt = trgt.loc[tEvents]\n",
    "        trgt = trgt[trgt > self.minRet]\n",
    "        \n",
    "        # 2. è¨­å®šå‚ç›´å±éšœ\n",
    "        t1 = self.get_vertical_barriers(close, tEvents)\n",
    "        \n",
    "        # 3. å–å¾—æ–¹å‘\n",
    "        side_ = side.loc[trgt.index]\n",
    "        \n",
    "        # 4. çµ„åˆäº‹ä»¶ç‰©ä»¶\n",
    "        events = pd.concat({\n",
    "            't1': t1.loc[trgt.index], \n",
    "            'trgt': trgt, \n",
    "            'side': side_\n",
    "        }, axis=1)\n",
    "        events = events.dropna(subset=['trgt'])\n",
    "        \n",
    "        # 5. è¨ˆç®—æ­¢ç›ˆæ­¢æè§¸åŠæ™‚é–“\n",
    "        df0 = self.apply_pt_sl_on_t1(close, events, events.index)\n",
    "        \n",
    "        # 6. æ‰¾å‡ºç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼ˆæœ€æ—©çš„æ™‚é–“ï¼‰\n",
    "        events['t1'] = df0.dropna(how='all').min(axis=1)\n",
    "        \n",
    "        # 7. å„²å­˜å±éšœé…ç½®\n",
    "        events['pt'] = self.ptSl[0]\n",
    "        events['sl'] = self.ptSl[1]\n",
    "        \n",
    "        self.events = events\n",
    "        return events\n",
    "    \n",
    "    def get_bins(self, close: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ¨™ç±¤ï¼ˆåŸºæ–¼ Snippet 3.5ï¼Œä¸¦æ“´å±•ç‚ºæ”¯æ´ Meta-Labelingï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        1. å°é½Šåƒ¹æ ¼ï¼šå–å¾—äº‹ä»¶é–‹å§‹å’ŒçµæŸæ™‚é–“çš„æ‰€æœ‰åƒ¹æ ¼é»\n",
    "        2. è¨ˆç®—å¯¦éš›å ±é…¬ï¼š\n",
    "           ret = (close[t1] / close[t0] - 1) Ã— side\n",
    "        3. åˆ¤æ–·è§¸åŠçš„å±éšœï¼š\n",
    "           - å¦‚æœ ret > 0 ä¸” ret > pt Ã— trgt â†’ bin = 1ï¼ˆè§¸åŠæ­¢ç›ˆï¼‰\n",
    "           - å¦‚æœ ret < 0 ä¸” ret < -sl Ã— trgt â†’ bin = -1ï¼ˆè§¸åŠæ­¢æï¼‰\n",
    "           - å¦å‰‡ â†’ bin = 0ï¼ˆè§¸åŠå‚ç›´å±éšœï¼‰\n",
    "        \n",
    "        æ¨™ç±¤å®šç¾©ï¼š\n",
    "        --------\n",
    "        - bin = 1:  æˆåŠŸï¼ˆè§¸åŠæ­¢ç›ˆå±éšœï¼‰\n",
    "        - bin = 0:  æ™‚é–“åˆ°æœŸï¼ˆè§¸åŠå‚ç›´å±éšœï¼‰\n",
    "        - bin = -1: è™§æï¼ˆè§¸åŠæ­¢æå±éšœï¼‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        bins : pd.DataFrame\n",
    "            åŒ…å« ret, trgt, bin, side çš„æ¨™ç±¤ DataFrame\n",
    "        \"\"\"\n",
    "        if self.events is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_events()\")\n",
    "            return None\n",
    "        \n",
    "        events_ = self.events.dropna(subset=['t1'])\n",
    "        px = events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "        px = close.reindex(px, method='bfill')\n",
    "        \n",
    "        out = pd.DataFrame(index=events_.index)\n",
    "        out['ret'] = px.loc[events_['t1'].values].values / px.loc[events_.index] - 1\n",
    "        out['ret'] *= events_['side']\n",
    "        out['trgt'] = events_['trgt']\n",
    "        \n",
    "        # åˆ¤æ–·è§¸åŠå“ªå€‹å±éšœ\n",
    "        out['bin'] = 0  # é è¨­ç‚ºæ™‚é–“åˆ°æœŸ\n",
    "        for date_time, values in out.iterrows():\n",
    "            ret = values['ret']\n",
    "            target = values['trgt']\n",
    "            pt_level = ret > target * self.events.loc[date_time, 'pt']\n",
    "            sl_level = ret < -target * self.events.loc[date_time, 'sl']\n",
    "            \n",
    "            if ret > 0.0 and pt_level:\n",
    "                # è§¸åŠæ­¢ç›ˆå±éšœ â†’ bin = 1 (æˆåŠŸ)\n",
    "                out.loc[date_time, 'bin'] = 1\n",
    "            elif ret < 0.0 and sl_level:\n",
    "                # è§¸åŠæ­¢æå±éšœ â†’ bin = -1 (è™§æ)\n",
    "                out.loc[date_time, 'bin'] = -1\n",
    "            else:\n",
    "                # æ™‚é–“åˆ°æœŸï¼ˆå‚ç›´å±éšœï¼‰â†’ bin = 0\n",
    "                out.loc[date_time, 'bin'] = 0\n",
    "        \n",
    "        # å¦‚æœæœ‰ sideï¼ˆMeta-Labelingï¼‰ï¼Œä¿ç•™ side è³‡è¨Š\n",
    "        if 'side' in events_:\n",
    "            out['side'] = events_['side']\n",
    "        \n",
    "        self.bins = out\n",
    "        return out\n",
    "    \n",
    "    def drop_rare_labels(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç§»é™¤ç¨€æœ‰æ¨™ç±¤ï¼ˆåŸºæ–¼ Snippet 3.8ï¼‰\n",
    "        \n",
    "        ç›®çš„ï¼š\n",
    "        -----\n",
    "        ç§»é™¤æ¨£æœ¬æ•¸éå°‘çš„æ¨™ç±¤é¡åˆ¥ï¼Œé¿å…æ¨¡å‹å­¸ç¿’åæ–œã€‚\n",
    "        \n",
    "        æµç¨‹ï¼š\n",
    "        -----\n",
    "        1. è¨ˆç®—æ¯å€‹æ¨™ç±¤çš„æ¯”ä¾‹\n",
    "        2. å¦‚æœæœ€å°æ¯”ä¾‹ < min_label_pctï¼Œç§»é™¤è©²æ¨™ç±¤\n",
    "        3. é‡è¤‡ç›´åˆ°æ‰€æœ‰æ¨™ç±¤æ¯”ä¾‹éƒ½ >= min_label_pct\n",
    "        \"\"\"\n",
    "        if self.bins is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_bins()\")\n",
    "            return None\n",
    "        \n",
    "        bins = self.bins.copy()\n",
    "        \n",
    "        while True:\n",
    "            df0 = bins['bin'].value_counts(normalize=True)\n",
    "            if df0.min() > self.min_label_pct or df0.shape[0] < 3:\n",
    "                break\n",
    "            print(f\"Dropped label {df0.idxmin()}, percentage: {df0.min():.4f}\")\n",
    "            bins = bins[bins['bin'] != df0.idxmin()]\n",
    "        \n",
    "        self.bins = bins\n",
    "        self.events = self.events.loc[bins.index]\n",
    "        \n",
    "        return bins\n",
    "    \n",
    "    def get_label_stats(self) -> dict:\n",
    "        \"\"\"å–å¾—æ¨™ç±¤çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if self.bins is None:\n",
    "            return None\n",
    "        \n",
    "        label_counts = self.bins['bin'].value_counts()\n",
    "        \n",
    "        return {\n",
    "            'total_labels': len(self.bins),\n",
    "            'label_distribution': label_counts.to_dict(),\n",
    "            'avg_return': self.bins['ret'].mean(),\n",
    "            'win_rate': (self.bins['bin'] == 1).sum() / len(self.bins),\n",
    "            'loss_rate': (self.bins['bin'] == -1).sum() / len(self.bins),\n",
    "            'timeout_rate': (self.bins['bin'] == 0).sum() / len(self.bins)\n",
    "        }\n",
    "    \n",
    "    def plot_labels(self):\n",
    "        \"\"\"ç¹ªè£½æ¨™ç±¤åˆ†å¸ƒåœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.bins is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_bins()\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # æ¨™ç±¤åˆ†å¸ƒ\n",
    "        label_counts = self.bins['bin'].value_counts().sort_index()\n",
    "        colors = {-1: 'red', 0: 'gray', 1: 'green'}\n",
    "        label_colors = [colors.get(idx, 'steelblue') for idx in label_counts.index]\n",
    "        \n",
    "        label_counts.plot(kind='bar', ax=axes[0], color=label_colors)\n",
    "        axes[0].set_title('Label Distribution', fontsize=12)\n",
    "        axes[0].set_xlabel('Label (-1: Loss, 0: Timeout, 1: Win)')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # å›å ±åˆ†å¸ƒ\n",
    "        self.bins['ret'].hist(bins=50, ax=axes[1], alpha=0.7, color='steelblue')\n",
    "        axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Return')\n",
    "        axes[1].set_title('Return Distribution', fontsize=12)\n",
    "        axes[1].set_xlabel('Return')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5226ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Meta-Labelingï¼ˆä½¿ç”¨åŸå§‹å®Œæ•´æ•¸æ“šï¼‰\n",
    "meta = MetaLabeling(ptSl=[0.3, 0.3], numPeriods=16)\n",
    "events = meta.get_events(\n",
    "    nas100_raw['Close'],  # âœ… åŸå§‹å®Œæ•´æ•¸æ“š\n",
    "    tEvents,              # ç­–ç•¥ä¿¡è™Ÿçš„äº‹ä»¶é»\n",
    "    daily_vol,            # åŸºæ–¼åŸå§‹æ•¸æ“šè¨ˆç®—\n",
    "    df['side']            # ç­–ç•¥ä¿¡è™Ÿçš„æ–¹å‘ï¼ˆä¾†è‡ªéæ¿¾å¾Œçš„æ•¸æ“šï¼‰\n",
    ")\n",
    "bins = meta.get_bins(nas100_raw['Close'])  # âœ… åŸå§‹å®Œæ•´æ•¸æ“š\n",
    "meta.plot_labels()\n",
    "meta.drop_rare_labels()\n",
    "meta.get_label_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e442d",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ4 ï¼š æ¨£æœ¬æ¬Šé‡å¹³è¡¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c766862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4ï¸âƒ£ æ¨£æœ¬æ¬Šé‡ Classï¼ˆä¸¦ç™¼åº¦ã€å”¯ä¸€æ€§ã€æ™‚é–“è¡°æ¸›ï¼‰\n",
    "# =====================================================\n",
    "class SampleWeight:\n",
    "    \"\"\"\n",
    "    æ¨£æœ¬æ¬Šé‡è¨ˆç®—ï¼šä¸¦ç™¼åº¦ã€å”¯ä¸€æ€§ã€æ™‚é–“è¡°æ¸›\n",
    "    \n",
    "    Chapter 4: Sample Weights èªªæ˜\n",
    "    ===============================\n",
    "    \n",
    "    å•é¡ŒèƒŒæ™¯ï¼š\n",
    "    --------\n",
    "    åœ¨é‡‘èæ‡‰ç”¨ä¸­ï¼Œè§€æ¸¬å€¼ä¸æ˜¯ç”±ç¨ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰éç¨‹ç”Ÿæˆçš„ã€‚\n",
    "    ç•¶å…©å€‹æ¨™ç±¤ y_i å’Œ y_j çš„æ™‚é–“å€é–“æœ‰é‡ç–Šæ™‚ï¼ˆt_i,1 > t_j,0ï¼‰ï¼Œ\n",
    "    å®ƒå€‘æœƒä¾è³´å…±åŒçš„å ±é…¬ r_{t_j,0, min{t_i,1, t_j,1}}ã€‚\n",
    "    \n",
    "    é€™å°è‡´æ¨™ç±¤åºåˆ— {y_i}_{i=1,...,I} ä¸æ˜¯ IIDï¼Œé•åäº†å¤§å¤šæ•¸ ML ç®—æ³•çš„å‡è¨­ã€‚\n",
    "    \n",
    "    è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "    --------\n",
    "    é€šéè¨­è¨ˆæ¡æ¨£å’ŒåŠ æ¬Šæ–¹æ¡ˆä¾†ç³¾æ­£é‡ç–Šçµæœçš„ä¸ç•¶å½±éŸ¿ï¼š\n",
    "    1. è¨ˆç®—ä¸¦ç™¼åº¦ï¼ˆConcurrencyï¼‰ï¼šæ¯å€‹æ™‚é–“é»æœ‰å¤šå°‘æ¨™ç±¤åŒæ™‚å­˜æ´»\n",
    "    2. è¨ˆç®—å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰ï¼šæ¯å€‹æ¨™ç±¤çš„å¹³å‡å”¯ä¸€æ€§\n",
    "    3. æ‡‰ç”¨æ™‚é–“è¡°æ¸›ï¼ˆTime Decayï¼‰ï¼šè®“èˆŠæ¨£æœ¬æ¬Šé‡é™ä½\n",
    "    4. çµ„åˆæ¬Šé‡ï¼šæœ€çµ‚æ¬Šé‡ = å”¯ä¸€æ€§ Ã— æ™‚é–“è¡°æ¸›ï¼ˆå¯é¸ï¼šÃ— å ±é…¬æ­¸å› ï¼‰\n",
    "    \n",
    "    ä¸¦ç™¼åº¦ï¼ˆConcurrencyï¼‰ï¼š\n",
    "    ---------------------\n",
    "    å®šç¾©ï¼šå…©å€‹æ¨™ç±¤ y_i å’Œ y_j åœ¨æ™‚é–“ t æ˜¯ä¸¦ç™¼çš„ï¼Œç•¶å®ƒå€‘éƒ½ä¾è³´è‡³å°‘ä¸€å€‹\n",
    "    å…±åŒçš„å ±é…¬ r_{t-1,t} = p_t / p_{t-1} - 1ã€‚\n",
    "    \n",
    "    è¨ˆç®—æ–¹å¼ï¼š\n",
    "    å°æ¯å€‹æ™‚é–“é» t = 1,...,Tï¼Œå½¢æˆäºŒå…ƒé™£åˆ— {1_{t,i}}_{i=1,...,I}ï¼š\n",
    "    - 1_{t,i} = 1ï¼šå¦‚æœ [t_i,0, t_i,1] èˆ‡ [t-1, t] é‡ç–Š\n",
    "    - 1_{t,i} = 0ï¼šå¦å‰‡\n",
    "    \n",
    "    ä¸¦ç™¼åº¦ï¼šc_t = Î£_{i=1}^I 1_{t,i}\n",
    "    \n",
    "    å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰ï¼š\n",
    "    --------------------\n",
    "    å®šç¾©ï¼šæ¨™ç±¤ i åœ¨æ™‚é–“ t çš„å”¯ä¸€æ€§ç‚º u_{t,i} = 1_{t,i} / c_t\n",
    "    \n",
    "    å¹³å‡å”¯ä¸€æ€§ï¼šÅ«_i = (Î£_{t=1}^T 1_{t,i})^{-1} Ã— (Î£_{t=1}^T u_{t,i})\n",
    "    \n",
    "    ä¹Ÿå¯ä»¥è§£é‡‹ç‚ºï¼šæ¨™ç±¤ i å­˜æ´»æœŸé–“çš„ä¸¦ç™¼åº¦å€’æ•¸çš„èª¿å’Œå¹³å‡ã€‚\n",
    "    \n",
    "    å ±é…¬æ­¸å› ï¼ˆReturn Attributionï¼‰ï¼š\n",
    "    -----------------------------\n",
    "    ç•¶æ¨™ç±¤æ˜¯å ±é…¬ç¬¦è™Ÿçš„å‡½æ•¸æ™‚ï¼ˆ{-1,1} æˆ– {0,1}ï¼‰ï¼Œæ¨£æœ¬æ¬Šé‡å¯ä»¥å®šç¾©ç‚ºï¼š\n",
    "    \n",
    "    Ìƒw_i = |Î£_{t=t_i,0}^{t_i,1} (r_{t-1,t} / c_t)|\n",
    "    \n",
    "    å…¶ä¸­ r_{t-1,t} æ˜¯å°æ•¸å ±é…¬ï¼Œc_t æ˜¯æ™‚é–“ t çš„ä¸¦ç™¼åº¦ã€‚\n",
    "    \n",
    "    ç„¶å¾Œæ¨™æº–åŒ–ï¼šw_i = Ìƒw_i Ã— I / (Î£_{j=1}^I Ìƒw_j)\n",
    "    \n",
    "    æ™‚é–“è¡°æ¸›ï¼ˆTime Decayï¼‰ï¼š\n",
    "    ----------------------\n",
    "    å¸‚å ´æ˜¯é©æ‡‰æ€§ç³»çµ±ï¼ŒèˆŠæ¨£æœ¬ä¸å¦‚æ–°æ¨£æœ¬ç›¸é—œã€‚\n",
    "    \n",
    "    ç·šæ€§æ™‚é–“è¡°æ¸›ï¼š\n",
    "    d[x] = max{0, a + bx}\n",
    "    \n",
    "    é‚Šç•Œæ¢ä»¶ï¼š\n",
    "    1. d[Î£_{i=1}^I Å«_i] = 1ï¼ˆæœ€æ–°æ¨£æœ¬æ¬Šé‡ç‚º 1ï¼‰\n",
    "    2. d[0] = cï¼ˆæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º cï¼Œc âˆˆ [0,1]ï¼‰\n",
    "    \n",
    "    åƒæ•¸ c çš„æ„ç¾©ï¼š\n",
    "    - c = 1ï¼šç„¡æ™‚é–“è¡°æ¸›\n",
    "    - 0 < c < 1ï¼šç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\n",
    "    - c = 0ï¼šæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "    - c < 0ï¼šæœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "    \n",
    "    æ³¨æ„ï¼šæ™‚é–“è¡°æ¸›æ˜¯åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ï¼Œè€Œéæ™‚é–“é †åºï¼Œå› ç‚ºåœ¨å­˜åœ¨å†—é¤˜\n",
    "    è§€æ¸¬å€¼çš„æƒ…æ³ä¸‹ï¼ŒæŒ‰æ™‚é–“é †åºè¡°æ¸›æœƒä½¿æ¬Šé‡é™ä½å¤ªå¿«ã€‚\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Advances in Financial Machine Learning, Chapter 4\n",
    "    - Snippet 4.1: mpNumCoEvents - è¨ˆç®—ä¸¦ç™¼åº¦\n",
    "    - Snippet 4.2: mpSampleTW - è¨ˆç®—å¹³å‡å”¯ä¸€æ€§\n",
    "    - Snippet 4.10: mpSampleW - å ±é…¬æ­¸å› æ¬Šé‡\n",
    "    - Snippet 4.11: getTimeDecay - æ™‚é–“è¡°æ¸›å› å­\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_co_events = None\n",
    "        self.uniqueness = None\n",
    "        self.time_decay = None\n",
    "        self.sample_weights = None\n",
    "        \n",
    "    def compute_num_co_events(self, close_idx: pd.DatetimeIndex, \n",
    "                              t1: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸ï¼ˆConcurrencyï¼‰\n",
    "        åŸºæ–¼ Snippet 4.1: mpNumCoEvents\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹æ™‚é–“é» tï¼Œè¨ˆç®—æœ‰å¤šå°‘å€‹äº‹ä»¶çš„å­˜æ´»æœŸé–“ [t_i,0, t_i,1] \n",
    "        èˆ‡æ™‚é–“é» t é‡ç–Šã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        c_t = Î£_{i=1}^I 1_{t,i}\n",
    "        \n",
    "        å…¶ä¸­ 1_{t,i} = 1 å¦‚æœ [t_i,0, t_i,1] èˆ‡ [t-1, t] é‡ç–Šï¼Œå¦å‰‡ç‚º 0ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        close_idx : pd.DatetimeIndex\n",
    "            åƒ¹æ ¼æ•¸æ“šçš„ç´¢å¼•\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        num_co_events : pd.Series\n",
    "            æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸\n",
    "        \"\"\"\n",
    "        t1 = t1.fillna(close_idx[-1])  # æœªçµæŸäº‹ä»¶ä»è¦è¨ˆç®—\n",
    "        \n",
    "        # åˆå§‹åŒ–è¨ˆæ•¸åºåˆ—\n",
    "        iloc = close_idx.searchsorted(np.array([t1.index[0], t1.max()]))\n",
    "        count = pd.Series(0, index=close_idx[iloc[0]:iloc[1]+1])\n",
    "        \n",
    "        # å°æ¯å€‹äº‹ä»¶ï¼Œåœ¨å…¶å­˜æ´»æœŸé–“ +1\n",
    "        for tIn, tOut in t1.items():\n",
    "            count.loc[tIn:tOut] += 1.0\n",
    "        \n",
    "        self.num_co_events = count\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š ä¸¦ç™¼åº¦çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½æ™‚é–“é»æ•¸: {len(count):,}\")\n",
    "        print(f\"å¹³å‡ä¸¦ç™¼äº‹ä»¶æ•¸: {count.mean():.2f}\")\n",
    "        print(f\"æœ€å¤§ä¸¦ç™¼äº‹ä»¶æ•¸: {count.max():.0f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - ä¸¦ç™¼åº¦è¡¡é‡æ¯å€‹æ™‚é–“é»æœ‰å¤šå°‘å€‹æ¨™ç±¤åŒæ™‚å­˜æ´»\")\n",
    "        print(f\"  - é«˜ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ä¹‹é–“é‡ç–Šå¤šï¼Œè³‡è¨Šå†—é¤˜\")\n",
    "        print(f\"  - ä½ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ç›¸å°ç¨ç«‹\")\n",
    "        print(f\"ä¸¦ç™¼åº¦åˆ†å¸ƒ:\")\n",
    "        print(count.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return count\n",
    "\n",
    "    def compute_uniqueness(self, t1: pd.Series, \n",
    "                          num_co_events: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¯å€‹äº‹ä»¶çš„å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰\n",
    "        åŸºæ–¼ Snippet 4.2: mpSampleTW\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ iï¼Œè¨ˆç®—å…¶åœ¨å­˜æ´»æœŸé–“çš„å¹³å‡å”¯ä¸€æ€§ï¼š\n",
    "        \n",
    "        u_{t,i} = 1_{t,i} / c_t  ï¼ˆæ™‚é–“ t çš„å”¯ä¸€æ€§ï¼‰\n",
    "        Å«_i = (1 / T_i) Ã— Î£_{t=t_i,0}^{t_i,1} u_{t,i}\n",
    "        \n",
    "        å…¶ä¸­ T_i æ˜¯äº‹ä»¶ i çš„å­˜æ´»æœŸæ•¸ã€‚\n",
    "        \n",
    "        ä¹Ÿå¯ä»¥è§£é‡‹ç‚ºï¼šäº‹ä»¶å­˜æ´»æœŸé–“çš„ä¸¦ç™¼åº¦å€’æ•¸çš„èª¿å’Œå¹³å‡ã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        Å«_i = mean(1 / c_t) for t âˆˆ [t_i,0, t_i,1]\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "        num_co_events : pd.Series\n",
    "            æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        uniqueness : pd.Series\n",
    "            æ¯å€‹äº‹ä»¶çš„å”¯ä¸€æ€§ï¼ˆ1 / å¹³å‡ä¸¦ç™¼åº¦ï¼‰\n",
    "        \"\"\"\n",
    "        wght = pd.Series(index=t1.index)\n",
    "        \n",
    "        for tIn, tOut in t1.items():\n",
    "            # è¨ˆç®—äº‹ä»¶å­˜æ´»æœŸé–“çš„å¹³å‡ä¸¦ç™¼åº¦å€’æ•¸\n",
    "            avg_co_events = num_co_events.loc[tIn:tOut].mean()\n",
    "            wght.loc[tIn] = 1.0 / avg_co_events if avg_co_events > 0 else 0\n",
    "        \n",
    "        self.uniqueness = wght\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å”¯ä¸€æ€§çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½äº‹ä»¶æ•¸: {len(wght):,}\")\n",
    "        print(f\"å¹³å‡å”¯ä¸€æ€§: {wght.mean():.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¡¡é‡æ¯å€‹æ¨™ç±¤çš„éé‡ç–Šç¨‹åº¦\")\n",
    "        print(f\"  - å”¯ä¸€æ€§ = 1 / å¹³å‡ä¸¦ç™¼åº¦\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¶Šé«˜ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤è¶Šç¨ç«‹ï¼Œæ¬Šé‡æ‡‰è©²è¶Šå¤§\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¶Šä½ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤èˆ‡å…¶ä»–æ¨™ç±¤é‡ç–Šå¤šï¼Œæ¬Šé‡æ‡‰è©²è¶Šå°\")\n",
    "        print(f\"å”¯ä¸€æ€§åˆ†å¸ƒ:\")\n",
    "        print(wght.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return wght\n",
    "\n",
    "    def compute_return_attribution(self, \n",
    "                                  close: pd.Series,\n",
    "                                  t1: pd.Series,\n",
    "                                  num_co_events: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆReturn Attributionï¼‰\n",
    "        åŸºæ–¼ Snippet 4.10: mpSampleW\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ï¼Œè¨ˆç®—å…¶å­˜æ´»æœŸé–“çš„æ­¸å› å ±é…¬ï¼š\n",
    "        \n",
    "        Ìƒw_i = |Î£_{t=t_i,0}^{t_i,1} (r_{t-1,t} / c_t)|\n",
    "        \n",
    "        å…¶ä¸­ï¼š\n",
    "        - r_{t-1,t} = log(close_t / close_{t-1}) æ˜¯å°æ•¸å ±é…¬\n",
    "        - c_t æ˜¯æ™‚é–“ t çš„ä¸¦ç™¼åº¦\n",
    "        \n",
    "        ç„¶å¾Œæ¨™æº–åŒ–ï¼šw_i = Ìƒw_i Ã— I / (Î£_{j=1}^I Ìƒw_j)\n",
    "        \n",
    "        é€™æ¨£è¨­è¨ˆçš„æ„ç¾©ï¼š\n",
    "        - å°‡å ±é…¬æŒ‰ä¸¦ç™¼åº¦åˆ†é…ï¼Œé¿å…é‡ç–Šäº‹ä»¶é‡è¤‡è¨ˆç®—å ±é…¬\n",
    "        - çµ•å°å ±é…¬å¤§çš„äº‹ä»¶æ¬Šé‡æ›´é«˜\n",
    "        - æ¨™æº–åŒ–å¾Œæ¬Šé‡ç¸½å’Œç‚º Iï¼ˆæ¨£æœ¬æ•¸ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“\n",
    "        num_co_events : pd.Series\n",
    "            ä¸¦ç™¼åº¦åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        return_weights : pd.Series\n",
    "            å ±é…¬æ­¸å› æ¬Šé‡\n",
    "        \"\"\"\n",
    "        ret = np.log(close).diff()  # å°æ•¸å ±é…¬\n",
    "        wght = pd.Series(index=t1.index)\n",
    "        \n",
    "        for tIn, tOut in t1.items():\n",
    "            # è¨ˆç®—æ­¸å› å ±é…¬ï¼šå ±é…¬ / ä¸¦ç™¼åº¦\n",
    "            attributed_ret = (ret.loc[tIn:tOut] / \n",
    "                            num_co_events.loc[tIn:tOut]).sum()\n",
    "            wght.loc[tIn] = abs(attributed_ret)\n",
    "        \n",
    "        # æ¨™æº–åŒ–ï¼šæ¬Šé‡ç¸½å’Œ = æ¨£æœ¬æ•¸\n",
    "        wght = wght * len(wght) / wght.sum()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å ±é…¬æ­¸å› æ¬Šé‡çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½äº‹ä»¶æ•¸: {len(wght):,}\")\n",
    "        print(f\"å¹³å‡æ¬Šé‡: {wght.mean():.4f}\")\n",
    "        print(f\"æ¬Šé‡ç¸½å’Œ: {wght.sum():.0f} (æ‡‰ç­‰æ–¼æ¨£æœ¬æ•¸)\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - æ ¹æ“šæ­¸å› çš„çµ•å°å ±é…¬ä¾†åŠ æ¬Š\")\n",
    "        print(f\"  - å ±é…¬æŒ‰ä¸¦ç™¼åº¦åˆ†é…ï¼Œé¿å…é‡ç–Šäº‹ä»¶é‡è¤‡è¨ˆç®—\")\n",
    "        print(f\"  - çµ•å°å ±é…¬å¤§çš„äº‹ä»¶æ¬Šé‡æ›´é«˜\")\n",
    "        print(f\"æ¬Šé‡åˆ†å¸ƒ:\")\n",
    "        print(wght.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return wght\n",
    "\n",
    "    def get_time_decay_linear(self, tW: pd.Series, \n",
    "                              clf_last_w: float = 1.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        ç·šæ€§æ™‚é–“è¡°æ¸›ï¼ˆåŸºæ–¼ Snippet 4.11: getTimeDecayï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        æ‡‰ç”¨åˆ†æ®µç·šæ€§è¡°æ¸›åˆ°è§€æ¸¬çš„å”¯ä¸€æ€§ï¼ˆtWï¼‰ï¼š\n",
    "        \n",
    "        d[x] = max{0, a + bx}\n",
    "        \n",
    "        é‚Šç•Œæ¢ä»¶ï¼š\n",
    "        1. d[Î£_{i=1}^I Å«_i] = 1  ï¼ˆæœ€æ–°æ¨£æœ¬æ¬Šé‡ç‚º 1ï¼‰\n",
    "        2. d[0] = clf_last_w    ï¼ˆæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º clf_last_wï¼‰\n",
    "        \n",
    "        æ±‚è§£ï¼š\n",
    "        a = 1 - b Ã— Î£_{i=1}^I Å«_i\n",
    "        b = (1 - clf_last_w) / Î£_{i=1}^I Å«_i  ï¼ˆç•¶ clf_last_w >= 0ï¼‰\n",
    "        \n",
    "        æ³¨æ„ï¼šæ™‚é–“è¡°æ¸›æ˜¯åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ x âˆˆ [0, Î£_{i=1}^I Å«_i]ï¼Œ\n",
    "        è€Œéæ™‚é–“é †åºï¼Œå› ç‚ºåœ¨å­˜åœ¨å†—é¤˜è§€æ¸¬å€¼çš„æƒ…æ³ä¸‹ï¼ŒæŒ‰æ™‚é–“é †åº\n",
    "        è¡°æ¸›æœƒä½¿æ¬Šé‡é™ä½å¤ªå¿«ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        tW : pd.Series\n",
    "            å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé€šå¸¸æ˜¯ uniquenessï¼‰\n",
    "        clf_last_w : float\n",
    "            æœ€èˆŠæ¨£æœ¬çš„æ¬Šé‡ï¼ˆé è¨­ 1.0ï¼Œè¡¨ç¤ºä¸è¡°æ¸›ï¼‰\n",
    "            - c = 1: ç„¡æ™‚é–“è¡°æ¸›\n",
    "            - 0 < c < 1: ç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\n",
    "            - c = 0: æœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "            - c < 0: æœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        time_decay : pd.Series\n",
    "            æ™‚é–“è¡°æ¸›æ¬Šé‡\n",
    "        \"\"\"\n",
    "        clfW = tW.sort_index().cumsum()\n",
    "        \n",
    "        if clfW.iloc[-1] > 0:\n",
    "            if clf_last_w >= 0:\n",
    "                # ç·šæ€§è¡°æ¸›ï¼šd[0] = clf_last_w, d[Î£Å«] = 1\n",
    "                slope = (1.0 - clf_last_w) / clfW.iloc[-1]\n",
    "            else:\n",
    "                # ç•¶ clf_last_w < 0 æ™‚çš„ç‰¹æ®Šè™•ç†\n",
    "                slope = 1 / ((clf_last_w + 1) * clfW.iloc[-1])\n",
    "        else:\n",
    "            slope = 0\n",
    "        \n",
    "        const = 1.0 - slope * clfW.iloc[-1]\n",
    "        clfW = const + slope * clfW\n",
    "        clfW[clfW < 0] = 0\n",
    "        \n",
    "        self.time_decay = clfW\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š ç·šæ€§æ™‚é–“è¡°æ¸›\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"æ™‚é–“è¡°æ¸›åƒæ•¸: const={const:.4f}, slope={slope:.6f}\")\n",
    "        print(f\"æœ€èˆŠæ¨£æœ¬æ¬Šé‡ (c): {clf_last_w:.4f}\")\n",
    "        print(f\"æœ€æ–°æ¨£æœ¬æ¬Šé‡: 1.0\")\n",
    "        print(f\"å¹³å‡æ™‚é–“è¡°æ¸›æ¬Šé‡: {clfW.mean():.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        if clf_last_w == 1.0:\n",
    "            print(f\"  - c = 1: ç„¡æ™‚é–“è¡°æ¸›\")\n",
    "        elif 0 < clf_last_w < 1:\n",
    "            print(f\"  - 0 < c < 1: ç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\")\n",
    "        elif clf_last_w == 0:\n",
    "            print(f\"  - c = 0: æœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\")\n",
    "        elif clf_last_w < 0:\n",
    "            print(f\"  - c < 0: æœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\")\n",
    "        print(f\"  - è¡°æ¸›åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ï¼Œè€Œéæ™‚é–“é †åº\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return clfW\n",
    "    \n",
    "    def get_time_decay_exp(self, tW: pd.Series, \n",
    "                          decay_rate: float = 1.0,\n",
    "                          percent_of_zero_wts: float = 0.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        æŒ‡æ•¸æ™‚é–“è¡°æ¸›\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        ä½¿ç”¨æŒ‡æ•¸å‡½æ•¸é€²è¡Œæ™‚é–“è¡°æ¸›ï¼š\n",
    "        \n",
    "        d[x] = exp((decay_rate - 1) Ã— (Î£Å« - x))\n",
    "        \n",
    "        å…¶ä¸­ x æ˜¯ç´¯ç©å”¯ä¸€æ€§ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        tW : pd.Series\n",
    "            å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé€šå¸¸æ˜¯ uniquenessï¼‰\n",
    "        decay_rate : float\n",
    "            è¡°æ¸›ç‡ï¼ˆ>1 è¡¨ç¤ºè¡°æ¸›æ›´å¿«ï¼Œ<1 è¡¨ç¤ºè¡°æ¸›æ›´æ…¢ï¼‰\n",
    "        percent_of_zero_wts : float\n",
    "            æœ€èˆŠæ¨£æœ¬ä¸­è¨­ç‚º 0 çš„æ¯”ä¾‹\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        time_decay : pd.Series\n",
    "            æ™‚é–“è¡°æ¸›æ¬Šé‡\n",
    "        \"\"\"\n",
    "        clf_w = tW.sort_index().cumsum()\n",
    "        last_value = clf_w.iloc[-1]\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ•¸è¡°æ¸›æ¬Šé‡\n",
    "        out_wts = []\n",
    "        zero_threshold = int(round(len(clf_w) * percent_of_zero_wts))\n",
    "        \n",
    "        for i in range(len(clf_w)):\n",
    "            if i < zero_threshold:\n",
    "                out_wts.append(0.0)\n",
    "            else:\n",
    "                # æŒ‡æ•¸è¡°æ¸›ï¼šexp((decay_rate - 1) * (last_value - current_value))\n",
    "                decay_factor = np.exp((decay_rate - 1.0) * (last_value - clf_w.iloc[i]))\n",
    "                out_wts.append(decay_factor)\n",
    "        \n",
    "        time_decay = pd.Series(out_wts, index=clf_w.index)\n",
    "        self.time_decay = time_decay\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æŒ‡æ•¸æ™‚é–“è¡°æ¸›çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"è¡°æ¸›ç‡ (decay_rate): {decay_rate}\")\n",
    "        print(f\"é›¶æ¬Šé‡æ¨£æœ¬æ¯”ä¾‹: {percent_of_zero_wts:.2%}\")\n",
    "        print(f\"å¹³å‡æ™‚é–“è¡°æ¸›æ¬Šé‡: {time_decay.mean():.4f}\")\n",
    "        print(f\"æ™‚é–“è¡°æ¸›åˆ†å¸ƒ:\")\n",
    "        print(time_decay.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return time_decay\n",
    "    \n",
    "    def compute_sample_weights(self, \n",
    "                              t1: pd.Series,\n",
    "                              close_idx: pd.DatetimeIndex,\n",
    "                              close: Optional[pd.Series] = None,\n",
    "                              use_uniqueness: bool = True,\n",
    "                              use_time_decay: bool = True,\n",
    "                              use_return_attribution: bool = False,\n",
    "                              time_decay_type: str = 'linear',\n",
    "                              time_decay_params: dict = None) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æœ€çµ‚æ¨£æœ¬æ¬Šé‡ï¼ˆå¯é¸æ“‡ä½¿ç”¨å“ªäº›æ©Ÿåˆ¶ï¼‰\n",
    "        \n",
    "        æ¬Šé‡çµ„åˆæ–¹å¼ï¼š\n",
    "        ------------\n",
    "        1. åŸºç¤æ¬Šé‡ = å”¯ä¸€æ€§ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        2. å¯é¸ï¼šÃ— å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        3. å¯é¸ï¼šÃ— æ™‚é–“è¡°æ¸›ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        \n",
    "        æœ€çµ‚æ¬Šé‡ = uniqueness Ã— (return_attribution) Ã— (time_decay)\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "        close_idx : pd.DatetimeIndex\n",
    "            åƒ¹æ ¼æ•¸æ“šçš„ç´¢å¼•\n",
    "        close : pd.Series, optional\n",
    "            åƒ¹æ ¼åºåˆ—ï¼ˆç”¨æ–¼è¨ˆç®—å ±é…¬æ­¸å› ï¼‰\n",
    "        use_uniqueness : bool\n",
    "            æ˜¯å¦ä½¿ç”¨å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_time_decay : bool\n",
    "            æ˜¯å¦ä½¿ç”¨æ™‚é–“è¡°æ¸›ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_return_attribution : bool\n",
    "            æ˜¯å¦ä½¿ç”¨å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆé è¨­ Falseï¼‰\n",
    "        time_decay_type : str\n",
    "            æ™‚é–“è¡°æ¸›é¡å‹ï¼š'linear' æˆ– 'exp'ï¼ˆé è¨­ 'linear'ï¼‰\n",
    "        time_decay_params : dict, optional\n",
    "            æ™‚é–“è¡°æ¸›åƒæ•¸\n",
    "            - linear: {'clf_last_w': 0.5}\n",
    "            - exp: {'decay_rate': 1.2, 'percent_of_zero_wts': 0.0}\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        sample_weights : pd.Series\n",
    "            æœ€çµ‚æ¨£æœ¬æ¬Šé‡ï¼ˆå¦‚æœéƒ½ä¸ä½¿ç”¨ï¼Œè¿”å›å…¨ç‚º 1 çš„æ¬Šé‡ï¼‰\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡è¨ˆç®—é…ç½®\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ä½¿ç”¨å”¯ä¸€æ€§: {use_uniqueness}\")\n",
    "        print(f\"ä½¿ç”¨å ±é…¬æ­¸å› : {use_return_attribution}\")\n",
    "        print(f\"ä½¿ç”¨æ™‚é–“è¡°æ¸›: {use_time_decay}\")\n",
    "        if use_time_decay:\n",
    "            print(f\"æ™‚é–“è¡°æ¸›é¡å‹: {time_decay_type}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # å¦‚æœéƒ½ä¸ä½¿ç”¨ï¼Œè¿”å›å‡å‹»æ¬Šé‡\n",
    "        if not use_uniqueness and not use_time_decay and not use_return_attribution:\n",
    "            sample_weights = pd.Series(1.0, index=t1.index)\n",
    "            self.sample_weights = sample_weights\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡çµ±è¨ˆï¼ˆå‡å‹»æ¬Šé‡ï¼‰\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"ç¸½æ¨£æœ¬æ•¸: {len(sample_weights):,}\")\n",
    "            print(f\"æ‰€æœ‰æ¨£æœ¬æ¬Šé‡: 1.0\")\n",
    "            print(\"=\" * 60)\n",
    "            return sample_weights\n",
    "        \n",
    "        # è¨ˆç®—ä¸¦ç™¼åº¦ï¼ˆå”¯ä¸€æ€§å’Œå ±é…¬æ­¸å› éƒ½éœ€è¦ï¼‰\n",
    "        if use_uniqueness or use_return_attribution:\n",
    "            num_co_events = self.compute_num_co_events(close_idx, t1)\n",
    "        else:\n",
    "            num_co_events = None\n",
    "        \n",
    "        # è¨ˆç®—å”¯ä¸€æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_uniqueness:\n",
    "            uniqueness = self.compute_uniqueness(t1, num_co_events)\n",
    "        else:\n",
    "            uniqueness = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨å”¯ä¸€æ€§ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—å ±é…¬æ­¸å› ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_return_attribution:\n",
    "            if close is None:\n",
    "                raise ValueError(\"ä½¿ç”¨å ±é…¬æ­¸å› æ™‚éœ€è¦æä¾› close åƒæ•¸\")\n",
    "            return_weights = self.compute_return_attribution(close, t1, num_co_events)\n",
    "        else:\n",
    "            return_weights = pd.Series(1.0, index=t1.index)\n",
    "        \n",
    "        # è¨ˆç®—æ™‚é–“è¡°æ¸›ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_time_decay:\n",
    "            if time_decay_params is None:\n",
    "                time_decay_params = {}\n",
    "            \n",
    "            if time_decay_type == 'linear':\n",
    "                clf_last_w = time_decay_params.get('clf_last_w', 1.0)\n",
    "                time_decay = self.get_time_decay_linear(uniqueness, clf_last_w)\n",
    "            elif time_decay_type == 'exp':\n",
    "                decay_rate = time_decay_params.get('decay_rate', 1.0)\n",
    "                percent_of_zero_wts = time_decay_params.get('percent_of_zero_wts', 0.0)\n",
    "                time_decay = self.get_time_decay_exp(uniqueness, decay_rate, percent_of_zero_wts)\n",
    "            else:\n",
    "                raise ValueError(f\"ä¸æ”¯æ´çš„æ™‚é–“è¡°æ¸›é¡å‹: {time_decay_type}\")\n",
    "        else:\n",
    "            time_decay = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨æ™‚é–“è¡°æ¸›ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—æœ€çµ‚æ¬Šé‡\n",
    "        common_idx = uniqueness.index.intersection(time_decay.index).intersection(return_weights.index)\n",
    "        sample_weights = (uniqueness.loc[common_idx] * \n",
    "                          return_weights.loc[common_idx] * \n",
    "                          time_decay.loc[common_idx])\n",
    "        \n",
    "        self.sample_weights = sample_weights\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½æ¨£æœ¬æ•¸: {len(sample_weights):,}\")\n",
    "        print(f\"å¹³å‡æ¬Šé‡: {sample_weights.mean():.4f}\")\n",
    "        print(f\"æ¬Šé‡åˆ†å¸ƒ:\")\n",
    "        print(sample_weights.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return sample_weights\n",
    "    \n",
    "    def plot_concurrency_vs_volatility(self, daily_vol: pd.Series):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ä¸¦ç™¼åº¦ vs æ³¢å‹•ç‡çš„é—œä¿‚åœ–\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        daily_vol : pd.Series\n",
    "            æ—¥æ³¢å‹•ç‡åºåˆ—\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        if self.num_co_events is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_num_co_events()\")\n",
    "            return\n",
    "        \n",
    "        # å°é½Šæ•¸æ“šï¼ˆä½¿ç”¨åŸå§‹æ™‚é–“é»ï¼‰\n",
    "        to_plot = pd.DataFrame({\n",
    "            'conc_events': self.num_co_events,\n",
    "            'daily_vol': daily_vol\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(to_plot) == 0:\n",
    "            print(\"âš ï¸ å°é½Šå¾Œç„¡æœ‰æ•ˆæ•¸æ“š\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # 1. æ™‚é–“åºåˆ—åœ–\n",
    "        ax1 = axes[0]\n",
    "        ax1_twin = ax1.twinx()\n",
    "        \n",
    "        line1 = ax1.plot(to_plot.index, to_plot['conc_events'], \n",
    "                        label='Concurrent Events', color='blue', linewidth=1.5)\n",
    "        line2 = ax1_twin.plot(to_plot.index, to_plot['daily_vol'], \n",
    "                            label='Daily Volatility', color='red', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        ax1.set_xlabel('Time', fontsize=12)\n",
    "        ax1.set_ylabel('Concurrent Events', color='blue', fontsize=12)\n",
    "        ax1_twin.set_ylabel('Daily Volatility', color='red', fontsize=12)\n",
    "        ax1.set_title('Concurrent Events vs Daily Volatility', fontsize=14)\n",
    "        ax1.tick_params(axis='y', labelcolor='blue')\n",
    "        ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1_twin.legend(loc='upper right')\n",
    "        \n",
    "        # 2. æ•£é»åœ–\n",
    "        sns.regplot(x=to_plot['conc_events'], y=to_plot['daily_vol'], \n",
    "                ax=axes[1], scatter_kws={'alpha': 0.6, 's': 50})\n",
    "        axes[1].set_xlabel('Concurrent Events', fontsize=12)\n",
    "        axes[1].set_ylabel('Daily Volatility', fontsize=12)\n",
    "        axes[1].set_title('Concurrent Events vs Daily Volatility (Scatter Plot)', fontsize=14)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ‰“å°ç›¸é—œæ€§\n",
    "        correlation = to_plot['conc_events'].corr(to_plot['daily_vol'])\n",
    "        print(f\"\\nğŸ“Š ä¸¦ç™¼åº¦èˆ‡æ³¢å‹•ç‡çš„ç›¸é—œä¿‚æ•¸: {correlation:.4f}\")\n",
    "    \n",
    "    def plot_weights_distribution(self):\n",
    "        \"\"\"ç¹ªè£½æ¬Šé‡åˆ†å¸ƒåœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.sample_weights is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_sample_weights()\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. å”¯ä¸€æ€§åˆ†å¸ƒ\n",
    "        if self.uniqueness is not None:\n",
    "            self.uniqueness.hist(bins=50, ax=axes[0, 0], alpha=0.7, color='blue')\n",
    "            axes[0, 0].set_title('Uniqueness Distribution')\n",
    "            axes[0, 0].set_xlabel('Uniqueness')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. æ™‚é–“è¡°æ¸›åˆ†å¸ƒ\n",
    "        if self.time_decay is not None:\n",
    "            self.time_decay.hist(bins=50, ax=axes[0, 1], alpha=0.7, color='green')\n",
    "            axes[0, 1].set_title('Time Decay Distribution')\n",
    "            axes[0, 1].set_xlabel('Time Decay Weight')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. æ¨£æœ¬æ¬Šé‡åˆ†å¸ƒ\n",
    "        self.sample_weights.hist(bins=50, ax=axes[1, 0], alpha=0.7, color='orange')\n",
    "        axes[1, 0].set_title('Sample Weights Distribution')\n",
    "        axes[1, 0].set_xlabel('Sample Weight')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. æ¬Šé‡éš¨æ™‚é–“è®ŠåŒ–\n",
    "        if len(self.sample_weights) > 0:\n",
    "            sorted_weights = self.sample_weights.sort_index()\n",
    "            axes[1, 1].plot(sorted_weights.index, sorted_weights.values, \n",
    "                           linewidth=1, alpha=0.7, color='purple')\n",
    "            axes[1, 1].set_title('Sample Weights Over Time')\n",
    "            axes[1, 1].set_xlabel('Time')\n",
    "            axes[1, 1].set_ylabel('Sample Weight')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_uniqueness_autocorr(self, lag: int = 1) -> float:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å”¯ä¸€æ€§çš„åºåˆ—ç›¸é—œæ€§ï¼ˆAR(1)ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        lag : int\n",
    "            æ»¯å¾ŒæœŸæ•¸ï¼ˆé è¨­ 1ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        autocorr : float\n",
    "            åºåˆ—ç›¸é—œæ€§\n",
    "        \"\"\"\n",
    "        if self.uniqueness is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_uniqueness()\")\n",
    "            return None\n",
    "        \n",
    "        autocorr = self.uniqueness.autocorr(lag=lag)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å”¯ä¸€æ€§åºåˆ—ç›¸é—œæ€§\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"AR({lag}) è‡ªç›¸é—œä¿‚æ•¸: {autocorr:.4f}\")\n",
    "        \n",
    "        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "        n = len(self.uniqueness)\n",
    "        critical_value = 1.96 / np.sqrt(n)  # 95% ä¿¡å¿ƒæ°´æº–\n",
    "        \n",
    "        if abs(autocorr) > critical_value:\n",
    "            print(f\"âœ… çµ±è¨ˆé¡¯è‘— (|{autocorr:.4f}| > {critical_value:.4f})\")\n",
    "            print(f\"\\nè§£é‡‹:\")\n",
    "            print(f\"  - å”¯ä¸€æ€§å­˜åœ¨åºåˆ—ç›¸é—œæ€§ï¼Œè¡¨ç¤ºå¸‚å ´ç‹€æ…‹å…·æœ‰æŒçºŒæ€§\")\n",
    "            print(f\"  - é€™åœ¨é‡‘èæ•¸æ“šä¸­æ˜¯å¸¸è¦‹ä¸”é æœŸçš„ç¾è±¡\")\n",
    "            print(f\"  - æ™‚é–“è¡°æ¸›æ©Ÿåˆ¶æœ‰åŠ©æ–¼é™ä½èˆŠæ•¸æ“šçš„å½±éŸ¿\")\n",
    "        else:\n",
    "            print(f\"âŒ çµ±è¨ˆä¸é¡¯è‘— (|{autocorr:.4f}| <= {critical_value:.4f})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return autocorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# å®Œæ•´ç¯„ä¾‹ï¼šæ¨£æœ¬æ¬Šé‡è¨ˆç®—ï¼ˆæ‰€æœ‰é¸é …ï¼‰\n",
    "# =====================================================\n",
    "\n",
    "sample_weight = SampleWeight()\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … A: æ¨™æº–é…ç½®ï¼ˆå”¯ä¸€æ€§ + ç·šæ€§æ™‚é–“è¡°æ¸›ï¼‰\n",
    "# =====================================================\n",
    "final_weights_A = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=True,\n",
    "    use_time_decay=True,\n",
    "    time_decay_type='linear',\n",
    "    time_decay_params={'clf_last_w': 0.5}\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … B: å®Œæ•´é…ç½®ï¼ˆå”¯ä¸€æ€§ + å ±é…¬æ­¸å›  + æ™‚é–“è¡°æ¸›ï¼‰\n",
    "# =====================================================\n",
    "final_weights_B = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    close=nas100_raw['Close'],\n",
    "    use_uniqueness=True,\n",
    "    use_time_decay=True,\n",
    "    use_return_attribution=True,  # åŠ å…¥å ±é…¬æ­¸å› \n",
    "    time_decay_type='linear',\n",
    "    time_decay_params={'clf_last_w': 0.5}\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … C: æŒ‡æ•¸æ™‚é–“è¡°æ¸›\n",
    "# =====================================================\n",
    "final_weights_C = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=True,\n",
    "    use_time_decay=True,\n",
    "    time_decay_type='exp',\n",
    "    time_decay_params={'decay_rate': 1.2, 'percent_of_zero_wts': 0.0}\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … D: åªä½¿ç”¨å”¯ä¸€æ€§ï¼ˆæœ€ç°¡å–®ï¼‰\n",
    "# =====================================================\n",
    "final_weights_D = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=True,\n",
    "    use_time_decay=False,\n",
    "    use_return_attribution=False\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … E: åªä½¿ç”¨å ±é…¬æ­¸å› \n",
    "# =====================================================\n",
    "final_weights_E = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    close=nas100_raw['Close'],\n",
    "    use_uniqueness=False,\n",
    "    use_time_decay=False,\n",
    "    use_return_attribution=True\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … F: å‡å‹»æ¬Šé‡ï¼ˆåŸºæº–å°ç…§ï¼‰\n",
    "# =====================================================\n",
    "final_weights_F = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=False,\n",
    "    use_time_decay=False,\n",
    "    use_return_attribution=False\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# æ¯”è¼ƒä¸åŒé…ç½®çš„æ¬Šé‡çµ±è¨ˆ\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ä¸åŒé…ç½®çš„æ¬Šé‡æ¯”è¼ƒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'A: å”¯ä¸€æ€§+ç·šæ€§è¡°æ¸›': final_weights_A.describe(),\n",
    "    'B: å”¯ä¸€æ€§+å ±é…¬æ­¸å› +ç·šæ€§è¡°æ¸›': final_weights_B.describe(),\n",
    "    'C: å”¯ä¸€æ€§+æŒ‡æ•¸è¡°æ¸›': final_weights_C.describe(),\n",
    "    'D: åªå”¯ä¸€æ€§': final_weights_D.describe(),\n",
    "    'E: åªå ±é…¬æ­¸å› ': final_weights_E.describe(),\n",
    "    'F: å‡å‹»æ¬Šé‡': final_weights_F.describe()\n",
    "})\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "# =====================================================\n",
    "# è¦–è¦ºåŒ–æ¯”è¼ƒ\n",
    "# =====================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. æ¬Šé‡åˆ†å¸ƒæ¯”è¼ƒ\n",
    "axes[0, 0].hist(final_weights_A, bins=50, alpha=0.5, label='A: Uniqueness+Linear', color='blue')\n",
    "axes[0, 0].hist(final_weights_B, bins=50, alpha=0.5, label='B: å®Œæ•´é…ç½®', color='red')\n",
    "axes[0, 0].hist(final_weights_D, bins=50, alpha=0.5, label='D: åªå”¯ä¸€æ€§', color='green')\n",
    "axes[0, 0].set_title('Weight Distribution Comparison')\n",
    "axes[0, 0].set_xlabel('Sample Weight')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. æ¬Šé‡éš¨æ™‚é–“è®ŠåŒ–\n",
    "sorted_A = final_weights_A.sort_index()\n",
    "axes[0, 1].plot(sorted_A.index, sorted_A.values, label='A: å”¯ä¸€æ€§+ç·šæ€§', alpha=0.7)\n",
    "axes[0, 1].set_title('Weights Over Time')\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Weight')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. æ¬Šé‡åˆ†ä½æ•¸æ¯”è¼ƒ\n",
    "weights_list = [final_weights_A, final_weights_B, final_weights_D]\n",
    "labels_list = ['A: å”¯ä¸€æ€§+ç·šæ€§', 'B: å®Œæ•´é…ç½®', 'D: åªå”¯ä¸€æ€§']\n",
    "percentiles = [25, 50, 75, 90, 95]\n",
    "\n",
    "for i, (w, label) in enumerate(zip(weights_list, labels_list)):\n",
    "    values = [w.quantile(p/100) for p in percentiles]\n",
    "    axes[1, 0].plot(percentiles, values, marker='o', label=label)\n",
    "\n",
    "axes[1, 0].set_title('Weight Percentiles Comparison')\n",
    "axes[1, 0].set_xlabel('Percentile')\n",
    "axes[1, 0].set_ylabel('Weight Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. æ¬Šé‡çµ±è¨ˆæ‘˜è¦\n",
    "stats_data = {\n",
    "    'Mean': [w.mean() for w in weights_list],\n",
    "    'Std': [w.std() for w in weights_list],\n",
    "    'Min': [w.min() for w in weights_list],\n",
    "    'Max': [w.max() for w in weights_list]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats_data, index=labels_list)\n",
    "stats_df.plot(kind='bar', ax=axes[1, 1], alpha=0.7)\n",
    "axes[1, 1].set_title('Weight Statistics Comparison')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =====================================================\n",
    "# æœ€çµ‚é¸æ“‡ï¼ˆæ ¹æ“šéœ€æ±‚é¸æ“‡ä¸€å€‹ï¼‰\n",
    "# =====================================================\n",
    "# æ¨è–¦ï¼šä½¿ç”¨é¸é … A æˆ– B\n",
    "final_weights = final_weights_A  # æˆ– final_weights_B\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… æœ€çµ‚é¸æ“‡çš„æ¬Šé‡é…ç½®\")\n",
    "print(\"=\"*60)\n",
    "print(f\"é…ç½®: å”¯ä¸€æ€§ + ç·šæ€§æ™‚é–“è¡°æ¸›\")\n",
    "print(f\"æ¨£æœ¬æ•¸: {len(final_weights):,}\")\n",
    "print(f\"å¹³å‡æ¬Šé‡: {final_weights.mean():.4f}\")\n",
    "print(f\"æ¬Šé‡ç¯„åœ: [{final_weights.min():.4f}, {final_weights.max():.4f}]\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f61e80",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ5 ï¼š åˆ†éšå·®å¹³ç©©åƒ¹æ ¼ç‰¹å¾µæå–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7e886",
   "metadata": {},
   "source": [
    "ä¸»å•†å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# å®Œæ•´ä»£ç¢¼ï¼šç‚ºæ‰€æœ‰å•†å“ç”¢ç”Ÿåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# 2. ä¿®æ­£å¾Œçš„ FractionalDiff classï¼ˆç¢ºä¿ä¿ç•™æ‰€æœ‰ indexï¼‰\n",
    "# =====================================================\n",
    "\n",
    "class FractionalDiff:\n",
    "    \"\"\"åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimal_d = None\n",
    "        self.fracdiff_series = None\n",
    "        self.features = None\n",
    "        \n",
    "    def get_weights(self, d: float, size: int) -> np.ndarray:\n",
    "        \"\"\"è¨ˆç®—æ¬Šé‡ï¼ˆæ“´å±•çª—å£ï¼‰\"\"\"\n",
    "        w = [1.0]\n",
    "        for k in range(1, size):\n",
    "            w_ = -w[-1] / k * (d - k + 1)\n",
    "            w.append(float(w_))\n",
    "        return np.array(w[::-1]).reshape(-1, 1)\n",
    "    \n",
    "    def get_weights_FFD(self, d: float, thres: float = 1e-5) -> np.ndarray:\n",
    "        \"\"\"è¨ˆç®— FFD æ¬Šé‡ï¼ˆå›ºå®šçª—å£ï¼‰\"\"\"\n",
    "        w = [1.0]\n",
    "        k = 1\n",
    "        while True:\n",
    "            w_ = -w[-1] / k * (d - k + 1)\n",
    "            if abs(w_) < thres:\n",
    "                break\n",
    "            w.append(float(w_))\n",
    "            k += 1\n",
    "            if k > 10000:\n",
    "                break\n",
    "        return np.array(w[::-1]).reshape(-1, 1)\n",
    "    \n",
    "    def frac_diff_FFD(self, series: pd.Series, d: float, thres: float = 1e-5) -> pd.Series:\n",
    "        \"\"\"\n",
    "        åˆ†æ•¸éšå·®åˆ†ï¼ˆå›ºå®šçª—å£æ–¹æ³•ï¼ŒFFDï¼‰\n",
    "        \n",
    "        é‡è¦ï¼šä¿ç•™æ‰€æœ‰åŸå§‹ indexï¼Œåªåœ¨å‰ width å€‹é»è¨­ç‚º NaN\n",
    "        \"\"\"\n",
    "        # 1. è¨ˆç®—æ¬Šé‡ï¼ˆå›ºå®šçª—å£ï¼‰\n",
    "        w = self.get_weights_FFD(d, thres)\n",
    "        width = len(w) - 1\n",
    "        \n",
    "        if width < 1:\n",
    "            return pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        # 2. æ‡‰ç”¨æ¬Šé‡åˆ°æ•¸å€¼\n",
    "        # é‡è¦ï¼šå…ˆå¡«å…… NaNï¼Œä½†ä¿ç•™åŸå§‹ index\n",
    "        series_filled = series.fillna(method='ffill')\n",
    "        \n",
    "        # å»ºç«‹çµæœ Seriesï¼Œä½¿ç”¨åŸå§‹ series çš„å®Œæ•´ index\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        # 3. å¾ width é–‹å§‹è¨ˆç®—ï¼ˆå‰ width å€‹é»æœƒæ˜¯ NaNï¼‰\n",
    "        valid_count = 0\n",
    "        for iloc1 in range(width, len(series_filled)):\n",
    "            if iloc1 >= len(series.index):\n",
    "                break\n",
    "                \n",
    "            loc0 = series_filled.index[iloc1 - width]\n",
    "            loc1 = series_filled.index[iloc1]\n",
    "            \n",
    "            # æª¢æŸ¥åŸå§‹ series åœ¨ loc1 æ˜¯å¦æœ‰æœ‰æ•ˆå€¼\n",
    "            if loc1 not in series.index or not np.isfinite(series.loc[loc1]):\n",
    "                continue\n",
    "            \n",
    "            # ä½¿ç”¨å›ºå®šçª—å£ï¼š[X_{t-width}, X_{t-width+1}, ..., X_t]\n",
    "            window_data = series_filled.loc[loc0:loc1].values\n",
    "            \n",
    "            if len(window_data) == len(w):\n",
    "                result = np.dot(w.flatten(), window_data)\n",
    "                df_[loc1] = result\n",
    "                valid_count += 1\n",
    "        \n",
    "        return df_\n",
    "    \n",
    "    def frac_diff(self, series: pd.Series, d: float, thres: float = 0.01) -> pd.Series:\n",
    "        \"\"\"åˆ†æ•¸éšå·®åˆ†ï¼ˆæ“´å±•çª—å£ï¼‰\"\"\"\n",
    "        w = self.get_weights(d, series.shape[0])\n",
    "        w_ = np.cumsum(np.abs(w))\n",
    "        w_ /= w_[-1]\n",
    "        skip = w_[w_ > thres].shape[0]\n",
    "        \n",
    "        series_filled = series.fillna(method='ffill')\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        for iloc in range(skip, series_filled.shape[0]):\n",
    "            loc = series_filled.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc]):\n",
    "                continue\n",
    "            \n",
    "            window_data = series_filled.loc[:loc].values\n",
    "            w_subset = w[-(iloc + 1):, :].flatten()\n",
    "            \n",
    "            if len(window_data) == len(w_subset):\n",
    "                df_[loc] = np.dot(w_subset, window_data)\n",
    "        \n",
    "        return df_\n",
    "    \n",
    "    def find_optimal_d(self, series: pd.Series, \n",
    "                      d_range: Tuple[float, float] = (0, 1),\n",
    "                      d_step: float = 0.05,\n",
    "                      method: str = 'FFD',\n",
    "                      thres: float = 1e-5,\n",
    "                      target_pvalue: float = 0.05,\n",
    "                      min_corr: float = 0.5) -> dict:\n",
    "        \"\"\"æ‰¾å‡ºæœ€é©åˆçš„ d å€¼\"\"\"\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        from scipy.stats import jarque_bera\n",
    "        \n",
    "        results = []\n",
    "        d_values = np.arange(d_range[0], d_range[1] + d_step, d_step)\n",
    "        \n",
    "        for d in d_values:\n",
    "            try:\n",
    "                if method == 'FFD':\n",
    "                    fracdiff_series = self.frac_diff_FFD(series, d, thres).dropna()\n",
    "                else:\n",
    "                    fracdiff_series = self.frac_diff(series, d, thres).dropna()\n",
    "                \n",
    "                if len(fracdiff_series) < 10:\n",
    "                    continue\n",
    "                \n",
    "                adf_result = adfuller(fracdiff_series, maxlag=1, regression='c', autolag=None)\n",
    "                adf_stat = adf_result[0]\n",
    "                p_value = adf_result[1]\n",
    "                \n",
    "                common_idx = series.index.intersection(fracdiff_series.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    corr = np.corrcoef(\n",
    "                        series.loc[common_idx].values,\n",
    "                        fracdiff_series.loc[common_idx].values\n",
    "                    )[0, 1]\n",
    "                else:\n",
    "                    corr = 0\n",
    "                \n",
    "                jb_stat, jb_pvalue = jarque_bera(fracdiff_series)[:2]\n",
    "                \n",
    "                results.append({\n",
    "                    'd': d,\n",
    "                    'adf_stat': adf_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'correlation': corr,\n",
    "                    'jb_stat': jb_stat,\n",
    "                    'jb_pvalue': jb_pvalue,\n",
    "                    'is_stationary': p_value < target_pvalue,\n",
    "                    'meets_corr': corr >= min_corr,\n",
    "                    'sample_size': len(fracdiff_series)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            return None\n",
    "        \n",
    "        valid_results = results_df[\n",
    "            (results_df['is_stationary']) & \n",
    "            (results_df['meets_corr'])\n",
    "        ]\n",
    "        \n",
    "        if len(valid_results) > 0:\n",
    "            optimal = valid_results.loc[valid_results['p_value'].idxmin()]\n",
    "            self.optimal_d = optimal['d']\n",
    "        else:\n",
    "            optimal = results_df.loc[results_df['p_value'].idxmin()]\n",
    "            self.optimal_d = optimal['d']\n",
    "        \n",
    "        return {\n",
    "            'optimal_d': self.optimal_d,\n",
    "            'results': results_df,\n",
    "            'optimal_stats': optimal.to_dict()\n",
    "        }\n",
    "\n",
    "    def find_min_d_adf(self, series: pd.Series,\n",
    "                    d_range: Tuple[float, float] = (0, 1),\n",
    "                    n_points: int = 11,\n",
    "                    method: str = 'FFD',\n",
    "                    thres: float = 0.01,\n",
    "                    target_pvalue: float = 0.1) -> dict:\n",
    "        \"\"\"\n",
    "        å°‹æ‰¾æœ€å° d å€¼ï¼ˆåŸºæ–¼ Snippet 5.4ï¼‰\n",
    "        \n",
    "        é€™å€‹æ–¹æ³•èˆ‡ find_optimal_d é¡ä¼¼ï¼Œä½†å°ˆé–€ç”¨æ–¼å°‹æ‰¾æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„æœ€å° d å€¼ã€‚\n",
    "        é€™æ˜¯ AFML æ›¸ä¸­ Snippet 5.4 çš„å¯¦ç¾ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        series : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        d_range : tuple\n",
    "            d å€¼æœå°‹ç¯„åœ (min, max)\n",
    "        n_points : int\n",
    "            æ¸¬è©¦çš„ d å€¼æ•¸é‡ï¼ˆå°æ‡‰ np.linspace(d_range[0], d_range[1], n_points)ï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼ï¼ˆFFD æ–¹æ³•ï¼‰æˆ–ç´¯ç©æ¬Šé‡é–¾å€¼ï¼ˆexpanding æ–¹æ³•ï¼‰\n",
    "        target_pvalue : float\n",
    "            ç›®æ¨™ p-valueï¼ˆADF æª¢é©—ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        result : dict\n",
    "            åŒ…å«ä»¥ä¸‹éµå€¼ï¼š\n",
    "            - 'min_d': æœ€å° d å€¼ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰\n",
    "            - 'results': DataFrameï¼ŒåŒ…å«æ‰€æœ‰æ¸¬è©¦çµæœ\n",
    "            - 'min_d_stats': æœ€å° d å€¼çš„çµ±è¨ˆè³‡è¨Š\n",
    "        \"\"\"\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        \n",
    "        results = []\n",
    "        d_values = np.linspace(d_range[0], d_range[1], n_points)\n",
    "        \n",
    "        print(f\"ğŸ” æœå°‹æœ€å° d å€¼ (ç¯„åœ: {d_range}, æ¸¬è©¦é»æ•¸: {n_points})\")\n",
    "        print(f\"   æ–¹æ³•: {method}, é–¾å€¼: {thres}, ç›®æ¨™ p-value: {target_pvalue}\")\n",
    "        \n",
    "        for i, d in enumerate(d_values):\n",
    "            try:\n",
    "                # è¨ˆç®—åˆ†æ•¸éšå·®åˆ†\n",
    "                if method == 'FFD':\n",
    "                    fracdiff_series = self.frac_diff_FFD(series, d, thres).dropna()\n",
    "                else:\n",
    "                    fracdiff_series = self.frac_diff(series, d, thres).dropna()\n",
    "                \n",
    "                if len(fracdiff_series) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # ADF æª¢é©—\n",
    "                adf_result = adfuller(fracdiff_series, maxlag=1, regression='c', autolag=None)\n",
    "                adf_stat = adf_result[0]\n",
    "                p_value = adf_result[1]\n",
    "                \n",
    "                # è¨ˆç®—èˆ‡åŸå§‹åºåˆ—çš„ç›¸é—œæ€§\n",
    "                common_idx = series.index.intersection(fracdiff_series.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    corr = np.corrcoef(\n",
    "                        series.loc[common_idx].values,\n",
    "                        fracdiff_series.loc[common_idx].values\n",
    "                    )[0, 1]\n",
    "                else:\n",
    "                    corr = 0\n",
    "                \n",
    "                is_stationary = p_value < target_pvalue\n",
    "                \n",
    "                results.append({\n",
    "                    'd': d,\n",
    "                    'adf_stat': adf_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'correlation': corr,\n",
    "                    'is_stationary': is_stationary,\n",
    "                    'sample_size': len(fracdiff_series)\n",
    "                })\n",
    "                \n",
    "                if (i + 1) % 5 == 0 or i == len(d_values) - 1:\n",
    "                    print(f\"   é€²åº¦: {i+1}/{len(d_values)} (d={d:.3f}, p={p_value:.4f}, stationary={is_stationary})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçµæœ\")\n",
    "            return None\n",
    "        \n",
    "        # å°‹æ‰¾æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„æœ€å° d å€¼\n",
    "        stationary_results = results_df[results_df['is_stationary'] == True]\n",
    "        \n",
    "        if len(stationary_results) > 0:\n",
    "            min_d_row = stationary_results.loc[stationary_results['d'].idxmin()]\n",
    "            min_d = min_d_row['d']\n",
    "            self.optimal_d = min_d\n",
    "            \n",
    "            print(f\"\\nâœ… æ‰¾åˆ°æœ€å° d = {min_d:.4f}\")\n",
    "            print(f\"   ADF p-value: {min_d_row['p_value']:.4f}\")\n",
    "            print(f\"   ç›¸é—œæ€§: {min_d_row['correlation']:.4f}\")\n",
    "        else:\n",
    "            # å¦‚æœæ²’æœ‰æ»¿è¶³æ¢ä»¶çš„ï¼Œé¸æ“‡ p-value æœ€å°çš„\n",
    "            min_d_row = results_df.loc[results_df['p_value'].idxmin()]\n",
    "            min_d = min_d_row['d']\n",
    "            self.optimal_d = min_d\n",
    "            \n",
    "            print(f\"\\nâš ï¸ æœªæ‰¾åˆ°æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„ d å€¼\")\n",
    "            print(f\"   ä½¿ç”¨ p-value æœ€å°çš„ d = {min_d:.4f}\")\n",
    "            print(f\"   ADF p-value: {min_d_row['p_value']:.4f} (ç›®æ¨™: < {target_pvalue})\")\n",
    "        \n",
    "        return {\n",
    "            'min_d': min_d,\n",
    "            'results': results_df,\n",
    "            'min_d_stats': min_d_row.to_dict()\n",
    "        }\n",
    "\n",
    "    def _plot_d_search_results(self, result: dict):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ d å€¼æœå°‹çµæœ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        result : dict\n",
    "            find_min_d_adf æˆ– find_optimal_d çš„è¿”å›çµæœ\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if result is None or 'results' not in result:\n",
    "            print(\"âš ï¸ ç„¡æ•ˆçš„çµæœ\")\n",
    "            return\n",
    "        \n",
    "        results_df = result['results']\n",
    "        min_d = result.get('min_d') or result.get('optimal_d')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. ADF p-value vs d\n",
    "        axes[0, 0].plot(results_df['d'], results_df['p_value'], 'o-', linewidth=2, markersize=6)\n",
    "        if min_d is not None:\n",
    "            min_d_pvalue = results_df[results_df['d'] == min_d]['p_value'].values[0]\n",
    "            axes[0, 0].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[0, 0].scatter([min_d], [min_d_pvalue], color='red', s=100, zorder=5)\n",
    "        axes[0, 0].axhline(0.05, color='gray', linestyle=':', alpha=0.5, label='p=0.05')\n",
    "        axes[0, 0].axhline(0.1, color='gray', linestyle=':', alpha=0.5, label='p=0.1')\n",
    "        axes[0, 0].set_xlabel('d', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('ADF p-value', fontsize=12)\n",
    "        axes[0, 0].set_title('ADF Test p-value vs d', fontsize=14)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. ADF Statistic vs d\n",
    "        axes[0, 1].plot(results_df['d'], results_df['adf_stat'], 'o-', linewidth=2, markersize=6, color='green')\n",
    "        if min_d is not None:\n",
    "            min_d_stat = results_df[results_df['d'] == min_d]['adf_stat'].values[0]\n",
    "            axes[0, 1].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[0, 1].scatter([min_d], [min_d_stat], color='red', s=100, zorder=5)\n",
    "        axes[0, 1].axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "        axes[0, 1].set_xlabel('d', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('ADF Statistic', fontsize=12)\n",
    "        axes[0, 1].set_title('ADF Test Statistic vs d', fontsize=14)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Correlation vs d\n",
    "        axes[1, 0].plot(results_df['d'], results_df['correlation'], 'o-', linewidth=2, markersize=6, color='orange')\n",
    "        if min_d is not None:\n",
    "            min_d_corr = results_df[results_df['d'] == min_d]['correlation'].values[0]\n",
    "            axes[1, 0].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[1, 0].scatter([min_d], [min_d_corr], color='red', s=100, zorder=5)\n",
    "        axes[1, 0].set_xlabel('d', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Correlation with Original', fontsize=12)\n",
    "        axes[1, 0].set_title('Correlation vs d', fontsize=14)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Stationarity Status\n",
    "        if 'is_stationary' in results_df.columns:\n",
    "            colors = ['red' if not stat else 'green' for stat in results_df['is_stationary']]\n",
    "            axes[1, 1].scatter(results_df['d'], results_df['is_stationary'].astype(int), \n",
    "                            c=colors, s=100, alpha=0.6)\n",
    "            if min_d is not None:\n",
    "                axes[1, 1].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[1, 1].set_xlabel('d', fontsize=12)\n",
    "            axes[1, 1].set_ylabel('Is Stationary (1=Yes, 0=No)', fontsize=12)\n",
    "            axes[1, 1].set_title('Stationarity Status vs d', fontsize=14)\n",
    "            axes[1, 1].set_ylim(-0.1, 1.1)\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No stationarity data', \n",
    "                            ha='center', va='center', fontsize=12)\n",
    "            axes[1, 1].set_title('Stationarity Status vs d', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_min_d_results(self, result: dict, d: float):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ä½¿ç”¨æœ€å° d å€¼å¾Œçš„çµæœ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        result : dict\n",
    "            find_min_d_adf çš„è¿”å›çµæœ\n",
    "        d : float\n",
    "            ä½¿ç”¨çš„ d å€¼\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"âš ï¸ ç„¡æ•ˆçš„çµæœ\")\n",
    "            return\n",
    "        \n",
    "        # é€™å€‹æ–¹æ³•å¯ä»¥ç¹ªè£½ä½¿ç”¨ d å€¼å¾Œçš„æ™‚é–“åºåˆ—å°æ¯”\n",
    "        # ä½†éœ€è¦å¯¦éš›çš„åˆ†æ•¸éšå·®åˆ†åºåˆ—ï¼Œæ‰€ä»¥é€™è£¡åªé¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "        min_d_stats = result.get('min_d_stats', {})\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æœ€å° d å€¼çµ±è¨ˆè³‡è¨Š\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ä½¿ç”¨çš„ d å€¼: {d:.4f}\")\n",
    "        if min_d_stats:\n",
    "            print(f\"ADF Statistic: {min_d_stats.get('adf_stat', 'N/A'):.4f}\")\n",
    "            print(f\"ADF p-value: {min_d_stats.get('p_value', 'N/A'):.4f}\")\n",
    "            print(f\"èˆ‡åŸå§‹åºåˆ—ç›¸é—œæ€§: {min_d_stats.get('correlation', 'N/A'):.4f}\")\n",
    "            print(f\"æ¨£æœ¬æ•¸: {min_d_stats.get('sample_size', 'N/A'):,}\")\n",
    "            print(f\"æ˜¯å¦å¹³ç©©: {min_d_stats.get('is_stationary', 'N/A')}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    def generate_features(self, df: pd.DataFrame, event_indices: pd.DatetimeIndex,\n",
    "                      price_col: str = 'Close', d: Optional[float] = None,\n",
    "                      method: str = 'FFD', thres: float = 1e-5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç‚ºäº‹ä»¶é»ç”Ÿæˆåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            åŒ…å«åƒ¹æ ¼è³‡æ–™çš„ DataFrame\n",
    "        event_indices : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»\n",
    "        price_col : str\n",
    "            åƒ¹æ ¼æ¬„ä½åç¨±\n",
    "        d : float, optional\n",
    "            åˆ†æ•¸éšå·®åˆ†éšæ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ self.optimal_dï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        features : pd.DataFrame\n",
    "            åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µï¼ˆindex ç‚º event_indicesï¼‰\n",
    "        \"\"\"\n",
    "        if price_col not in df.columns:\n",
    "            raise ValueError(f\"æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½: {price_col}\")\n",
    "        \n",
    "        # ä½¿ç”¨æŒ‡å®šçš„ d å€¼æˆ–æœ€å„ª d å€¼\n",
    "        if d is None:\n",
    "            if self.optimal_d is None:\n",
    "                raise ValueError(\"è«‹å…ˆåŸ·è¡Œ find_optimal_d() æˆ– find_min_d_adf()ï¼Œæˆ–æŒ‡å®š d åƒæ•¸\")\n",
    "            d = self.optimal_d\n",
    "        \n",
    "        print(f\"ğŸ“Š ç”Ÿæˆåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ (d={d:.4f}, method={method})\")\n",
    "        \n",
    "        # è¨ˆç®—æ•´å€‹åºåˆ—çš„åˆ†æ•¸éšå·®åˆ†\n",
    "        price_series = df[price_col]\n",
    "        \n",
    "        if method == 'FFD':\n",
    "            fracdiff_series = self.frac_diff_FFD(price_series, d, thres)\n",
    "        else:\n",
    "            fracdiff_series = self.frac_diff(price_series, d, thres)\n",
    "        \n",
    "        # æå–äº‹ä»¶é»çš„ç‰¹å¾µå€¼\n",
    "        features = pd.DataFrame(index=event_indices)\n",
    "        features['fracdiff'] = fracdiff_series.loc[event_indices].values\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_count = features['fracdiff'].notna().sum()\n",
    "        print(f\"âœ… ç‰¹å¾µç”Ÿæˆå®Œæˆ\")\n",
    "        print(f\"   äº‹ä»¶æ•¸: {len(event_indices):,}\")\n",
    "        print(f\"   æœ‰æ•ˆç‰¹å¾µæ•¸: {valid_count:,}\")\n",
    "        print(f\"   NaN æ•¸: {len(event_indices) - valid_count:,}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def plot_price_vs_fracdiff(self, price_series: pd.Series, \n",
    "                            d: Optional[float] = None,\n",
    "                            method: str = 'FFD',\n",
    "                            thres: float = 1e-5,\n",
    "                            n_points: Optional[int] = None,\n",
    "                            normalize: bool = False):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½åŸå§‹åƒ¹æ ¼èˆ‡åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µçš„å°æ¯”åœ–\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        price_series : pd.Series\n",
    "            åŸå§‹åƒ¹æ ¼åºåˆ—\n",
    "        d : float, optional\n",
    "            åˆ†æ•¸éšå·®åˆ†éšæ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ self.optimal_d æˆ–é è¨­å€¼ 0.5ï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼\n",
    "        n_points : int, optional\n",
    "            å¦‚æœæŒ‡å®šï¼Œåªç¹ªè£½æœ€è¿‘ n_points å€‹é»ï¼ˆç”¨æ–¼å¤§é‡æ•¸æ“šæ™‚ï¼‰\n",
    "        normalize : bool\n",
    "            æ˜¯å¦æ¨™æº–åŒ–å¾Œç¹ªè£½åœ¨åŒä¸€å¼µåœ–ä¸Šï¼ˆé è¨­ Falseï¼Œåˆ†é–‹å…©å€‹å­åœ–ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fracdiff_series : pd.Series\n",
    "            è¨ˆç®—å‡ºçš„åˆ†æ•¸éšå·®åˆ†åºåˆ—\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # ç¢ºå®šä½¿ç”¨çš„ d å€¼\n",
    "        if d is None:\n",
    "            if self.optimal_d is not None:\n",
    "                d_value = self.optimal_d\n",
    "            else:\n",
    "                d_value = 0.5\n",
    "                print(f\"âš ï¸ æœªæŒ‡å®š d å€¼ï¼Œä½¿ç”¨é è¨­å€¼ {d_value}\")\n",
    "        else:\n",
    "            d_value = d\n",
    "        \n",
    "        # è¨ˆç®—æ•´å€‹åºåˆ—çš„åˆ†æ•¸éšå·®åˆ†\n",
    "        print(f\"ğŸ“Š è¨ˆç®—åˆ†æ•¸éšå·®åˆ† (d={d_value:.4f}, method={method})...\")\n",
    "        if method == 'FFD':\n",
    "            fracdiff_series = self.frac_diff_FFD(price_series, d_value, thres)\n",
    "        else:\n",
    "            fracdiff_series = self.frac_diff(price_series, d_value, thres)\n",
    "        \n",
    "        # é¸æ“‡è¦ç¹ªè£½çš„æ•¸æ“šç¯„åœ\n",
    "        if n_points is not None and len(price_series) > n_points:\n",
    "            plot_idx = price_series.index[-n_points:]\n",
    "            price_plot = price_series.loc[plot_idx]\n",
    "            fracdiff_plot = fracdiff_series.loc[plot_idx]\n",
    "            title_suffix = f\" (Last {n_points:,} Points)\"\n",
    "        else:\n",
    "            plot_idx = price_series.index\n",
    "            price_plot = price_series\n",
    "            fracdiff_plot = fracdiff_series\n",
    "            title_suffix = \"\"\n",
    "        \n",
    "        # ç¹ªè£½åœ–è¡¨\n",
    "        if normalize:\n",
    "            # æ–¹æ³• 1: æ¨™æº–åŒ–å¾Œç¹ªè£½åœ¨åŒä¸€å¼µåœ–ä¸Š\n",
    "            common_idx = plot_idx.intersection(fracdiff_plot.dropna().index)\n",
    "            if len(common_idx) > 0:\n",
    "                price_normalized = (price_plot.loc[common_idx] - price_plot.loc[common_idx].mean()) / price_plot.loc[common_idx].std()\n",
    "                fracdiff_normalized = (fracdiff_plot.loc[common_idx] - fracdiff_plot.loc[common_idx].mean()) / fracdiff_plot.loc[common_idx].std()\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(15, 6))\n",
    "                \n",
    "                ax.plot(common_idx, price_normalized, \n",
    "                    label='Original Price (Normalized)', linewidth=1.5, color='blue', alpha=0.7)\n",
    "                ax.plot(common_idx, fracdiff_normalized, \n",
    "                    label='Fractional Differentiation (Normalized)', linewidth=1.5, color='red', alpha=0.7)\n",
    "                ax.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "                ax.set_xlabel('Time', fontsize=12)\n",
    "                ax.set_ylabel('Normalized Value', fontsize=12)\n",
    "                ax.set_title(f'Normalized Comparison: Price vs Fractional Differentiation{title_suffix}\\n(d={d_value:.4f}, method={method})', fontsize=14)\n",
    "                ax.legend(loc='upper left')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            # æ–¹æ³• 2: åˆ†é–‹å…©å€‹å­åœ–ï¼ˆé è¨­ï¼‰\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "            \n",
    "            # ä¸Šåœ–ï¼šåŸå§‹åƒ¹æ ¼\n",
    "            axes[0].plot(plot_idx, price_plot.values, \n",
    "                        label='Original Price', linewidth=1.5, color='blue', alpha=0.7)\n",
    "            axes[0].set_ylabel('Price', fontsize=12)\n",
    "            axes[0].set_title(f'Original Price vs Fractional Differentiation{title_suffix}\\n(d={d_value:.4f}, method={method})', fontsize=14)\n",
    "            axes[0].legend(loc='upper left')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # ä¸‹åœ–ï¼šåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "            valid_fracdiff = fracdiff_plot.dropna()\n",
    "            if len(valid_fracdiff) > 0:\n",
    "                axes[1].plot(valid_fracdiff.index, valid_fracdiff.values, \n",
    "                            label='Fractional Differentiation', linewidth=1.5, color='red', alpha=0.7)\n",
    "            axes[1].axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "            axes[1].set_xlabel('Time', fontsize=12)\n",
    "            axes[1].set_ylabel('Fractional Differentiation', fontsize=12)\n",
    "            axes[1].set_title('Fractional Differentiation Feature', fontsize=14)\n",
    "            axes[1].legend(loc='upper left')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_count = fracdiff_series.notna().sum()\n",
    "        print(f\"âœ… ç¹ªåœ–å®Œæˆ\")\n",
    "        print(f\"   åŸå§‹æ•¸æ“šé»æ•¸: {len(price_series):,}\")\n",
    "        print(f\"   åˆ†æ•¸éšå·®åˆ†æœ‰æ•ˆå€¼: {valid_count:,}\")\n",
    "        print(f\"   NaN å€¼: {len(price_series) - valid_count:,}\")\n",
    "        \n",
    "        return fracdiff_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ä½¿ç”¨ç¯„ä¾‹ï¼šå°‹æ‰¾æœ€å° d å€¼\n",
    "# =====================================================\n",
    "\n",
    "fracdiff = FractionalDiff()\n",
    "\n",
    "# æ–¹æ³• 1: åŸºæœ¬ä½¿ç”¨ï¼ˆèˆ‡ Snippet 5.4 é¡ä¼¼ï¼‰\n",
    "result = fracdiff.find_min_d_adf(\n",
    "    series=nas100_raw['Close'],\n",
    "    d_range=(0, 1),\n",
    "    n_points=11,  # å°æ‡‰ np.linspace(0, 1, 11)\n",
    "    method='FFD',\n",
    "    thres=0.01,   # èˆ‡ Snippet 5.4 ä¸€è‡´\n",
    "    target_pvalue=0.05\n",
    ")\n",
    "\n",
    "# # æ–¹æ³• 3: æ›´ç²¾ç´°çš„æœå°‹\n",
    "# result = fracdiff.find_min_d_adf(\n",
    "#     series=nas100_raw['Close'],\n",
    "#     d_range=(0, 0.6),  # æ ¹æ“šç ”ç©¶ï¼Œé€šå¸¸ d < 0.6 å³å¯\n",
    "#     n_points=25,       # æ›´å¯†é›†çš„æ¸¬è©¦\n",
    "#     method='FFD',\n",
    "#     thres=0.01\n",
    "# )\n",
    "\n",
    "# ä½¿ç”¨æ‰¾åˆ°çš„æœ€å° d å€¼\n",
    "if result and result['min_d'] is not None:\n",
    "    min_d = result['min_d']\n",
    "    print(f\"\\nâœ… ä½¿ç”¨æœ€å° d = {min_d:.4f} ç”Ÿæˆç‰¹å¾µ\")\n",
    "    \n",
    "    # ç”Ÿæˆç‰¹å¾µ\n",
    "    features = fracdiff.generate_features(\n",
    "        df=nas100_raw,\n",
    "        event_indices=events.index,\n",
    "        price_col='Close',\n",
    "        d=min_d,\n",
    "        method='FFD'\n",
    "    )\n",
    "    \n",
    "# ç¹ªè£½çµæœ\n",
    "fracdiff._plot_d_search_results(result)\n",
    "fracdiff._plot_min_d_results(result, min_d)\n",
    "# åŸºæœ¬ä½¿ç”¨ï¼ˆå…©å€‹å­åœ–ï¼‰\n",
    "fracdiff.plot_price_vs_fracdiff(\n",
    "    price_series=nas100_raw['Close'],\n",
    "    d=None,  # ä½¿ç”¨ self.optimal_d æˆ–é è¨­å€¼\n",
    "    method='FFD',\n",
    "    thres=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f44f8",
   "metadata": {},
   "source": [
    "å…¶ä»–å•†å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. è³‡æ–™è¼‰å…¥å‡½æ•¸\n",
    "# =====================================================\n",
    "\n",
    "def load_price_data(file_path: str, price_col: str = 'BidClose') -> Optional[pd.Series]:\n",
    "    \"\"\"è¼‰å…¥åƒ¹æ ¼è³‡æ–™\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df.set_index('Date', inplace=True)\n",
    "        else:\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        if price_col in df.columns:\n",
    "            price_series = df[price_col].copy()\n",
    "        elif 'Close' in df.columns:\n",
    "            price_series = df['Close'].copy()\n",
    "        elif 'BidClose' in df.columns:\n",
    "            price_series = df['BidClose'].copy()\n",
    "        else:\n",
    "            print(f\"âš ï¸ {file_path}: æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½\")\n",
    "            return None\n",
    "        \n",
    "        price_series = price_series.dropna()\n",
    "        \n",
    "        if len(price_series) < 100:\n",
    "            print(f\"âš ï¸ {file_path}: è³‡æ–™é»å¤ªå°‘ ({len(price_series)})\")\n",
    "            return None\n",
    "        \n",
    "        return price_series\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è®€å– {file_path} å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# =====================================================\n",
    "# 2. ç‚ºæ‰€æœ‰å•†å“ç”¢ç”Ÿå¤šéšåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "def generate_fracdiff_features_for_all_products(\n",
    "    equity_prices_dir: str = 'equity_prices',\n",
    "    main_product: str = 'NAS100',\n",
    "    price_col: str = 'BidClose',\n",
    "    d_values: list = [0.5, 0.75, 1.0],\n",
    "    method: str = 'FFD',\n",
    "    thres: float = 1e-5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ç‚ºæ‰€æœ‰å•†å“ç”¢ç”Ÿå¤šéšåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    equity_prices_dir : str\n",
    "        å•†å“è³‡æ–™å¤¾è·¯å¾‘\n",
    "    main_product : str\n",
    "        ä¸»å•†å“åç¨±ï¼ˆç”¨æ–¼è¨­å®š indexï¼‰\n",
    "    price_col : str\n",
    "        åƒ¹æ ¼æ¬„ä½åç¨±\n",
    "    d_values : list\n",
    "        è¦è¨ˆç®—çš„åˆ†æ•¸éšå·®åˆ†éšæ•¸åˆ—è¡¨ [0.5, 0.75, 1.0]\n",
    "    method : str\n",
    "        æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "    thres : float\n",
    "        æ¬Šé‡æˆªæ–·é–¾å€¼\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    features_df : pd.DataFrame\n",
    "        æ‰€æœ‰å•†å“çš„å¤šéšåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "        columns: [å•†å“å_d0.5, å•†å“å_d0.75, å•†å“å_d1.0, ...]\n",
    "        index: èˆ‡ä¸»å•†å“ç›¸åŒ\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š ç‚ºæ‰€æœ‰å•†å“ç”¢ç”Ÿå¤šéšåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fracdiff = FractionalDiff()\n",
    "    \n",
    "    # æ‰¾åˆ°æ‰€æœ‰ CSV æª”æ¡ˆ\n",
    "    equity_dir = Path(equity_prices_dir)\n",
    "    csv_files = []\n",
    "    \n",
    "    for csv_file in equity_dir.glob('*.csv'):\n",
    "        if csv_file.name != 'database_refresh.py':\n",
    "            csv_files.append(csv_file)\n",
    "    \n",
    "    commodity_dir = equity_dir / 'commodity_prices'\n",
    "    if commodity_dir.exists():\n",
    "        for csv_file in commodity_dir.glob('*.csv'):\n",
    "            csv_files.append(csv_file)\n",
    "    \n",
    "    print(f\"æ‰¾åˆ° {len(csv_files)} å€‹å•†å“æª”æ¡ˆ\")\n",
    "    \n",
    "    # è¼‰å…¥ä¸»å•†å“è³‡æ–™ï¼ˆç”¨æ–¼è¨­å®š indexï¼‰\n",
    "    main_file = equity_dir / f'{main_product}.csv'\n",
    "    if not main_file.exists():\n",
    "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ°ä¸»å•†å“æª”æ¡ˆ: {main_file}\")\n",
    "    \n",
    "    main_price = load_price_data(str(main_file), price_col)\n",
    "    if main_price is None:\n",
    "        raise ValueError(f\"ç„¡æ³•è¼‰å…¥ä¸»å•†å“: {main_product}\")\n",
    "    \n",
    "    print(f\"\\nä¸»å•†å“: {main_product}\")\n",
    "    print(f\"ä¸»å•†å“è³‡æ–™æœŸé–“: {main_price.index[0]} è‡³ {main_price.index[-1]}\")\n",
    "    print(f\"ä¸»å•†å“è³‡æ–™é»æ•¸: {len(main_price):,}\")\n",
    "    print(f\"è¨ˆç®—éšæ•¸: {d_values}\")\n",
    "    \n",
    "    # å»ºç«‹ DataFrameï¼Œä½¿ç”¨ä¸»å•†å“çš„å®Œæ•´ index\n",
    "    features_df = pd.DataFrame(index=main_price.index)\n",
    "    \n",
    "    # ç‚ºæ¯å€‹å•†å“è¨ˆç®—å¤šéšåˆ†æ•¸éšå·®åˆ†\n",
    "    for csv_file in csv_files:\n",
    "        product_name = csv_file.stem\n",
    "        \n",
    "        price_series = load_price_data(str(csv_file), price_col)\n",
    "        if price_series is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nè™•ç†å•†å“: {product_name}\")\n",
    "        print(f\"  åŸå§‹è³‡æ–™é»æ•¸: {len(price_series):,}\")\n",
    "        \n",
    "        # ç‚ºæ¯å€‹ d å€¼è¨ˆç®—åˆ†æ•¸éšå·®åˆ†\n",
    "        for d in d_values:\n",
    "            try:\n",
    "                if method == 'FFD':\n",
    "                    fracdiff_series = fracdiff.frac_diff_FFD(price_series, d, thres)\n",
    "                else:\n",
    "                    fracdiff_series = fracdiff.frac_diff(price_series, d, thres)\n",
    "                \n",
    "                # å°é½Šåˆ°ä¸»å•†å“çš„å®Œæ•´ index\n",
    "                fracdiff_aligned = fracdiff_series.reindex(\n",
    "                    main_price.index,\n",
    "                    method=None\n",
    "                )\n",
    "                \n",
    "                # å„²å­˜ç‰¹å¾µï¼ˆæ¬„ä½åç¨±ï¼šå•†å“å_déšæ•¸ï¼‰\n",
    "                col_name = f'{product_name}_d{d}'\n",
    "                features_df[col_name] = fracdiff_aligned\n",
    "                \n",
    "                valid_points = fracdiff_aligned.notna().sum()\n",
    "                print(f\"  d={d}: æœ‰æ•ˆå€¼ {valid_points:,}/{len(fracdiff_aligned):,}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ d={d} è¨ˆç®—å¤±æ•—: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… ç‰¹å¾µç”Ÿæˆå®Œæˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ç¸½å•†å“æ•¸: {len(csv_files)}\")\n",
    "    print(f\"ç¸½ç‰¹å¾µæ•¸: {len(features_df.columns)}\")\n",
    "    print(f\"ç¸½æ™‚é–“é»æ•¸: {len(features_df):,}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# =====================================================\n",
    "# 3. åŸ·è¡Œä¸¦å„²å­˜\n",
    "# =====================================================\n",
    "\n",
    "# åŸ·è¡Œè¨ˆç®—\n",
    "fracdiff_features_all = generate_fracdiff_features_for_all_products(\n",
    "    equity_prices_dir='equity_prices',\n",
    "    main_product='NAS100',\n",
    "    price_col='BidClose',\n",
    "    d_values=[0.5, 0.75, 1.0],  # æ¯å€‹å•†å“è¨ˆç®—é€™ä¸‰å€‹éšæ•¸\n",
    "    method='FFD',\n",
    "    thres=1e-5\n",
    ")\n",
    "\n",
    "# å‘å‰å¡«å…… NaN å€¼\n",
    "fracdiff_features_filled = fracdiff_features_all.fillna(method='ffill')\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "output_file = 'fracdiff_features_all_products.csv'\n",
    "fracdiff_features_filled.to_csv(output_file)\n",
    "print(f\"\\nâœ… å·²ä¿å­˜åˆ°: {output_file}\")\n",
    "print(f\"å„²å­˜æ¨£æœ¬æ•¸: {len(fracdiff_features_filled):,}\")\n",
    "print(f\"ç‰¹å¾µæ•¸: {len(fracdiff_features_filled.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7be784",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ6 ï¼š å…¶ä»–ç‰¹å¾µç”¢ç”Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349eb19",
   "metadata": {},
   "source": [
    "æŠ€è¡“æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"15åˆ†é˜è³‡æ–™ç”¨çš„æŠ€è¡“æŒ‡æ¨™ï¼ˆå››çµ„å°ºåº¦ï¼šSS=4Hã€S=9Hã€M=24Hã€L=72Hï¼‰\"\"\"\n",
    "    tech_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    Open = df['Open']\n",
    "    High = df['High']\n",
    "    Low = df['Low']\n",
    "    Close = df['Close']\n",
    "    Volume = df['Volume']\n",
    "\n",
    "    # é€šç”¨å·¥å…·\n",
    "    def wilders_rma(s, period):\n",
    "        return s.ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "    def true_range(high, low, close):\n",
    "        prev_close = close.shift(1)\n",
    "        return pd.concat([\n",
    "            (high - low),\n",
    "            (high - prev_close).abs(),\n",
    "            (low - prev_close).abs()\n",
    "        ], axis=1).max(axis=1)\n",
    "\n",
    "    def mfi(high, low, close, volume, period=14):\n",
    "        tp = (high + low + close) / 3.0\n",
    "        rmf = tp * volume\n",
    "        pos = (tp > tp.shift(1)).astype(int)\n",
    "        neg = (tp < tp.shift(1)).astype(int)\n",
    "        pos_mf = (rmf * pos).rolling(period).sum()\n",
    "        neg_mf = (rmf * neg).rolling(period).sum()\n",
    "        mr = pos_mf / neg_mf\n",
    "        return 100 - (100 / (1 + mr))\n",
    "\n",
    "    def dm_di_dx(high, low, close, period=14):\n",
    "        up_move = high.diff()\n",
    "        down_move = -low.diff()\n",
    "        plus_dm = ((up_move > down_move) & (up_move > 0)) * up_move\n",
    "        minus_dm = ((down_move > up_move) & (down_move > 0)) * down_move\n",
    "        tr = true_range(high, low, close)\n",
    "        atr = wilders_rma(tr, period)\n",
    "        plus_di = 100 * wilders_rma(plus_dm, period) / atr\n",
    "        minus_di = 100 * wilders_rma(minus_dm, period) / atr\n",
    "        dx = 100 * (plus_di - minus_di).abs() / (plus_di + minus_di)\n",
    "        return plus_dm, minus_dm, plus_di, minus_di, dx\n",
    "\n",
    "    # ========= TRANGE/ATR/NATRï¼ˆåŠ å…¥ SSï¼‰=========\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 14H=56, 24H=96, 72H=288\n",
    "    tech_df['TRANGE'] = true_range(High, Low, Close)\n",
    "    for p, tag in [(16,'SS'), (56,'S'), (96,'M'), (288,'L')]:\n",
    "        atr = wilders_rma(tech_df['TRANGE'], p)\n",
    "        tech_df[f'ATR_{tag}'] = atr\n",
    "        tech_df[f'NATR_{tag}'] = atr / Close * 100.0\n",
    "\n",
    "    # ========= Momentum & Oscillators =========\n",
    "    # MOM / ROC / ROCRï¼ˆåŠ å…¥ SSï¼‰\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 10H=40, 24H=96, 72H=288\n",
    "    for p, tag in [(16,'SS'), (40,'S'), (96,'M'), (288,'L')]:\n",
    "        tech_df[f'MOM_{tag}'] = Close - Close.shift(p)\n",
    "        tech_df[f'ROC_{tag}'] = Close.pct_change(p) * 100.0\n",
    "        tech_df[f'ROCR_{tag}'] = Close / Close.shift(p)\n",
    "\n",
    "    # MFIï¼ˆåŠ å…¥ SSï¼‰\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 14H=56, 24H=96, 72H=288\n",
    "    for p, tag in [(16,'SS'), (56,'S'), (96,'M'), (288,'L')]:\n",
    "        tech_df[f'MFI_{tag}'] = mfi(High, Low, Close, Volume, p)\n",
    "\n",
    "    # ADX/DI/DM/DXï¼ˆåªä¿ç•™ MINUS_DMï¼‰\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 14H=56, 24H=96, 72H=288\n",
    "    for p, tag in [(16,'SS'), (56,'S'), (96,'M'), (288,'L')]:\n",
    "        _, minus_dm, _, _, _ = dm_di_dx(High, Low, Close, p)\n",
    "        tech_df[f'MINUS_DM_{tag}'] = minus_dm\n",
    "\n",
    "    # ========= Volume & Accumulation =========\n",
    "    # Volume_SMA_20: 20å€‹15åˆ†é˜Kç·šï¼ˆç´„5å°æ™‚ï¼‰ï¼Œä¿æŒä¸è®Š\n",
    "    tech_df['Volume_SMA_20'] = Volume.rolling(20).mean()\n",
    "    tech_df['Volume_Ratio_20'] = Volume / tech_df['Volume_SMA_20']\n",
    "\n",
    "    return tech_df\n",
    "\n",
    "# è¨ˆç®—æŠ€è¡“æŒ‡æ¨™\n",
    "tech_indicators = calculate_technical_indicators(nas100_raw)  # ä½¿ç”¨15åˆ†é˜åŸå§‹è³‡æ–™\n",
    "print(f\"æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œæˆï¼Œå…± {len(tech_indicators.columns)} å€‹æŒ‡æ¨™\")\n",
    "\n",
    "# æª¢æŸ¥è³‡æ–™å“è³ª\n",
    "print(f\"\\nè³‡æ–™å“è³ªæª¢æŸ¥:\")\n",
    "print(f\"ç¸½ç­†æ•¸: {len(tech_indicators)}\")\n",
    "print(f\"ç¼ºå¤±å€¼çµ±è¨ˆ:\")\n",
    "missing_stats = tech_indicators.isnull().sum()\n",
    "print(missing_stats[missing_stats > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27180da",
   "metadata": {},
   "source": [
    "åŸºç¤çµ±è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¢å‹•ç‡\n",
    "\n",
    "def rogers_satchell_volatility(df, window=None):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Rogersâ€“Satchell Volatilityï¼ˆå…è¨±æ¼‚ç§»ï¼‰\n",
    "    \n",
    "    å…¬å¼:\n",
    "    Ïƒ_RS = sqrt((1/N) * Î£[ln(H_t/C_t) * ln(H_t/O_t) + ln(L_t/C_t) * ln(L_t/O_t)])\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        éœ€åŒ…å« 'Open', 'High', 'Low', 'Close' æ¬„ä½\n",
    "    window : int, optional\n",
    "        æ»¾å‹•çª—å£ï¼ˆå¦‚æœç‚º Noneï¼Œå‰‡è¨ˆç®—æ•´é«”æ³¢å‹•ç‡ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    volatility : pd.Series or float\n",
    "        å¦‚æœ window ç‚º Noneï¼Œè¿”å›å–®ä¸€æ•¸å€¼\n",
    "        å¦‚æœ window æœ‰å€¼ï¼Œè¿”å›æ»¾å‹•æ³¢å‹•ç‡åºåˆ—\n",
    "    \"\"\"\n",
    "    # ç¢ºä¿æœ‰å¿…è¦çš„æ¬„ä½\n",
    "    required_cols = ['Open', 'High', 'Low', 'Close']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"ç¼ºå°‘å¿…è¦æ¬„ä½: {col}\")\n",
    "    \n",
    "    Open = df['Open']\n",
    "    High = df['High']\n",
    "    Low = df['Low']\n",
    "    Close = df['Close']\n",
    "    \n",
    "    # è¨ˆç®—å°æ•¸é …\n",
    "    # ln(H_t/C_t) * ln(H_t/O_t) + ln(L_t/C_t) * ln(L_t/O_t)\n",
    "    term1 = np.log(High / Close) * np.log(High / Open)\n",
    "    term2 = np.log(Low / Close) * np.log(Low / Open)\n",
    "    \n",
    "    # çµ„åˆé …\n",
    "    rs_terms = term1 + term2\n",
    "    \n",
    "    # è™•ç†ç„¡æ•ˆå€¼ï¼ˆé¿å… log(0) æˆ–è² æ•¸ï¼‰\n",
    "    rs_terms = rs_terms.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    if window is None:\n",
    "        # è¨ˆç®—æ•´é«”æ³¢å‹•ç‡\n",
    "        rs_terms_clean = rs_terms.dropna()\n",
    "        if len(rs_terms_clean) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        mean_term = rs_terms_clean.mean()\n",
    "        volatility = np.sqrt(mean_term)\n",
    "        return volatility\n",
    "    else:\n",
    "        # è¨ˆç®—æ»¾å‹•æ³¢å‹•ç‡\n",
    "        mean_terms = rs_terms.rolling(window=window, min_periods=1).mean()\n",
    "        volatility = np.sqrt(mean_terms)\n",
    "        return volatility\n",
    "\n",
    "# =====================================================\n",
    "# è¼‰å…¥è³‡æ–™ä¸¦è¨ˆç®—\n",
    "# =====================================================\n",
    "\n",
    "file_path = r'C:\\Users\\a124a\\OneDrive\\æ¡Œé¢\\ç­–ç•¥é–‹ç™¼\\Xgboost_MlFinlab\\minute_prices\\NAS100.csv'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š è¨ˆç®— Rogersâ€“Satchell Volatility\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è®€å–è³‡æ–™\n",
    "print(f\"\\nè¼‰å…¥è³‡æ–™: {file_path}\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # è½‰æ›æ™‚é–“æ ¼å¼\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "    elif df.index.dtype == 'object':\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # æª¢æŸ¥ä¸¦é¸æ“‡åƒ¹æ ¼æ¬„ä½\n",
    "    if 'BidOpen' in df.columns and 'BidHigh' in df.columns:\n",
    "        df['Open'] = df['BidOpen']\n",
    "        df['High'] = df['BidHigh']\n",
    "        df['Low'] = df['BidLow']\n",
    "        df['Close'] = df['BidClose']\n",
    "    elif 'Open' not in df.columns:\n",
    "        print(\"âš ï¸ æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½ï¼Œå˜—è©¦ä½¿ç”¨ Close æ¬„ä½\")\n",
    "        if 'Close' in df.columns:\n",
    "            df['Open'] = df['Close'].shift(1).fillna(df['Close'])\n",
    "            df['High'] = df['Close']\n",
    "            df['Low'] = df['Close']\n",
    "            df['Close'] = df['Close']\n",
    "    \n",
    "    print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ\")\n",
    "    print(f\"   è³‡æ–™ç­†æ•¸: {len(df):,}\")\n",
    "    print(f\"   è³‡æ–™æœŸé–“: {df.index[0]} è‡³ {df.index[-1]}\")\n",
    "    print(f\"   æ¬„ä½: {list(df.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}\")\n",
    "    raise\n",
    "\n",
    "# è¨ˆç®—æ•´é«” Rogersâ€“Satchell Volatility\n",
    "print(f\"\\nè¨ˆç®—æ•´é«” Rogersâ€“Satchell Volatility...\")\n",
    "rs_vol_overall = rogers_satchell_volatility(df, window=None)\n",
    "print(f\"âœ… æ•´é«”æ³¢å‹•ç‡: {rs_vol_overall:.6f}\")\n",
    "\n",
    "# è¨ˆç®—æ»¾å‹• Rogersâ€“Satchell Volatilityï¼ˆå¯é¸ï¼‰\n",
    "print(f\"\\nè¨ˆç®—æ»¾å‹• Rogersâ€“Satchell Volatility...\")\n",
    "# ä½¿ç”¨ä¸åŒçš„çª—å£å¤§å°\n",
    "windows = [20, 60, 240, 1440]  # 20åˆ†é˜, 1å°æ™‚, 4å°æ™‚, 1å¤©ï¼ˆå‡è¨­æ˜¯åˆ†é˜è³‡æ–™ï¼‰\n",
    "\n",
    "for window in windows:\n",
    "    rs_vol_rolling = rogers_satchell_volatility(df, window=window)\n",
    "    rs_vol_rolling_clean = rs_vol_rolling.dropna()\n",
    "    \n",
    "    if len(rs_vol_rolling_clean) > 0:\n",
    "        mean_vol = rs_vol_rolling_clean.mean()\n",
    "        std_vol = rs_vol_rolling_clean.std()\n",
    "        print(f\"  çª—å£ {window} ({window}åˆ†é˜):\")\n",
    "        print(f\"    å¹³å‡æ³¢å‹•ç‡: {mean_vol:.6f}\")\n",
    "        print(f\"    æ¨™æº–å·®: {std_vol:.6f}\")\n",
    "        print(f\"    æœ€å°å€¼: {rs_vol_rolling_clean.min():.6f}\")\n",
    "        print(f\"    æœ€å¤§å€¼: {rs_vol_rolling_clean.max():.6f}\")\n",
    "\n",
    "# å°‡æ»¾å‹•æ³¢å‹•ç‡åŠ å…¥ DataFrame\n",
    "df['RS_Volatility_20'] = rogers_satchell_volatility(df, window=20)\n",
    "df['RS_Volatility_60'] = rogers_satchell_volatility(df, window=60)\n",
    "df['RS_Volatility_240'] = rogers_satchell_volatility(df, window=240)\n",
    "df['RS_Volatility_1440'] = rogers_satchell_volatility(df, window=1440)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… è¨ˆç®—å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataFrame æ–°å¢æ¬„ä½:\")\n",
    "print(f\"  - RS_Volatility_20 (20åˆ†é˜æ»¾å‹•)\")\n",
    "print(f\"  - RS_Volatility_60 (60åˆ†é˜æ»¾å‹•)\")\n",
    "print(f\"  - RS_Volatility_240 (4å°æ™‚æ»¾å‹•)\")\n",
    "print(f\"  - RS_Volatility_1440 (1å¤©æ»¾å‹•)\")\n",
    "\n",
    "# é¡¯ç¤ºæœ€å¾Œå¹¾ç­†è³‡æ–™\n",
    "print(f\"\\næœ€å¾Œ 5 ç­†è³‡æ–™:\")\n",
    "print(df[['Open', 'High', 'Low', 'Close', 'RS_Volatility_20', 'RS_Volatility_60']].tail())\n",
    "\n",
    "# å¯é¸ï¼šå„²å­˜çµæœ\n",
    "# output_file = 'NAS100_RS_Volatility.csv'\n",
    "# df.to_csv(output_file)\n",
    "# print(f\"\\nâœ… å·²ä¿å­˜åˆ°: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd037c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# 1ï¸âƒ£ Skewness / Kurtosis\n",
    "# =====================================================\n",
    "\n",
    "def rolling_skewness_kurtosis(returns, window=60):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ»¾å‹•ååº¦å’Œå³°åº¦\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°ï¼ˆåˆ†é˜æ•¸ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    skewness : pd.Series\n",
    "        ååº¦åºåˆ—\n",
    "    kurtosis : pd.Series\n",
    "        å³°åº¦åºåˆ—\n",
    "    \"\"\"\n",
    "    skewness = returns.rolling(window=window, min_periods=window).skew()\n",
    "    kurtosis = returns.rolling(window=window, min_periods=window).kurt()\n",
    "    \n",
    "    return skewness, kurtosis\n",
    "\n",
    "# =====================================================\n",
    "# 2ï¸âƒ£ Drawdown / Recovery Rate\n",
    "# =====================================================\n",
    "\n",
    "def drawdown_recovery_rate(prices, returns, window=None):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æœ€å¤§å›æ’¤å’Œå›å¾©é€Ÿåº¦\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    prices : pd.Series\n",
    "        åƒ¹æ ¼åºåˆ—\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int, optional\n",
    "        æ»¾å‹•çª—å£ï¼ˆå¦‚æœç‚º Noneï¼Œè¨ˆç®—æ•´é«”å›æ’¤ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    drawdown : pd.Series\n",
    "        å›æ’¤åºåˆ—\n",
    "    recovery_rate : pd.Series\n",
    "        å›å¾©é€Ÿåº¦åºåˆ—\n",
    "    max_drawdown : pd.Series\n",
    "        æœ€å¤§å›æ’¤åºåˆ—\n",
    "    \"\"\"\n",
    "    if window is None:\n",
    "        # æ•´é«”å›æ’¤\n",
    "        cummax = prices.cummax()\n",
    "        drawdown = 1 - prices / cummax\n",
    "        \n",
    "        # å›å¾©é€Ÿåº¦\n",
    "        recovery_rate = returns / drawdown.replace(0, np.nan)\n",
    "        \n",
    "        # æœ€å¤§å›æ’¤ï¼ˆç´¯ç©ï¼‰\n",
    "        max_drawdown = drawdown.cummax()\n",
    "        \n",
    "    else:\n",
    "        # æ»¾å‹•çª—å£å›æ’¤\n",
    "        drawdown_list = []\n",
    "        recovery_rate_list = []\n",
    "        max_drawdown_list = []\n",
    "        \n",
    "        for i in range(len(prices)):\n",
    "            if i < window:\n",
    "                drawdown_list.append(np.nan)\n",
    "                recovery_rate_list.append(np.nan)\n",
    "                max_drawdown_list.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            window_prices = prices.iloc[i-window:i+1]\n",
    "            window_returns = returns.iloc[i-window:i+1]\n",
    "            \n",
    "            # çª—å£å…§çš„æœ€é«˜åƒ¹\n",
    "            window_cummax = window_prices.cummax()\n",
    "            \n",
    "            # å›æ’¤\n",
    "            window_dd = 1 - window_prices / window_cummax\n",
    "            drawdown_list.append(window_dd.iloc[-1])\n",
    "            \n",
    "            # å›å¾©é€Ÿåº¦\n",
    "            if window_dd.iloc[-1] > 0:\n",
    "                recovery = window_returns.iloc[-1] / window_dd.iloc[-1]\n",
    "            else:\n",
    "                recovery = np.nan\n",
    "            recovery_rate_list.append(recovery)\n",
    "            \n",
    "            # æœ€å¤§å›æ’¤\n",
    "            max_drawdown_list.append(window_dd.max())\n",
    "        \n",
    "        drawdown = pd.Series(drawdown_list, index=prices.index)\n",
    "        recovery_rate = pd.Series(recovery_rate_list, index=prices.index)\n",
    "        max_drawdown = pd.Series(max_drawdown_list, index=prices.index)\n",
    "    \n",
    "    return drawdown, recovery_rate, max_drawdown\n",
    "\n",
    "# =====================================================\n",
    "# 3ï¸âƒ£ Volatility Clustering Index (VCI)\n",
    "# =====================================================\n",
    "\n",
    "def volatility_clustering_index(returns, window=60):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ³¢å‹•ç¾¤èšæŒ‡æ•¸\n",
    "    \n",
    "    ä½¿ç”¨å ±é…¬çµ•å°å€¼çš„è‡ªç›¸é—œä¿‚æ•¸\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    vci : pd.Series\n",
    "        æ³¢å‹•ç¾¤èšæŒ‡æ•¸åºåˆ—\n",
    "    \"\"\"\n",
    "    abs_returns = np.abs(returns)\n",
    "    \n",
    "    def calc_vci(x):\n",
    "        if len(x) < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        # è¨ˆç®—ä¸€éšè‡ªç›¸é—œ\n",
    "        autocorr = pd.Series(x).autocorr(lag=1)\n",
    "        return autocorr if not pd.isna(autocorr) else 0.0\n",
    "    \n",
    "    vci = abs_returns.rolling(window=window, min_periods=window).apply(calc_vci, raw=False)\n",
    "    \n",
    "    return vci\n",
    "\n",
    "def volatility_clustering_garch_approximation(returns, window=60):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ GARCH(1,1) è¿‘ä¼¼è¨ˆç®—æ³¢å‹•ç¾¤èšæŒ‡æ•¸\n",
    "    \n",
    "    ç°¡åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ Î± + Î² çš„ä¼°è¨ˆå€¼\n",
    "    å…¶ä¸­ Î± å’Œ Î² ä¾†è‡ªç°¡åŒ–çš„ GARCH æ¨¡å‹\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    vci_garch : pd.Series\n",
    "        GARCH è¿‘ä¼¼çš„æ³¢å‹•ç¾¤èšæŒ‡æ•¸\n",
    "    \"\"\"\n",
    "    def calc_garch_params(x):\n",
    "        if len(x) < 20:\n",
    "            return np.nan\n",
    "        \n",
    "        # ç°¡åŒ– GARCH(1,1) åƒæ•¸ä¼°è¨ˆ\n",
    "        # ä½¿ç”¨ ARCH(1) æ¨¡å‹è¿‘ä¼¼\n",
    "        squared_returns = x ** 2\n",
    "        lagged_squared = squared_returns[:-1]\n",
    "        current_squared = squared_returns[1:]\n",
    "        \n",
    "        # ç°¡å–®ç·šæ€§è¿´æ­¸ï¼šÏƒÂ²_t = Î±â‚€ + Î±â‚ * rÂ²_{t-1}\n",
    "        if len(lagged_squared) < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        # è¨ˆç®—è‡ªç›¸é—œä½œç‚ºè¿‘ä¼¼\n",
    "        autocorr = pd.Series(squared_returns).autocorr(lag=1)\n",
    "        \n",
    "        # Î± + Î² çš„è¿‘ä¼¼ï¼ˆç°¡åŒ–ï¼‰\n",
    "        alpha_beta_sum = autocorr if not pd.isna(autocorr) else 0.0\n",
    "        \n",
    "        return alpha_beta_sum\n",
    "    \n",
    "    vci_garch = returns.rolling(window=window, min_periods=window).apply(calc_garch_params, raw=True)\n",
    "    \n",
    "    return vci_garch\n",
    "\n",
    "# =====================================================\n",
    "# 4ï¸âƒ£ Entropy of Volume\n",
    "# =====================================================\n",
    "\n",
    "def entropy_of_volume(volume, window=60):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æˆäº¤é‡è³‡è¨Šç†µ\n",
    "    \n",
    "    å°æˆäº¤é‡æ­£è¦åŒ–å¾Œå–é¦™è¾²ç†µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    volume : pd.Series\n",
    "        æˆäº¤é‡åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    volume_entropy : pd.Series\n",
    "        æˆäº¤é‡ç†µåºåˆ—\n",
    "    \"\"\"\n",
    "    def calc_volume_entropy(x):\n",
    "        if len(x) == 0 or x.sum() == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # æ­£è¦åŒ–ï¼ˆè½‰ç‚ºæ©Ÿç‡åˆ†å¸ƒï¼‰\n",
    "        p = x / x.sum()\n",
    "        \n",
    "        # ç§»é™¤é›¶å€¼ï¼ˆé¿å… log(0)ï¼‰\n",
    "        p = p[p > 0]\n",
    "        \n",
    "        if len(p) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # è¨ˆç®—é¦™è¾²ç†µ\n",
    "        entropy = -np.sum(p * np.log2(p))\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    volume_entropy = volume.rolling(window=window, min_periods=window).apply(calc_volume_entropy, raw=True)\n",
    "    \n",
    "    return volume_entropy\n",
    "\n",
    "def entropy_of_volume_binned(volume, window=60, n_bins=10):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨åˆ†ç®±æ–¹æ³•è¨ˆç®—æˆäº¤é‡ç†µï¼ˆæ›´ç©©å®šï¼‰\n",
    "    \n",
    "    å°‡æˆäº¤é‡åˆ†ç‚º n_bins å€‹å€é–“ï¼Œè¨ˆç®—å„å€é–“å‡ºç¾é »ç‡çš„ç†µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    volume : pd.Series\n",
    "        æˆäº¤é‡åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "    n_bins : int\n",
    "        åˆ†ç®±æ•¸é‡\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    volume_entropy : pd.Series\n",
    "        æˆäº¤é‡ç†µåºåˆ—\n",
    "    \"\"\"\n",
    "    def calc_entropy_binned(x):\n",
    "        if len(x) < n_bins:\n",
    "            return np.nan\n",
    "        \n",
    "        # åˆ†ç®±\n",
    "        try:\n",
    "            bins = pd.cut(x, bins=n_bins, labels=False, duplicates='drop')\n",
    "            # è¨ˆç®—å„ç®±çš„é »ç‡\n",
    "            counts = pd.Series(bins).value_counts()\n",
    "            p = counts / len(bins)\n",
    "            \n",
    "            # ç§»é™¤é›¶å€¼\n",
    "            p = p[p > 0]\n",
    "            \n",
    "            if len(p) == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # è¨ˆç®—ç†µ\n",
    "            entropy = -np.sum(p * np.log2(p))\n",
    "            \n",
    "            return entropy\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    volume_entropy = volume.rolling(window=window, min_periods=window).apply(calc_entropy_binned, raw=True)\n",
    "    \n",
    "    return volume_entropy\n",
    "\n",
    "# =====================================================\n",
    "# ä¸»ç¨‹å¼ï¼šè¼‰å…¥è³‡æ–™ä¸¦è¨ˆç®—æ‰€æœ‰é™„åŠ ç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "file_path = r'C:\\Users\\a124a\\OneDrive\\æ¡Œé¢\\ç­–ç•¥é–‹ç™¼\\Xgboost_MlFinlab\\minute_prices\\NAS100.csv'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š é™„åŠ å»¶ä¼¸ç‰¹å¾µè¨ˆç®—\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "print(f\"\\nè¼‰å…¥è³‡æ–™: {file_path}\")\n",
    "# å…ˆè®€å–å‰10è¬ç­†æ¸¬è©¦ï¼ˆå¯æ ¹æ“šéœ€è¦èª¿æ•´ï¼‰\n",
    "df = pd.read_csv(file_path, nrows=100000)\n",
    "\n",
    "# è½‰æ›æ™‚é–“æ ¼å¼\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "else:\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# é¸æ“‡åƒ¹æ ¼å’Œæˆäº¤é‡æ¬„ä½\n",
    "if 'BidClose' in df.columns:\n",
    "    df['Close'] = df['BidClose']\n",
    "    if 'BidHigh' in df.columns:\n",
    "        df['High'] = df['BidHigh']\n",
    "    if 'BidLow' in df.columns:\n",
    "        df['Low'] = df['BidLow']\n",
    "    if 'BidOpen' in df.columns:\n",
    "        df['Open'] = df['BidOpen']\n",
    "elif 'Close' not in df.columns:\n",
    "    raise ValueError(\"æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½\")\n",
    "\n",
    "if 'Volume' not in df.columns:\n",
    "    if 'BidVolume' in df.columns:\n",
    "        df['Volume'] = df['BidVolume']\n",
    "    else:\n",
    "        raise ValueError(\"æ‰¾ä¸åˆ°æˆäº¤é‡æ¬„ä½\")\n",
    "\n",
    "print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ\")\n",
    "print(f\"   è³‡æ–™ç­†æ•¸: {len(df):,}\")\n",
    "print(f\"   è³‡æ–™æœŸé–“: {df.index[0]} è‡³ {df.index[-1]}\")\n",
    "\n",
    "# è¨ˆç®—å ±é…¬\n",
    "df['Returns'] = df['Close'].pct_change()\n",
    "\n",
    "# =====================================================\n",
    "# 1. Skewness / Kurtosis\n",
    "# =====================================================\n",
    "print(f\"\\n1ï¸âƒ£ è¨ˆç®— Skewness / Kurtosis...\")\n",
    "df['Skewness_30'], df['Kurtosis_30'] = rolling_skewness_kurtosis(df['Returns'], window=30)\n",
    "df['Skewness_60'], df['Kurtosis_60'] = rolling_skewness_kurtosis(df['Returns'], window=60)\n",
    "df['Skewness_240'], df['Kurtosis_60'] = rolling_skewness_kurtosis(df['Returns'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Drawdown / Recovery Rate\n",
    "# =====================================================\n",
    "print(f\"\\n2ï¸âƒ£ è¨ˆç®— Drawdown / Recovery Rate...\")\n",
    "# æ•´é«”å›æ’¤\n",
    "dd_overall, rr_overall, mdd_overall = drawdown_recovery_rate(\n",
    "    df['Close'], df['Returns'], window=None\n",
    ")\n",
    "df['Drawdown_Overall'] = dd_overall\n",
    "df['Recovery_Rate_Overall'] = rr_overall\n",
    "df['Max_Drawdown_Overall'] = mdd_overall\n",
    "\n",
    "# æ»¾å‹•å›æ’¤\n",
    "dd_60, rr_60, mdd_60 = drawdown_recovery_rate(\n",
    "    df['Close'], df['Returns'], window=60\n",
    ")\n",
    "df['Drawdown_60'] = dd_60\n",
    "df['Recovery_Rate_60'] = rr_60\n",
    "df['Max_Drawdown_60'] = mdd_60\n",
    "\n",
    "dd_240, rr_240, mdd_240 = drawdown_recovery_rate(\n",
    "    df['Close'], df['Returns'], window=240\n",
    ")\n",
    "df['Drawdown_240'] = dd_240\n",
    "df['Recovery_Rate_240'] = rr_240\n",
    "df['Max_Drawdown_240'] = mdd_240\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. Volatility Clustering Index (VCI)\n",
    "# =====================================================\n",
    "print(f\"\\n3ï¸âƒ£ è¨ˆç®— Volatility Clustering Index...\")\n",
    "df['VCI_30'] = volatility_clustering_index(df['Returns'], window=30)\n",
    "df['VCI_60'] = volatility_clustering_index(df['Returns'], window=60)\n",
    "df['VCI_240'] = volatility_clustering_index(df['Returns'], window=240)\n",
    "\n",
    "# GARCH è¿‘ä¼¼ç‰ˆæœ¬\n",
    "df['VCI_GARCH_60'] = volatility_clustering_garch_approximation(df['Returns'], window=60)\n",
    "df['VCI_GARCH_240'] = volatility_clustering_garch_approximation(df['Returns'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Entropy of Volume\n",
    "# =====================================================\n",
    "print(f\"\\n4ï¸âƒ£ è¨ˆç®— Entropy of Volume...\")\n",
    "df['Volume_Entropy_30'] = entropy_of_volume(df['Volume'], window=30)\n",
    "df['Volume_Entropy_60'] = entropy_of_volume(df['Volume'], window=60)\n",
    "df['Volume_Entropy_240'] = entropy_of_volume(df['Volume'], window=240)\n",
    "\n",
    "# åˆ†ç®±ç‰ˆæœ¬ï¼ˆæ›´ç©©å®šï¼‰\n",
    "df['Volume_Entropy_Binned_60'] = entropy_of_volume_binned(df['Volume'], window=60, n_bins=10)\n",
    "df['Volume_Entropy_Binned_240'] = entropy_of_volume_binned(df['Volume'], window=240, n_bins=10)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# çµæœçµ±è¨ˆ\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… è¨ˆç®—å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\næ–°å¢æ¬„ä½:\")\n",
    "print(f\"  - Skewness_30/60/240: ååº¦ï¼ˆä¸åŒçª—å£ï¼‰\")\n",
    "print(f\"  - Kurtosis_30/60/240: å³°åº¦ï¼ˆä¸åŒçª—å£ï¼‰\")\n",
    "print(f\"  - Drawdown_Overall/60/240: å›æ’¤ï¼ˆæ•´é«”/æ»¾å‹•ï¼‰\")\n",
    "print(f\"  - Recovery_Rate_Overall/60/240: å›å¾©é€Ÿåº¦\")\n",
    "print(f\"  - Max_Drawdown_Overall/60/240: æœ€å¤§å›æ’¤\")\n",
    "print(f\"  - VCI_30/60/240: æ³¢å‹•ç¾¤èšæŒ‡æ•¸ï¼ˆè‡ªç›¸é—œæ–¹æ³•ï¼‰\")\n",
    "print(f\"  - VCI_GARCH_60/240: æ³¢å‹•ç¾¤èšæŒ‡æ•¸ï¼ˆGARCH è¿‘ä¼¼ï¼‰\")\n",
    "print(f\"  - Volume_Entropy_30/60/240: æˆäº¤é‡ç†µ\")\n",
    "print(f\"  - Volume_Entropy_Binned_60/240: æˆäº¤é‡ç†µï¼ˆåˆ†ç®±ç‰ˆæœ¬ï¼‰\")\n",
    "\n",
    "# é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "print(f\"\\nç‰¹å¾µçµ±è¨ˆ:\")\n",
    "extended_cols = [\n",
    "    col for col in df.columns \n",
    "    if any(x in col for x in ['Skewness', 'Kurtosis', 'Drawdown', 'Recovery', 'Max_Drawdown', \n",
    "                               'VCI', 'Volume_Entropy'])\n",
    "]\n",
    "for col in extended_cols:\n",
    "    valid = df[col].notna().sum()\n",
    "    if valid > 0:\n",
    "        mean_val = df[col].mean()\n",
    "        std_val = df[col].std()\n",
    "        print(f\"  {col}: æœ‰æ•ˆå€¼ {valid:,}, å¹³å‡ {mean_val:.6f}, æ¨™æº–å·® {std_val:.6f}\")\n",
    "\n",
    "# é¡¯ç¤ºæœ€å¾Œå¹¾ç­†\n",
    "print(f\"\\næœ€å¾Œ 5 ç­†è³‡æ–™:\")\n",
    "display_cols = [\n",
    "    'Close', 'Volume', 'Returns',\n",
    "    'Skewness_60', 'Kurtosis_60',\n",
    "    'Drawdown_60', 'Recovery_Rate_60',\n",
    "    'VCI_60', 'Volume_Entropy_60'\n",
    "]\n",
    "available_cols = [col for col in display_cols if col in df.columns]\n",
    "print(df[available_cols].tail())\n",
    "\n",
    "# # å„²å­˜çµæœ\n",
    "# output_file = 'NAS100_Extended_Features.csv'\n",
    "# df.to_csv(output_file)\n",
    "# print(f\"\\nâœ… å·²ä¿å­˜åˆ°: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86535cb5",
   "metadata": {},
   "source": [
    "é‡åƒ¹è¡Œç‚º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44897089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# 1ï¸âƒ£ CUSUM Statisticï¼ˆç´¯ç©å’Œæª¢å®šï¼‰\n",
    "# =====================================================\n",
    "\n",
    "def cusum_statistic(returns, window=None):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— CUSUM Statisticï¼ˆç´¯ç©å’Œæª¢å®šï¼‰\n",
    "    \n",
    "    å…¬å¼: S_t = Î£(x_i - xÌ„)\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int, optional\n",
    "        æ»¾å‹•çª—å£ï¼ˆå¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨æ•´é«”å‡å€¼ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cusum_stat : pd.Series\n",
    "        CUSUM çµ±è¨ˆé‡\n",
    "    trigger_flag : pd.Series\n",
    "        è§¸ç™¼äº‹ä»¶æ¨™è¨˜ï¼ˆç•¶ |S_t - S_{t-1}| > threshold æ™‚ï¼‰\n",
    "    \"\"\"\n",
    "    if window is None:\n",
    "        mean_return = returns.mean()\n",
    "    else:\n",
    "        mean_return = returns.rolling(window=window).mean()\n",
    "    \n",
    "    # è¨ˆç®—ç´¯ç©åå·®\n",
    "    deviation = returns - mean_return\n",
    "    cusum_stat = deviation.cumsum()\n",
    "    \n",
    "    # è¨ˆç®—è®ŠåŒ–é‡\n",
    "    cusum_change = cusum_stat.diff().abs()\n",
    "    \n",
    "    return cusum_stat, cusum_change\n",
    "\n",
    "def cusum_filter_events(returns, threshold, window=None):\n",
    "    \"\"\"\n",
    "    CUSUM Filter - åµæ¸¬äº‹ä»¶æ™‚é–“é»\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    threshold : float\n",
    "        è§¸ç™¼é–¾å€¼\n",
    "    window : int, optional\n",
    "        æ»¾å‹•çª—å£\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    events : pd.DatetimeIndex\n",
    "        äº‹ä»¶æ™‚é–“é»\n",
    "    \"\"\"\n",
    "    _, cusum_change = cusum_statistic(returns, window)\n",
    "    \n",
    "    # è§¸ç™¼äº‹ä»¶\n",
    "    events = cusum_change[cusum_change > threshold].index\n",
    "    \n",
    "    return pd.DatetimeIndex(events)\n",
    "\n",
    "# =====================================================\n",
    "# 2ï¸âƒ£ Brownâ€“Durbinâ€“Evans (BDE) CUSUM Test\n",
    "# =====================================================\n",
    "\n",
    "def bde_cusum_test(y, X, k=None):\n",
    "    \"\"\"\n",
    "    Brownâ€“Durbinâ€“Evans CUSUM Testï¼ˆéè¿´æ®˜å·®CUSUMï¼‰\n",
    "    \n",
    "    æª¢æ¸¬è¿´æ­¸æ¨¡å‹ä¿‚æ•¸æ˜¯å¦éš¨æ™‚é–“æ”¹è®Š\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    y : pd.Series or np.array\n",
    "        å› è®Šæ•¸\n",
    "    X : pd.DataFrame or np.array\n",
    "        è‡ªè®Šæ•¸ï¼ˆéœ€åŒ…å«å¸¸æ•¸é …ï¼‰\n",
    "    k : int, optional\n",
    "        åˆå§‹æ¨£æœ¬æ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ X.shape[1] + 1ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cusum_stat : pd.Series\n",
    "        CUSUM çµ±è¨ˆé‡\n",
    "    break_flag : pd.Series\n",
    "        çµæ§‹è®Šå‹•æ¨™è¨˜\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    if len(y) != len(X):\n",
    "        raise ValueError(\"y å’Œ X é•·åº¦ä¸ä¸€è‡´\")\n",
    "    \n",
    "    n = len(y)\n",
    "    if k is None:\n",
    "        k = X.shape[1] + 1\n",
    "    \n",
    "    # å„²å­˜éè¿´æ®˜å·®\n",
    "    recursive_residuals = []\n",
    "    cusum_stats = []\n",
    "    \n",
    "    # å¾ k+1 é–‹å§‹è¨ˆç®—éè¿´æ®˜å·®\n",
    "    for t in range(k, n):\n",
    "        # ä½¿ç”¨å‰ t å€‹è§€æ¸¬å€¼ä¼°è¨ˆåƒæ•¸\n",
    "        y_t = y[:t]\n",
    "        X_t = X[:t]\n",
    "        \n",
    "        try:\n",
    "            # OLS ä¼°è¨ˆ\n",
    "            model = OLS(y_t, X_t).fit()\n",
    "            \n",
    "            # é æ¸¬ç¬¬ t+1 æœŸ\n",
    "            x_t1 = X[t].reshape(1, -1)\n",
    "            y_pred = model.predict(x_t1)[0]\n",
    "            \n",
    "            # éè¿´æ®˜å·®\n",
    "            residual = y[t] - y_pred\n",
    "            \n",
    "            # æ¨™æº–åŒ–\n",
    "            # ç°¡åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨æ®˜å·®æ¨™æº–å·®\n",
    "            if t > k:\n",
    "                sigma_hat = np.std(recursive_residuals)\n",
    "            else:\n",
    "                sigma_hat = model.resid.std()\n",
    "            \n",
    "            if sigma_hat > 0:\n",
    "                standardized_residual = residual / sigma_hat\n",
    "            else:\n",
    "                standardized_residual = 0\n",
    "            \n",
    "            recursive_residuals.append(standardized_residual)\n",
    "            \n",
    "            # CUSUM çµ±è¨ˆé‡\n",
    "            cusum = np.sum(recursive_residuals) / np.sqrt(len(recursive_residuals))\n",
    "            cusum_stats.append(cusum)\n",
    "            \n",
    "        except:\n",
    "            recursive_residuals.append(0)\n",
    "            cusum_stats.append(0)\n",
    "    \n",
    "    # å»ºç«‹çµæœ Series\n",
    "    index = pd.RangeIndex(start=k, stop=n)\n",
    "    cusum_stat = pd.Series(cusum_stats, index=index)\n",
    "    \n",
    "    # è‡¨ç•Œå€¼ï¼ˆç°¡åŒ–ï¼šä½¿ç”¨ Â±2 ä½œç‚ºè¿‘ä¼¼ï¼‰\n",
    "    critical_value = 2.0\n",
    "    break_flag = (cusum_stat.abs() > critical_value).astype(int)\n",
    "    \n",
    "    return cusum_stat, break_flag\n",
    "\n",
    "# =====================================================\n",
    "# 3ï¸âƒ£ Chuâ€“Stinchcombeâ€“White (CSW) CUSUM Test\n",
    "# =====================================================\n",
    "\n",
    "def csw_cusum_test(log_prices, n_start=1, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Chuâ€“Stinchcombeâ€“White CUSUM Testï¼ˆåƒ¹æ ¼æ°´æº–CUSUMï¼‰\n",
    "    \n",
    "    æª¢æ¸¬ log-price æ˜¯å¦åé›¢ç©©æ…‹\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    log_prices : pd.Series\n",
    "        Log åƒ¹æ ¼åºåˆ—\n",
    "    n_start : int\n",
    "        èµ·å§‹é» n\n",
    "    alpha : float\n",
    "        é¡¯è‘—æ°´æº–ï¼ˆé è¨­ 0.05ï¼Œå°æ‡‰ b_alpha â‰ˆ 4.6ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cusum_stat : pd.Series\n",
    "        CUSUM çµ±è¨ˆé‡ S_{n,t}\n",
    "    break_flag : pd.Series\n",
    "        çµæ§‹è®Šå‹•æ¨™è¨˜\n",
    "    \"\"\"\n",
    "    y = log_prices.values\n",
    "    n = len(y)\n",
    "    \n",
    "    # b_alpha å€¼ï¼ˆæ ¹æ“šæ–‡ç»ï¼Œalpha=0.05 æ™‚ bâ‰ˆ4.6ï¼‰\n",
    "    b_alpha_dict = {0.05: 4.6, 0.01: 5.2, 0.10: 4.0}\n",
    "    b_alpha = b_alpha_dict.get(alpha, 4.6)\n",
    "    \n",
    "    cusum_stats = []\n",
    "    break_flags = []\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹ t çš„ S_{n,t}\n",
    "    for t in range(n_start + 1, n):\n",
    "        # è¨ˆç®— ÏƒÌ‚_t^2\n",
    "        delta_y = np.diff(y[1:t+1])\n",
    "        sigma_sq = np.var(delta_y) if len(delta_y) > 0 else 1.0\n",
    "        sigma_t = np.sqrt(sigma_sq) if sigma_sq > 0 else 1.0\n",
    "        \n",
    "        # è¨ˆç®— S_{n,t}\n",
    "        y_t = y[t]\n",
    "        y_n = y[n_start]\n",
    "        t_minus_n = t - n_start\n",
    "        \n",
    "        if sigma_t > 0 and t_minus_n > 0:\n",
    "            s_n_t = (y_t - y_n) / (sigma_t * np.sqrt(t_minus_n))\n",
    "        else:\n",
    "            s_n_t = 0\n",
    "        \n",
    "        cusum_stats.append(s_n_t)\n",
    "        \n",
    "        # è‡¨ç•Œå€¼ c_alpha[n,t] = sqrt(b_alpha + log(t-n))\n",
    "        c_alpha = np.sqrt(b_alpha + np.log(t_minus_n))\n",
    "        \n",
    "        # æª¢å®š\n",
    "        break_flag = 1 if abs(s_n_t) > c_alpha else 0\n",
    "        break_flags.append(break_flag)\n",
    "    \n",
    "    # å»ºç«‹çµæœ Series\n",
    "    index = log_prices.index[n_start + 1:]\n",
    "    cusum_stat = pd.Series(cusum_stats, index=index)\n",
    "    break_flag = pd.Series(break_flags, index=index)\n",
    "    \n",
    "    return cusum_stat, break_flag\n",
    "\n",
    "# =====================================================\n",
    "# 4ï¸âƒ£ SADF / GSADF Testsï¼ˆæ³¡æ²«æª¢å®šï¼‰\n",
    "# =====================================================\n",
    "\n",
    "def chow_df_test(log_prices, tau_star):\n",
    "    \"\"\"\n",
    "    Chow-type Dickeyâ€“Fuller Test\n",
    "    \n",
    "    æ¸¬è©¦å¾éš¨æ©Ÿæ¼«æ­¥ â†’ çˆ†ç™¼éç¨‹çš„è½‰æŠ˜é»\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    log_prices : pd.Series\n",
    "        Log åƒ¹æ ¼åºåˆ—\n",
    "    tau_star : int\n",
    "        å‡è¨­çš„æ–·é»ä½ç½®\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_stat : float\n",
    "        Dickey-Fuller çµ±è¨ˆé‡\n",
    "    p_value : float\n",
    "        p å€¼\n",
    "    \"\"\"\n",
    "    y = log_prices.values\n",
    "    n = len(y)\n",
    "    \n",
    "    if tau_star >= n - 1 or tau_star < 1:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # å»ºç«‹è™›æ“¬è®Šæ•¸ D_t[tau*]\n",
    "    D = np.zeros(n)\n",
    "    D[tau_star:] = 1\n",
    "    \n",
    "    # å»ºç«‹è¿´æ­¸è®Šæ•¸\n",
    "    delta_y = np.diff(y)\n",
    "    y_lag = y[:-1]\n",
    "    D_lag = D[:-1]\n",
    "    interaction = y_lag * D_lag\n",
    "    \n",
    "    # å»ºç«‹ DataFrame\n",
    "    df_reg = pd.DataFrame({\n",
    "        'delta_y': delta_y,\n",
    "        'y_lag': y_lag,\n",
    "        'interaction': interaction\n",
    "    }).dropna()\n",
    "    \n",
    "    if len(df_reg) < 10:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    try:\n",
    "        # OLS è¿´æ­¸\n",
    "        X = df_reg[['y_lag', 'interaction']].values\n",
    "        y_reg = df_reg['delta_y'].values\n",
    "        \n",
    "        model = OLS(y_reg, X).fit()\n",
    "        \n",
    "        # æª¢å®š interaction ä¿‚æ•¸ï¼ˆå°æ‡‰ Î´ï¼‰\n",
    "        coeff = model.params[1]  # interaction ä¿‚æ•¸\n",
    "        std_err = model.bse[1]\n",
    "        \n",
    "        if std_err > 0:\n",
    "            df_stat = coeff / std_err\n",
    "        else:\n",
    "            df_stat = 0\n",
    "        \n",
    "        # ç°¡åŒ– p å€¼è¨ˆç®—ï¼ˆä½¿ç”¨ t æª¢å®šï¼‰\n",
    "        p_value = 2 * (1 - stats.t.cdf(abs(df_stat), df=model.df_resid))\n",
    "        \n",
    "        return df_stat, p_value\n",
    "    \n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def sadf_test(log_prices, min_window=20, max_window=None):\n",
    "    \"\"\"\n",
    "    SADF (Supremum Augmented Dickeyâ€“Fuller) Test\n",
    "    \n",
    "    ç”¨ä¸æ–·æ“´å±•çš„è¦–çª—æ‰¾å‡ºæœ€å¤§å³å°¾ DF çµ±è¨ˆé‡\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    log_prices : pd.Series\n",
    "        Log åƒ¹æ ¼åºåˆ—\n",
    "    min_window : int\n",
    "        æœ€å°çª—å£å¤§å°\n",
    "    max_window : int, optional\n",
    "        æœ€å¤§çª—å£å¤§å°ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ len(log_prices)ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sadf_stat : float\n",
    "        SADF çµ±è¨ˆé‡\n",
    "    sadf_scores : pd.Series\n",
    "        æ¯å€‹æ™‚é–“é»çš„ SADF åˆ†æ•¸\n",
    "    \"\"\"\n",
    "    y = log_prices.values\n",
    "    n = len(y)\n",
    "    \n",
    "    if max_window is None:\n",
    "        max_window = n\n",
    "    \n",
    "    sadf_scores = []\n",
    "    \n",
    "    # å°æ¯å€‹çµæŸé» tï¼Œè¨ˆç®—æ“´å±•çª—å£çš„ SADF\n",
    "    for t in range(min_window, n):\n",
    "        # å¾ 1 åˆ° t-min_window æ“´å±•çª—å£\n",
    "        max_df = -np.inf\n",
    "        \n",
    "        for t0 in range(1, t - min_window + 1):\n",
    "            # æå–å­åºåˆ—\n",
    "            y_sub = y[t0:t+1]\n",
    "            \n",
    "            if len(y_sub) < min_window:\n",
    "                continue\n",
    "            \n",
    "            # è¨ˆç®— ADF çµ±è¨ˆé‡ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰\n",
    "            try:\n",
    "                delta_y = np.diff(y_sub)\n",
    "                y_lag = y_sub[:-1]\n",
    "                \n",
    "                if len(delta_y) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # ç°¡åŒ– ADFï¼šæª¢å®š y_lag ä¿‚æ•¸\n",
    "                X = y_lag.reshape(-1, 1)\n",
    "                model = OLS(delta_y, X).fit()\n",
    "                \n",
    "                coeff = model.params[0]\n",
    "                std_err = model.bse[0] if model.bse[0] > 0 else 1.0\n",
    "                \n",
    "                df_stat = coeff / std_err\n",
    "                max_df = max(max_df, df_stat)\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        sadf_scores.append(max_df if max_df > -np.inf else 0)\n",
    "    \n",
    "    # SADF çµ±è¨ˆé‡ = æ‰€æœ‰æ™‚é–“é»çš„æœ€å¤§å€¼\n",
    "    sadf_stat = max(sadf_scores) if sadf_scores else 0\n",
    "    \n",
    "    # å»ºç«‹çµæœ Series\n",
    "    index = log_prices.index[min_window:]\n",
    "    sadf_scores_series = pd.Series(sadf_scores, index=index)\n",
    "    \n",
    "    return sadf_stat, sadf_scores_series\n",
    "\n",
    "def gsadf_test(log_prices, min_window=20, max_window=None):\n",
    "    \"\"\"\n",
    "    GSADF (Generalized SADF) Test\n",
    "    \n",
    "    å…è¨±æ»¾å‹•èˆ‡æ“´å±•è¦–çª—åŒæ™‚ç§»å‹•ï¼Œå¯åµæ¸¬å¤šé‡æ³¡æ²«\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    log_prices : pd.Series\n",
    "        Log åƒ¹æ ¼åºåˆ—\n",
    "    min_window : int\n",
    "        æœ€å°çª—å£å¤§å°\n",
    "    max_window : int, optional\n",
    "        æœ€å¤§çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    gsadf_stat : float\n",
    "        GSADF çµ±è¨ˆé‡\n",
    "    gsadf_scores : pd.Series\n",
    "        æ¯å€‹æ™‚é–“é»çš„ GSADF åˆ†æ•¸\n",
    "    \"\"\"\n",
    "    y = log_prices.values\n",
    "    n = len(y)\n",
    "    \n",
    "    if max_window is None:\n",
    "        max_window = n\n",
    "    \n",
    "    gsadf_scores = []\n",
    "    \n",
    "    # å°æ¯å€‹çµæŸé» t\n",
    "    for t in range(min_window, n):\n",
    "        max_df = -np.inf\n",
    "        \n",
    "        # å…è¨±æ»¾å‹•çª—å£ï¼šr0 å’Œ r1 éƒ½å¯ä»¥ç§»å‹•\n",
    "        for r0 in range(0, t - min_window + 1):\n",
    "            for r1 in range(r0 + min_window, t + 1):\n",
    "                y_sub = y[r0:r1+1]\n",
    "                \n",
    "                if len(y_sub) < min_window:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    delta_y = np.diff(y_sub)\n",
    "                    y_lag = y_sub[:-1]\n",
    "                    \n",
    "                    if len(delta_y) < 5:\n",
    "                        continue\n",
    "                    \n",
    "                    X = y_lag.reshape(-1, 1)\n",
    "                    model = OLS(delta_y, X).fit()\n",
    "                    \n",
    "                    coeff = model.params[0]\n",
    "                    std_err = model.bse[0] if model.bse[0] > 0 else 1.0\n",
    "                    \n",
    "                    df_stat = coeff / std_err\n",
    "                    max_df = max(max_df, df_stat)\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        gsadf_scores.append(max_df if max_df > -np.inf else 0)\n",
    "    \n",
    "    gsadf_stat = max(gsadf_scores) if gsadf_scores else 0\n",
    "    \n",
    "    index = log_prices.index[min_window:]\n",
    "    gsadf_scores_series = pd.Series(gsadf_scores, index=index)\n",
    "    \n",
    "    return gsadf_stat, gsadf_scores_series\n",
    "\n",
    "# =====================================================\n",
    "# 5ï¸âƒ£ Structural Break Regime Label\n",
    "# =====================================================\n",
    "\n",
    "def structural_break_regime_label(df, methods=['cusum', 'csw', 'sadf']):\n",
    "    \"\"\"\n",
    "    çµåˆå¤šç¨®æª¢å®šçµæœï¼Œç”¢ç”Ÿçµæ§‹è®Šå‹• regime æ¨™ç±¤\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        éœ€åŒ…å«åƒ¹æ ¼è³‡æ–™å’Œå„ç¨®æª¢å®šçµæœ\n",
    "    methods : list\n",
    "        ä½¿ç”¨çš„æª¢å®šæ–¹æ³•\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    regime : pd.Series\n",
    "        Regime æ¨™ç±¤ï¼ˆ0=ç©©å®š, 1=çµæ§‹è®Šå‹•ï¼‰\n",
    "    \"\"\"\n",
    "    regime = pd.Series(0, index=df.index)\n",
    "    \n",
    "    # çµåˆå„ç¨®æª¢å®šçµæœ\n",
    "    if 'cusum' in methods and 'CUSUM_Break' in df.columns:\n",
    "        regime = regime | df['CUSUM_Break']\n",
    "    \n",
    "    if 'csw' in methods and 'CSW_Break' in df.columns:\n",
    "        regime = regime | df['CSW_Break']\n",
    "    \n",
    "    if 'sadf' in methods and 'SADF_Break' in df.columns:\n",
    "        regime = regime | df['SADF_Break']\n",
    "    \n",
    "    return regime\n",
    "\n",
    "# =====================================================\n",
    "# ä¸»ç¨‹å¼ï¼šè¼‰å…¥è³‡æ–™ä¸¦è¨ˆç®—æ‰€æœ‰æª¢å®š\n",
    "# =====================================================\n",
    "\n",
    "file_path = r'C:\\Users\\a124a\\OneDrive\\æ¡Œé¢\\ç­–ç•¥é–‹ç™¼\\Xgboost_MlFinlab\\minute_prices\\NAS100.csv'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š çµæ§‹æ€§è®Šå‹•æª¢æ¸¬\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "print(f\"\\nè¼‰å…¥è³‡æ–™: {file_path}\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# è½‰æ›æ™‚é–“æ ¼å¼\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "else:\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# é¸æ“‡åƒ¹æ ¼æ¬„ä½\n",
    "if 'BidClose' in df.columns:\n",
    "    df['Close'] = df['BidClose']\n",
    "elif 'Close' not in df.columns:\n",
    "    raise ValueError(\"æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½\")\n",
    "\n",
    "print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ\")\n",
    "print(f\"   è³‡æ–™ç­†æ•¸: {len(df):,}\")\n",
    "print(f\"   è³‡æ–™æœŸé–“: {df.index[0]} è‡³ {df.index[-1]}\")\n",
    "\n",
    "# è¨ˆç®— log åƒ¹æ ¼å’Œå ±é…¬\n",
    "df['Log_Price'] = np.log(df['Close'])\n",
    "df['Returns'] = df['Close'].pct_change()\n",
    "\n",
    "# =====================================================\n",
    "# 1. CUSUM Statistic\n",
    "# =====================================================\n",
    "print(f\"\\n1ï¸âƒ£ è¨ˆç®— CUSUM Statistic...\")\n",
    "cusum_stat, cusum_change = cusum_statistic(df['Returns'].dropna(), window=100)\n",
    "df['CUSUM_Stat'] = cusum_stat\n",
    "df['CUSUM_Change'] = cusum_change\n",
    "\n",
    "# CUSUM äº‹ä»¶\n",
    "cusum_threshold = df['Returns'].std() * 2\n",
    "cusum_events = cusum_filter_events(df['Returns'].dropna(), cusum_threshold, window=100)\n",
    "df['CUSUM_Event'] = 0\n",
    "df.loc[cusum_events, 'CUSUM_Event'] = 1\n",
    "df['CUSUM_Break'] = (df['CUSUM_Change'] > cusum_threshold).astype(int)\n",
    "\n",
    "print(f\"   CUSUM äº‹ä»¶æ•¸: {df['CUSUM_Event'].sum():,}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. CSW CUSUM Test\n",
    "# =====================================================\n",
    "# print(f\"\\n2ï¸âƒ£ è¨ˆç®— CSW CUSUM Test...\")\n",
    "# csw_stat, csw_break = csw_cusum_test(df['Log_Price'].dropna(), n_start=100, alpha=0.05)\n",
    "# df['CSW_Stat'] = csw_stat\n",
    "# df['CSW_Break'] = csw_break\n",
    "# print(f\"   CSW çµæ§‹è®Šå‹•æ•¸: {df['CSW_Break'].sum():,}\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. SADF Test\n",
    "# =====================================================\n",
    "# print(f\"\\n3ï¸âƒ£ è¨ˆç®— SADF Test...\")\n",
    "# sadf_stat, sadf_scores = sadf_test(df['Log_Price'].dropna(), min_window=60, max_window=1440)\n",
    "# df['SADF_Score'] = sadf_scores\n",
    "# # ä½¿ç”¨è‡¨ç•Œå€¼åˆ¤æ–·ï¼ˆç°¡åŒ–ï¼šä½¿ç”¨åˆ†ä½æ•¸ï¼‰\n",
    "# sadf_critical = sadf_scores.quantile(0.95)\n",
    "# df['SADF_Break'] = (df['SADF_Score'] > sadf_critical).astype(int)\n",
    "# print(f\"   SADF çµ±è¨ˆé‡: {sadf_stat:.4f}\")\n",
    "# print(f\"   SADF çµæ§‹è®Šå‹•æ•¸: {df['SADF_Break'].sum():,}\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. GSADF Testï¼ˆå¯é¸ï¼Œè¨ˆç®—è¼ƒæ…¢ï¼‰\n",
    "# =====================================================\n",
    "# print(f\"\\n4ï¸âƒ£ è¨ˆç®— GSADF Test...\")\n",
    "# print(\"   âš ï¸ GSADF è¨ˆç®—è¼ƒæ…¢ï¼Œä½¿ç”¨è¼ƒå°çª—å£...\")\n",
    "# gsadf_stat, gsadf_scores = gsadf_test(df['Log_Price'].dropna(), min_window=60, max_window=240)\n",
    "# df['GSADF_Score'] = gsadf_scores\n",
    "# gsadf_critical = gsadf_scores.quantile(0.95)\n",
    "# df['GSADF_Break'] = (df['GSADF_Score'] > gsadf_critical).astype(int)\n",
    "# print(f\"   GSADF çµ±è¨ˆé‡: {gsadf_stat:.4f}\")\n",
    "# print(f\"   GSADF çµæ§‹è®Šå‹•æ•¸: {df['GSADF_Break'].sum():,}\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. Structural Break Regime Label\n",
    "# =====================================================\n",
    "print(f\"\\n5ï¸âƒ£ ç”¢ç”Ÿ Structural Break Regime Label...\")\n",
    "df['Regime'] = structural_break_regime_label(df, methods=['cusum', 'csw', 'sadf'])\n",
    "print(f\"   çµæ§‹è®Šå‹•æœŸé–“æ•¸: {df['Regime'].sum():,}\")\n",
    "print(f\"   ç©©å®šæœŸé–“æ•¸: {(df['Regime'] == 0).sum():,}\")\n",
    "\n",
    "# =====================================================\n",
    "# çµæœçµ±è¨ˆ\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… è¨ˆç®—å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\næ–°å¢æ¬„ä½:\")\n",
    "print(f\"  - CUSUM_Stat: CUSUM çµ±è¨ˆé‡\")\n",
    "print(f\"  - CUSUM_Event: CUSUM äº‹ä»¶æ¨™è¨˜\")\n",
    "print(f\"  - CUSUM_Break: CUSUM çµæ§‹è®Šå‹•æ¨™è¨˜\")\n",
    "print(f\"  - CSW_Stat: CSW CUSUM çµ±è¨ˆé‡\")\n",
    "print(f\"  - CSW_Break: CSW çµæ§‹è®Šå‹•æ¨™è¨˜\")\n",
    "print(f\"  - SADF_Score: SADF åˆ†æ•¸\")\n",
    "print(f\"  - SADF_Break: SADF çµæ§‹è®Šå‹•æ¨™è¨˜\")\n",
    "print(f\"  - GSADF_Score: GSADF åˆ†æ•¸\")\n",
    "print(f\"  - GSADF_Break: GSADF çµæ§‹è®Šå‹•æ¨™è¨˜\")\n",
    "print(f\"  - Regime: ç¶œåˆçµæ§‹è®Šå‹•æ¨™ç±¤ (0=ç©©å®š, 1=è®Šå‹•)\")\n",
    "\n",
    "# é¡¯ç¤ºæœ€å¾Œå¹¾ç­†\n",
    "print(f\"\\næœ€å¾Œ 5 ç­†è³‡æ–™:\")\n",
    "display_cols = ['Close', 'CUSUM_Break', 'CSW_Break', 'SADF_Break', 'Regime']\n",
    "available_cols = [col for col in display_cols if col in df.columns]\n",
    "print(df[available_cols].tail())\n",
    "\n",
    "# # å„²å­˜çµæœ\n",
    "# output_file = 'NAS100_Structural_Breaks.csv'\n",
    "# df.to_csv(output_file)\n",
    "# print(f\"\\nâœ… å·²ä¿å­˜åˆ°: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47144768",
   "metadata": {},
   "source": [
    "è³‡è¨Šç†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5987d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# 1ï¸âƒ£ Shannon Entropy of Return Signs\n",
    "# =====================================================\n",
    "\n",
    "def shannon_entropy_return_signs(returns, window=30):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—å ±é…¬ç¬¦è™Ÿçš„é¦™è¾²ç†µ\n",
    "    \n",
    "    å…¬å¼: H = -Î£ p_i * log2(p_i)\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°ï¼ˆåˆ†é˜æ•¸ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    entropy : pd.Series\n",
    "        é¦™è¾²ç†µåºåˆ—\n",
    "    \"\"\"\n",
    "    # å°‡å ±é…¬è½‰ç‚ºç¬¦è™Ÿï¼ˆ+1, -1, 0ï¼‰\n",
    "    signs = returns.copy()\n",
    "    signs[signs > 0] = 1\n",
    "    signs[signs < 0] = -1\n",
    "    signs[signs == 0] = 0\n",
    "    \n",
    "    entropy_series = []\n",
    "    \n",
    "    for i in range(len(returns)):\n",
    "        if i < window:\n",
    "            entropy_series.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        # æå–çª—å£å…§çš„ç¬¦è™Ÿ\n",
    "        window_signs = signs.iloc[i-window:i]\n",
    "        \n",
    "        # è¨ˆç®—å„ç¬¦è™Ÿå‡ºç¾æ©Ÿç‡\n",
    "        p_up = (window_signs == 1).sum() / len(window_signs)\n",
    "        p_down = (window_signs == -1).sum() / len(window_signs)\n",
    "        p_zero = (window_signs == 0).sum() / len(window_signs)\n",
    "        \n",
    "        # è¨ˆç®—ç†µï¼ˆé¿å… log(0)ï¼‰\n",
    "        entropy = 0\n",
    "        if p_up > 0:\n",
    "            entropy -= p_up * np.log2(p_up)\n",
    "        if p_down > 0:\n",
    "            entropy -= p_down * np.log2(p_down)\n",
    "        if p_zero > 0:\n",
    "            entropy -= p_zero * np.log2(p_zero)\n",
    "        \n",
    "        entropy_series.append(entropy)\n",
    "    \n",
    "    return pd.Series(entropy_series, index=returns.index)\n",
    "\n",
    "def shannon_entropy_rolling(returns, window=30):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ»¾å‹•çª—å£è¨ˆç®—é¦™è¾²ç†µï¼ˆæ›´é«˜æ•ˆç‰ˆæœ¬ï¼‰\n",
    "    \"\"\"\n",
    "    # è½‰ç‚ºç¬¦è™Ÿ\n",
    "    signs = np.sign(returns)\n",
    "    \n",
    "    # ä½¿ç”¨æ»¾å‹•çª—å£è¨ˆç®—æ©Ÿç‡\n",
    "    def calc_entropy(x):\n",
    "        p_up = (x == 1).sum() / len(x)\n",
    "        p_down = (x == -1).sum() / len(x)\n",
    "        p_zero = (x == 0).sum() / len(x)\n",
    "        \n",
    "        entropy = 0\n",
    "        if p_up > 0:\n",
    "            entropy -= p_up * np.log2(p_up)\n",
    "        if p_down > 0:\n",
    "            entropy -= p_down * np.log2(p_down)\n",
    "        if p_zero > 0:\n",
    "            entropy -= p_zero * np.log2(p_zero)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    entropy = signs.rolling(window=window, min_periods=window).apply(calc_entropy, raw=True)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# =====================================================\n",
    "# 2ï¸âƒ£ Lempelâ€“Ziv Entropy\n",
    "# =====================================================\n",
    "\n",
    "def lempel_ziv_entropy(sequence):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Lempelâ€“Ziv ç†µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    sequence : array-like\n",
    "        äºŒé€²ä½åºåˆ—ï¼ˆ0/1 æˆ– -1/1ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    h_lz : float\n",
    "        Lempelâ€“Ziv ç†µ\n",
    "    \"\"\"\n",
    "    # è½‰ç‚ºå­—ä¸²\n",
    "    if isinstance(sequence, pd.Series):\n",
    "        seq = sequence.values\n",
    "    else:\n",
    "        seq = np.array(sequence)\n",
    "    \n",
    "    # è½‰ç‚º 0/1 äºŒé€²ä½\n",
    "    binary_seq = (seq > 0).astype(int)\n",
    "    n = len(binary_seq)\n",
    "    \n",
    "    if n == 0:\n",
    "        return 0\n",
    "    \n",
    "    # è¨ˆç®—æœ€çŸ­ä¸é‡è¤‡ç‰‡æ®µæ•¸ c(n)\n",
    "    i = 0\n",
    "    c = 0\n",
    "    substrings = set()\n",
    "    \n",
    "    while i < n:\n",
    "        j = i + 1\n",
    "        found = False\n",
    "        \n",
    "        while j <= n:\n",
    "            substring = tuple(binary_seq[i:j])\n",
    "            \n",
    "            if substring not in substrings:\n",
    "                substrings.add(substring)\n",
    "                c += 1\n",
    "                i = j\n",
    "                found = True\n",
    "                break\n",
    "            j += 1\n",
    "        \n",
    "        if not found:\n",
    "            i += 1\n",
    "    \n",
    "    # è¨ˆç®— LZ ç†µ\n",
    "    if n > 0:\n",
    "        h_lz = (c * np.log2(n)) / n\n",
    "    else:\n",
    "        h_lz = 0\n",
    "    \n",
    "    return h_lz\n",
    "\n",
    "def lempel_ziv_entropy_rolling(returns, window=60):\n",
    "    \"\"\"\n",
    "    æ»¾å‹•çª—å£è¨ˆç®— Lempelâ€“Ziv ç†µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lz_entropy : pd.Series\n",
    "        LZ ç†µåºåˆ—\n",
    "    \"\"\"\n",
    "    lz_series = []\n",
    "    \n",
    "    for i in range(len(returns)):\n",
    "        if i < window:\n",
    "            lz_series.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        window_returns = returns.iloc[i-window:i]\n",
    "        lz_val = lempel_ziv_entropy(window_returns)\n",
    "        lz_series.append(lz_val)\n",
    "    \n",
    "    return pd.Series(lz_series, index=returns.index)\n",
    "\n",
    "# =====================================================\n",
    "# 3ï¸âƒ£ Hurst Exponent\n",
    "# =====================================================\n",
    "\n",
    "def hurst_exponent_rs(series, max_lag=None):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ R/S (Rescaled Range) åˆ†æè¨ˆç®— Hurst æŒ‡æ•¸\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        æ™‚é–“åºåˆ—ï¼ˆlog price æˆ– returnï¼‰\n",
    "    max_lag : int, optional\n",
    "        æœ€å¤§æ»¯å¾ŒæœŸæ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ len(series) // 4ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    H : float\n",
    "        Hurst æŒ‡æ•¸\n",
    "    \"\"\"\n",
    "    n = len(series)\n",
    "    if max_lag is None:\n",
    "        max_lag = n // 4\n",
    "    \n",
    "    if max_lag < 2:\n",
    "        return 0.5\n",
    "    \n",
    "    # è¨ˆç®—ä¸åŒ lag çš„ R/S\n",
    "    lags = []\n",
    "    rs_values = []\n",
    "    \n",
    "    for lag in range(2, min(max_lag, n // 2)):\n",
    "        # å°‡åºåˆ—åˆ†æˆå¤šå€‹å­å€é–“\n",
    "        n_segments = n // lag\n",
    "        if n_segments < 2:\n",
    "            continue\n",
    "        \n",
    "        rs_list = []\n",
    "        \n",
    "        for i in range(n_segments):\n",
    "            segment = series.iloc[i*lag:(i+1)*lag]\n",
    "            \n",
    "            if len(segment) < 2:\n",
    "                continue\n",
    "            \n",
    "            # è¨ˆç®—å‡å€¼\n",
    "            mean_seg = segment.mean()\n",
    "            \n",
    "            # è¨ˆç®—ç´¯ç©åå·®\n",
    "            cumdev = (segment - mean_seg).cumsum()\n",
    "            \n",
    "            # Range\n",
    "            R = cumdev.max() - cumdev.min()\n",
    "            \n",
    "            # Standard deviation\n",
    "            S = segment.std()\n",
    "            \n",
    "            if S > 0:\n",
    "                rs_list.append(R / S)\n",
    "        \n",
    "        if rs_list:\n",
    "            lags.append(lag)\n",
    "            rs_values.append(np.mean(rs_list))\n",
    "    \n",
    "    if len(lags) < 2:\n",
    "        return 0.5\n",
    "    \n",
    "    # ç·šæ€§è¿´æ­¸ï¼šlog(R/S) vs log(lag)\n",
    "    log_lags = np.log(lags)\n",
    "    log_rs = np.log(rs_values)\n",
    "    \n",
    "    # ç§»é™¤ç„¡æ•ˆå€¼\n",
    "    valid = np.isfinite(log_lags) & np.isfinite(log_rs)\n",
    "    if valid.sum() < 2:\n",
    "        return 0.5\n",
    "    \n",
    "    log_lags_clean = log_lags[valid]\n",
    "    log_rs_clean = log_rs[valid]\n",
    "    \n",
    "    # ç·šæ€§è¿´æ­¸\n",
    "    slope, intercept = np.polyfit(log_lags_clean, log_rs_clean, 1)\n",
    "    H = slope\n",
    "    \n",
    "    return H\n",
    "\n",
    "def hurst_exponent_rolling(series, window=240, max_lag=None):\n",
    "    \"\"\"\n",
    "    æ»¾å‹•çª—å£è¨ˆç®— Hurst æŒ‡æ•¸\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        æ™‚é–“åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "    max_lag : int, optional\n",
    "        æœ€å¤§æ»¯å¾ŒæœŸæ•¸\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    hurst : pd.Series\n",
    "        Hurst æŒ‡æ•¸åºåˆ—\n",
    "    \"\"\"\n",
    "    hurst_series = []\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        if i < window:\n",
    "            hurst_series.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        window_series = series.iloc[i-window:i]\n",
    "        \n",
    "        if len(window_series) < 10:\n",
    "            hurst_series.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            H = hurst_exponent_rs(window_series, max_lag=max_lag)\n",
    "            hurst_series.append(H)\n",
    "        except:\n",
    "            hurst_series.append(np.nan)\n",
    "    \n",
    "    return pd.Series(hurst_series, index=series.index)\n",
    "\n",
    "def hurst_exponent_dfa(series, window=240):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ DFA (Detrended Fluctuation Analysis) è¨ˆç®— Hurst æŒ‡æ•¸\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        æ™‚é–“åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    hurst : pd.Series\n",
    "        Hurst æŒ‡æ•¸åºåˆ—\n",
    "    \"\"\"\n",
    "    def dfa_analysis(x):\n",
    "        n = len(x)\n",
    "        if n < 10:\n",
    "            return 0.5\n",
    "        \n",
    "        # ç´¯ç©å’Œ\n",
    "        y = np.cumsum(x - x.mean())\n",
    "        \n",
    "        # è¨ˆç®—ä¸åŒå°ºåº¦ä¸‹çš„æ³¢å‹•\n",
    "        scales = np.logspace(1, np.log10(n//4), 10).astype(int)\n",
    "        scales = scales[scales < n//4]\n",
    "        scales = scales[scales > 1]\n",
    "        \n",
    "        if len(scales) < 2:\n",
    "            return 0.5\n",
    "        \n",
    "        fluctuations = []\n",
    "        \n",
    "        for scale in scales:\n",
    "            n_segments = n // scale\n",
    "            if n_segments < 2:\n",
    "                continue\n",
    "            \n",
    "            f_list = []\n",
    "            \n",
    "            for i in range(n_segments):\n",
    "                segment = y[i*scale:(i+1)*scale]\n",
    "                \n",
    "                # å»è¶¨å‹¢ï¼ˆç·šæ€§ï¼‰\n",
    "                x_seg = np.arange(len(segment))\n",
    "                coeffs = np.polyfit(x_seg, segment, 1)\n",
    "                trend = np.polyval(coeffs, x_seg)\n",
    "                detrended = segment - trend\n",
    "                \n",
    "                f_list.append(np.sqrt(np.mean(detrended**2)))\n",
    "            \n",
    "            if f_list:\n",
    "                fluctuations.append(np.mean(f_list))\n",
    "        \n",
    "        if len(fluctuations) < 2:\n",
    "            return 0.5\n",
    "        \n",
    "        # ç·šæ€§è¿´æ­¸ï¼šlog(F) vs log(scale)\n",
    "        log_scales = np.log(scales[:len(fluctuations)])\n",
    "        log_fluct = np.log(fluctuations)\n",
    "        \n",
    "        valid = np.isfinite(log_scales) & np.isfinite(log_fluct)\n",
    "        if valid.sum() < 2:\n",
    "            return 0.5\n",
    "        \n",
    "        slope, _ = np.polyfit(log_scales[valid], log_fluct[valid], 1)\n",
    "        H = slope\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    hurst = series.rolling(window=window, min_periods=window).apply(dfa_analysis, raw=True)\n",
    "    \n",
    "    return hurst\n",
    "\n",
    "# =====================================================\n",
    "# 4ï¸âƒ£ Market Efficiency Index (MEI)\n",
    "# =====================================================\n",
    "\n",
    "def market_efficiency_index(returns, window=30, entropy_window=30):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—å¸‚å ´æ•ˆç‡æŒ‡æ•¸\n",
    "    \n",
    "    MEI = (H_Shannon / H_max) * Ïƒ_returns\n",
    "    \n",
    "    æˆ–ç°¡åŒ–ç‰ˆæœ¬ï¼š\n",
    "    MEI = entropy * returns.std()\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int\n",
    "        è¨ˆç®—æ³¢å‹•ç‡çš„çª—å£å¤§å°\n",
    "    entropy_window : int\n",
    "        è¨ˆç®—ç†µçš„çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    mei : pd.Series\n",
    "        å¸‚å ´æ•ˆç‡æŒ‡æ•¸\n",
    "    \"\"\"\n",
    "    # è¨ˆç®—é¦™è¾²ç†µ\n",
    "    entropy = shannon_entropy_rolling(returns, window=entropy_window)\n",
    "    \n",
    "    # è¨ˆç®—æ³¢å‹•ç‡\n",
    "    volatility = returns.rolling(window=window).std()\n",
    "    \n",
    "    # è¨ˆç®— MEI\n",
    "    # æ–¹æ³•1ï¼šç°¡åŒ–ç‰ˆæœ¬\n",
    "    mei = entropy * volatility\n",
    "    \n",
    "    # æ–¹æ³•2ï¼šæ¨™æº–åŒ–ç‰ˆæœ¬ï¼ˆå¯é¸ï¼‰\n",
    "    # H_max = np.log2(3)  # ä¸‰ç¨®ç¬¦è™Ÿï¼ˆ+1, -1, 0ï¼‰çš„æœ€å¤§ç†µ\n",
    "    # mei_normalized = (entropy / H_max) * volatility\n",
    "    \n",
    "    return mei\n",
    "\n",
    "# =====================================================\n",
    "# ä¸»ç¨‹å¼ï¼šè¼‰å…¥è³‡æ–™ä¸¦è¨ˆç®—æ‰€æœ‰ç†µç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "file_path = r'C:\\Users\\a124a\\OneDrive\\æ¡Œé¢\\ç­–ç•¥é–‹ç™¼\\Xgboost_MlFinlab\\minute_prices\\NAS100.csv'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š Entropy Features è¨ˆç®—\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "print(f\"\\nè¼‰å…¥è³‡æ–™: {file_path}\")\n",
    "df = pd.read_csv(file_path, nrows=100000)  # å…ˆè®€å–å‰10è¬ç­†æ¸¬è©¦\n",
    "\n",
    "# è½‰æ›æ™‚é–“æ ¼å¼\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "else:\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# é¸æ“‡åƒ¹æ ¼æ¬„ä½\n",
    "if 'BidClose' in df.columns:\n",
    "    df['Close'] = df['BidClose']\n",
    "elif 'Close' not in df.columns:\n",
    "    raise ValueError(\"æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½\")\n",
    "\n",
    "print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ\")\n",
    "print(f\"   è³‡æ–™ç­†æ•¸: {len(df):,}\")\n",
    "print(f\"   è³‡æ–™æœŸé–“: {df.index[0]} è‡³ {df.index[-1]}\")\n",
    "\n",
    "# è¨ˆç®— log åƒ¹æ ¼å’Œå ±é…¬\n",
    "df['Log_Price'] = np.log(df['Close'])\n",
    "df['Returns'] = df['Close'].pct_change()\n",
    "\n",
    "# =====================================================\n",
    "# 1. Shannon Entropy of Return Signs\n",
    "# =====================================================\n",
    "print(f\"\\n1ï¸âƒ£ è¨ˆç®— Shannon Entropy of Return Signs...\")\n",
    "df['Shannon_Entropy_30'] = shannon_entropy_rolling(df['Returns'], window=30)\n",
    "df['Shannon_Entropy_60'] = shannon_entropy_rolling(df['Returns'], window=60)\n",
    "df['Shannon_Entropy_240'] = shannon_entropy_rolling(df['Returns'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Lempelâ€“Ziv Entropy\n",
    "# =====================================================\n",
    "print(f\"\\n2ï¸âƒ£ è¨ˆç®— Lempelâ€“Ziv Entropy...\")\n",
    "print(\"   âš ï¸ LZ ç†µè¨ˆç®—è¼ƒæ…¢ï¼Œä½¿ç”¨è¼ƒå°çª—å£...\")\n",
    "df['LZ_Entropy_60'] = lempel_ziv_entropy_rolling(df['Returns'], window=60)\n",
    "df['LZ_Entropy_240'] = lempel_ziv_entropy_rolling(df['Returns'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. Hurst Exponent\n",
    "# =====================================================\n",
    "# print(f\"\\n3ï¸âƒ£ è¨ˆç®— Hurst Exponent (R/S æ–¹æ³•)...\")\n",
    "# print(\"   âš ï¸ Hurst è¨ˆç®—è¼ƒæ…¢ï¼Œä½¿ç”¨è¼ƒå°çª—å£...\")\n",
    "# df['Hurst_RS_240'] = hurst_exponent_rolling(df['Log_Price'], window=240, max_lag=60)\n",
    "# df['Hurst_RS_Returns_240'] = hurst_exponent_rolling(df['Returns'], window=240, max_lag=60)\n",
    "# print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# print(f\"\\n3ï¸âƒ£ è¨ˆç®— Hurst Exponent (DFA æ–¹æ³•)...\")\n",
    "# df['Hurst_DFA_240'] = hurst_exponent_dfa(df['Returns'], window=240)\n",
    "# print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Market Efficiency Index\n",
    "# =====================================================\n",
    "print(f\"\\n4ï¸âƒ£ è¨ˆç®— Market Efficiency Index...\")\n",
    "df['MEI_30'] = market_efficiency_index(df['Returns'], window=30, entropy_window=30)\n",
    "df['MEI_60'] = market_efficiency_index(df['Returns'], window=60, entropy_window=60)\n",
    "df['MEI_240'] = market_efficiency_index(df['Returns'], window=240, entropy_window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# çµæœçµ±è¨ˆ\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… è¨ˆç®—å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\næ–°å¢æ¬„ä½:\")\n",
    "print(f\"  - Shannon_Entropy_30/60/240: é¦™è¾²ç†µï¼ˆä¸åŒçª—å£ï¼‰\")\n",
    "print(f\"  - LZ_Entropy_60/240: Lempelâ€“Ziv ç†µ\")\n",
    "print(f\"  - Hurst_RS_240: Hurst æŒ‡æ•¸ï¼ˆR/S æ–¹æ³•ï¼Œlog priceï¼‰\")\n",
    "print(f\"  - Hurst_RS_Returns_240: Hurst æŒ‡æ•¸ï¼ˆR/S æ–¹æ³•ï¼Œreturnsï¼‰\")\n",
    "print(f\"  - Hurst_DFA_240: Hurst æŒ‡æ•¸ï¼ˆDFA æ–¹æ³•ï¼‰\")\n",
    "print(f\"  - MEI_30/60/240: å¸‚å ´æ•ˆç‡æŒ‡æ•¸\")\n",
    "\n",
    "# é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "print(f\"\\nç‰¹å¾µçµ±è¨ˆ:\")\n",
    "entropy_cols = [col for col in df.columns if 'Entropy' in col or 'Hurst' in col or 'MEI' in col]\n",
    "for col in entropy_cols:\n",
    "    valid = df[col].notna().sum()\n",
    "    if valid > 0:\n",
    "        mean_val = df[col].mean()\n",
    "        std_val = df[col].std()\n",
    "        print(f\"  {col}: æœ‰æ•ˆå€¼ {valid:,}, å¹³å‡ {mean_val:.4f}, æ¨™æº–å·® {std_val:.4f}\")\n",
    "\n",
    "# é¡¯ç¤ºæœ€å¾Œå¹¾ç­†\n",
    "print(f\"\\næœ€å¾Œ 5 ç­†è³‡æ–™:\")\n",
    "display_cols = ['Close', 'Returns', 'Shannon_Entropy_30', 'LZ_Entropy_60', 'Hurst_RS_240', 'MEI_30']\n",
    "available_cols = [col for col in display_cols if col in df.columns]\n",
    "print(df[available_cols].tail())\n",
    "\n",
    "# # å„²å­˜çµæœ\n",
    "# output_file = 'NAS100_Entropy_Features.csv'\n",
    "# df.to_csv(output_file)\n",
    "# print(f\"\\nâœ… å·²ä¿å­˜åˆ°: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4593ae",
   "metadata": {},
   "source": [
    "å¸‚å ´å¾®çµæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# 1ï¸âƒ£ Roll Spread Estimate\n",
    "# =====================================================\n",
    "\n",
    "def roll_spread_estimate(returns, window=60):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Roll éš±å«è²·è³£åƒ¹å·®\n",
    "    \n",
    "    å…¬å¼: S_Roll = 2 * sqrt(-Cov(r_t, r_{t-1}))\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°ï¼ˆåˆ†é˜æ•¸ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    roll_spread : pd.Series\n",
    "        Roll åƒ¹å·®åºåˆ—\n",
    "    \"\"\"\n",
    "    def calc_roll_spread(x):\n",
    "        if len(x) < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        # è¨ˆç®—ä¸€éšè‡ªç›¸é—œ\n",
    "        autocorr = x.autocorr(lag=1)\n",
    "        \n",
    "        if pd.isna(autocorr) or autocorr >= 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Roll spread\n",
    "        roll_spread = 2 * np.sqrt(-autocorr)\n",
    "        return roll_spread\n",
    "    \n",
    "    roll_spread = returns.rolling(window=window, min_periods=window).apply(calc_roll_spread, raw=False)\n",
    "    \n",
    "    return roll_spread\n",
    "\n",
    "# =====================================================\n",
    "# 2ï¸âƒ£ Amihud Illiquidity\n",
    "# =====================================================\n",
    "\n",
    "def amihud_illiquidity(returns, volume, window=60):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Amihud æµå‹•æ€§ç¼ºä¹æŒ‡æ¨™\n",
    "    \n",
    "    å…¬å¼: I_t = |r_t| / V_t\n",
    "    å¹³å‡: Amihud = (1/N) * Î£ |r_t| / V_t\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    volume : pd.Series\n",
    "        æˆäº¤é‡åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    amihud : pd.Series\n",
    "        Amihud æŒ‡æ¨™åºåˆ—\n",
    "    \"\"\"\n",
    "    # é¿å…é™¤ä»¥é›¶\n",
    "    volume_safe = volume.replace(0, np.nan)\n",
    "    \n",
    "    # è¨ˆç®—æ¯æœŸçš„ Amihud\n",
    "    amihud_daily = np.abs(returns) / volume_safe\n",
    "    \n",
    "    # æ»¾å‹•å¹³å‡\n",
    "    amihud = amihud_daily.rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    return amihud\n",
    "\n",
    "# =====================================================\n",
    "# 3ï¸âƒ£ Kyle's Î» (Kyle Lambda)\n",
    "# =====================================================\n",
    "\n",
    "def kyle_lambda(returns, volume, window=60):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Kyle Lambdaï¼ˆåƒ¹æ ¼è¡æ“Šä¿‚æ•¸ï¼‰\n",
    "    \n",
    "    å…¬å¼: Î» = Cov(r_t, q_t) / Var(q_t)\n",
    "    å…¶ä¸­ q_t ç‚º signed volume\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    volume : pd.Series\n",
    "        æˆäº¤é‡åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    kyle_lambda : pd.Series\n",
    "        Kyle Lambda åºåˆ—\n",
    "    \"\"\"\n",
    "    # è¨ˆç®— signed volumeï¼ˆç”¨å ±é…¬ç¬¦è™Ÿæ¨æ–·æ–¹å‘ï¼‰\n",
    "    signed_volume = np.sign(returns) * volume\n",
    "    \n",
    "    def calc_kyle_lambda(window_data):\n",
    "        rets = window_data['returns'].values\n",
    "        signed_vols = window_data['signed_volume'].values\n",
    "        \n",
    "        # ç§»é™¤ NaN\n",
    "        valid = ~(np.isnan(rets) | np.isnan(signed_vols))\n",
    "        rets_clean = rets[valid]\n",
    "        signed_vols_clean = signed_vols[valid]\n",
    "        \n",
    "        if len(rets_clean) < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        # è¨ˆç®—å…±è®Šç•°æ•¸å’Œè®Šç•°æ•¸\n",
    "        cov = np.cov(rets_clean, signed_vols_clean)[0, 1]\n",
    "        var = np.var(signed_vols_clean)\n",
    "        \n",
    "        if var == 0 or np.isnan(cov) or np.isnan(var):\n",
    "            return np.nan\n",
    "        \n",
    "        kyle_lambda = cov / var\n",
    "        return kyle_lambda\n",
    "    \n",
    "    # å»ºç«‹è‡¨æ™‚ DataFrame\n",
    "    temp_df = pd.DataFrame({\n",
    "        'returns': returns,\n",
    "        'signed_volume': signed_volume\n",
    "    })\n",
    "    \n",
    "    kyle_lambda_series = []\n",
    "    \n",
    "    for i in range(len(returns)):\n",
    "        if i < window:\n",
    "            kyle_lambda_series.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        window_data = temp_df.iloc[i-window:i]\n",
    "        kyle_val = calc_kyle_lambda(window_data)\n",
    "        kyle_lambda_series.append(kyle_val)\n",
    "    \n",
    "    return pd.Series(kyle_lambda_series, index=returns.index)\n",
    "\n",
    "# =====================================================\n",
    "# 4ï¸âƒ£ Order Flow Imbalance (OFI)\n",
    "# =====================================================\n",
    "\n",
    "def order_flow_imbalance(returns, volume=None, window=30):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—è¨‚å–®æµä¸å‡è¡¡\n",
    "    \n",
    "    ç°¡åŒ–ç‰ˆæœ¬: OFI_t â‰ˆ sign(r_t)\n",
    "    å«æˆäº¤é‡ç‰ˆæœ¬: OFI_t = (volume * sign(returns)).rolling().mean()\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    volume : pd.Series, optional\n",
    "        æˆäº¤é‡åºåˆ—ï¼ˆå¦‚æœæä¾›ï¼Œæœƒä½¿ç”¨åŠ æ¬Šç‰ˆæœ¬ï¼‰\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ofi : pd.Series\n",
    "        Order Flow Imbalance åºåˆ—\n",
    "    \"\"\"\n",
    "    # åŸºæœ¬ç‰ˆæœ¬ï¼šä½¿ç”¨å ±é…¬ç¬¦è™Ÿ\n",
    "    ofi_basic = np.sign(returns)\n",
    "    \n",
    "    if volume is not None:\n",
    "        # åŠ æ¬Šç‰ˆæœ¬ï¼šä½¿ç”¨æˆäº¤é‡åŠ æ¬Š\n",
    "        signed_volume = np.sign(returns) * volume\n",
    "        ofi = signed_volume.rolling(window=window, min_periods=1).mean()\n",
    "    else:\n",
    "        # ç°¡å–®ç‰ˆæœ¬ï¼šæ»¾å‹•å¹³å‡ç¬¦è™Ÿ\n",
    "        ofi = ofi_basic.rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    return ofi\n",
    "\n",
    "# =====================================================\n",
    "# 5ï¸âƒ£ Volume Imbalance Ratio (VIR)\n",
    "# =====================================================\n",
    "\n",
    "def volume_imbalance_ratio(returns, volume, window=60):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æˆäº¤é‡ä¸å°ç¨±æ¯”\n",
    "    \n",
    "    å…¬å¼: VIR = (V_up - V_down) / (V_up + V_down)\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    volume : pd.Series\n",
    "        æˆäº¤é‡åºåˆ—\n",
    "    window : int\n",
    "        æ»¾å‹•çª—å£å¤§å°\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    vir : pd.Series\n",
    "        Volume Imbalance Ratio åºåˆ—\n",
    "    \"\"\"\n",
    "    def calc_vir(window_data):\n",
    "        rets = window_data['returns'].values\n",
    "        vols = window_data['volume'].values\n",
    "        \n",
    "        # è¨ˆç®—ä¸Šæ¼²å’Œä¸‹è·Œçš„æˆäº¤é‡\n",
    "        v_up = vols[rets > 0].sum()\n",
    "        v_down = vols[rets < 0].sum()\n",
    "        \n",
    "        total = v_up + v_down\n",
    "        \n",
    "        if total == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        vir = (v_up - v_down) / total\n",
    "        return vir\n",
    "    \n",
    "    # å»ºç«‹è‡¨æ™‚ DataFrame\n",
    "    temp_df = pd.DataFrame({\n",
    "        'returns': returns,\n",
    "        'volume': volume\n",
    "    })\n",
    "    \n",
    "    vir_series = []\n",
    "    \n",
    "    for i in range(len(returns)):\n",
    "        if i < window:\n",
    "            vir_series.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        window_data = temp_df.iloc[i-window:i]\n",
    "        vir_val = calc_vir(window_data)\n",
    "        vir_series.append(vir_val)\n",
    "    \n",
    "    return pd.Series(vir_series, index=returns.index)\n",
    "\n",
    "# =====================================================\n",
    "# 6ï¸âƒ£ Realized Spread / Price Impact\n",
    "# =====================================================\n",
    "\n",
    "def realized_spread_price_impact(prices, returns, delta_minutes=5):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Realized Spread å’Œ Price Impact\n",
    "    \n",
    "    Realized Spread: RS_t = 2 * (p_t - p_{t+Î”}) * d_t\n",
    "    Price Impact: PI_t = (p_{t+Î”} - p_t) * d_t\n",
    "    \n",
    "    å…¶ä¸­ d_t = sign(r_t) ç‚ºäº¤æ˜“æ–¹å‘\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    prices : pd.Series\n",
    "        åƒ¹æ ¼åºåˆ—\n",
    "    returns : pd.Series\n",
    "        å ±é…¬åºåˆ—\n",
    "    delta_minutes : int\n",
    "        å»¶å¾Œæ™‚é–“ï¼ˆåˆ†é˜æ•¸ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    realized_spread : pd.Series\n",
    "        Realized Spread åºåˆ—\n",
    "    price_impact : pd.Series\n",
    "        Price Impact åºåˆ—\n",
    "    \"\"\"\n",
    "    # äº¤æ˜“æ–¹å‘\n",
    "    direction = np.sign(returns)\n",
    "    \n",
    "    # å»¶å¾Œåƒ¹æ ¼\n",
    "    future_prices = prices.shift(-delta_minutes)\n",
    "    \n",
    "    # è¨ˆç®—åƒ¹æ ¼è®Šå‹•\n",
    "    price_change = future_prices - prices\n",
    "    \n",
    "    # Realized Spread\n",
    "    realized_spread = 2 * price_change * direction\n",
    "    \n",
    "    # Price Impact\n",
    "    price_impact = price_change * direction\n",
    "    \n",
    "    return realized_spread, price_impact\n",
    "\n",
    "# =====================================================\n",
    "# ä¸»ç¨‹å¼ï¼šè¼‰å…¥è³‡æ–™ä¸¦è¨ˆç®—æ‰€æœ‰å¾®çµæ§‹ç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "file_path = r'C:\\Users\\a124a\\OneDrive\\æ¡Œé¢\\ç­–ç•¥é–‹ç™¼\\Xgboost_MlFinlab\\minute_prices\\NAS100.csv'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š Microstructure Features è¨ˆç®—\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "print(f\"\\nè¼‰å…¥è³‡æ–™: {file_path}\")\n",
    "# å…ˆè®€å–å‰10è¬ç­†æ¸¬è©¦ï¼ˆå¯æ ¹æ“šéœ€è¦èª¿æ•´ï¼‰\n",
    "df = pd.read_csv(file_path, nrows=100000)\n",
    "\n",
    "# è½‰æ›æ™‚é–“æ ¼å¼\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "else:\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# é¸æ“‡åƒ¹æ ¼å’Œæˆäº¤é‡æ¬„ä½\n",
    "if 'BidClose' in df.columns:\n",
    "    df['Close'] = df['BidClose']\n",
    "elif 'Close' not in df.columns:\n",
    "    raise ValueError(\"æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½\")\n",
    "\n",
    "if 'Volume' not in df.columns:\n",
    "    if 'BidVolume' in df.columns:\n",
    "        df['Volume'] = df['BidVolume']\n",
    "    else:\n",
    "        raise ValueError(\"æ‰¾ä¸åˆ°æˆäº¤é‡æ¬„ä½\")\n",
    "\n",
    "print(f\"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ\")\n",
    "print(f\"   è³‡æ–™ç­†æ•¸: {len(df):,}\")\n",
    "print(f\"   è³‡æ–™æœŸé–“: {df.index[0]} è‡³ {df.index[-1]}\")\n",
    "\n",
    "# è¨ˆç®—å ±é…¬\n",
    "df['Returns'] = df['Close'].pct_change()\n",
    "\n",
    "# =====================================================\n",
    "# 1. Roll Spread Estimate\n",
    "# =====================================================\n",
    "print(f\"\\n1ï¸âƒ£ è¨ˆç®— Roll Spread Estimate...\")\n",
    "df['Roll_Spread_30'] = roll_spread_estimate(df['Returns'], window=30)\n",
    "df['Roll_Spread_60'] = roll_spread_estimate(df['Returns'], window=60)\n",
    "df['Roll_Spread_240'] = roll_spread_estimate(df['Returns'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Amihud Illiquidity\n",
    "# =====================================================\n",
    "print(f\"\\n2ï¸âƒ£ è¨ˆç®— Amihud Illiquidity...\")\n",
    "df['Amihud_30'] = amihud_illiquidity(df['Returns'], df['Volume'], window=30)\n",
    "df['Amihud_60'] = amihud_illiquidity(df['Returns'], df['Volume'], window=60)\n",
    "df['Amihud_240'] = amihud_illiquidity(df['Returns'], df['Volume'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. Kyle's Î»\n",
    "# =====================================================\n",
    "print(f\"\\n3ï¸âƒ£ è¨ˆç®— Kyle's Î»...\")\n",
    "print(\"   âš ï¸ Kyle Lambda è¨ˆç®—è¼ƒæ…¢...\")\n",
    "df['Kyle_Lambda_60'] = kyle_lambda(df['Returns'], df['Volume'], window=60)\n",
    "df['Kyle_Lambda_240'] = kyle_lambda(df['Returns'], df['Volume'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Order Flow Imbalance (OFI)\n",
    "# =====================================================\n",
    "print(f\"\\n4ï¸âƒ£ è¨ˆç®— Order Flow Imbalance...\")\n",
    "df['OFI_Basic'] = order_flow_imbalance(df['Returns'], volume=None, window=30)\n",
    "df['OFI_Volume_30'] = order_flow_imbalance(df['Returns'], volume=df['Volume'], window=30)\n",
    "df['OFI_Volume_60'] = order_flow_imbalance(df['Returns'], volume=df['Volume'], window=60)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. Volume Imbalance Ratio (VIR)\n",
    "# =====================================================\n",
    "print(f\"\\n5ï¸âƒ£ è¨ˆç®— Volume Imbalance Ratio...\")\n",
    "df['VIR_30'] = volume_imbalance_ratio(df['Returns'], df['Volume'], window=30)\n",
    "df['VIR_60'] = volume_imbalance_ratio(df['Returns'], df['Volume'], window=60)\n",
    "df['VIR_240'] = volume_imbalance_ratio(df['Returns'], df['Volume'], window=240)\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. Realized Spread / Price Impact\n",
    "# =====================================================\n",
    "print(f\"\\n6ï¸âƒ£ è¨ˆç®— Realized Spread / Price Impact...\")\n",
    "df['Realized_Spread_5min'], df['Price_Impact_5min'] = realized_spread_price_impact(\n",
    "    df['Close'], df['Returns'], delta_minutes=5\n",
    ")\n",
    "df['Realized_Spread_15min'], df['Price_Impact_15min'] = realized_spread_price_impact(\n",
    "    df['Close'], df['Returns'], delta_minutes=15\n",
    ")\n",
    "print(f\"   âœ… å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# çµæœçµ±è¨ˆ\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… è¨ˆç®—å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\næ–°å¢æ¬„ä½:\")\n",
    "print(f\"  - Roll_Spread_30/60/240: Roll åƒ¹å·®ï¼ˆä¸åŒçª—å£ï¼‰\")\n",
    "print(f\"  - Amihud_30/60/240: Amihud æµå‹•æ€§ç¼ºä¹æŒ‡æ¨™\")\n",
    "print(f\"  - Kyle_Lambda_60/240: Kyle Lambdaï¼ˆåƒ¹æ ¼è¡æ“Šä¿‚æ•¸ï¼‰\")\n",
    "print(f\"  - OFI_Basic: åŸºæœ¬è¨‚å–®æµä¸å‡è¡¡\")\n",
    "print(f\"  - OFI_Volume_30/60: æˆäº¤é‡åŠ æ¬Šè¨‚å–®æµä¸å‡è¡¡\")\n",
    "print(f\"  - VIR_30/60/240: æˆäº¤é‡ä¸å°ç¨±æ¯”\")\n",
    "print(f\"  - Realized_Spread_5min/15min: æˆäº¤å¾Œåƒ¹æ ¼å›å¾©å¹…åº¦\")\n",
    "print(f\"  - Price_Impact_5min/15min: åƒ¹æ ¼æ°¸ä¹…å½±éŸ¿\")\n",
    "\n",
    "# é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "print(f\"\\nç‰¹å¾µçµ±è¨ˆ:\")\n",
    "microstructure_cols = [\n",
    "    col for col in df.columns \n",
    "    if any(x in col for x in ['Roll', 'Amihud', 'Kyle', 'OFI', 'VIR', 'Realized', 'Price_Impact'])\n",
    "]\n",
    "for col in microstructure_cols:\n",
    "    valid = df[col].notna().sum()\n",
    "    if valid > 0:\n",
    "        mean_val = df[col].mean()\n",
    "        std_val = df[col].std()\n",
    "        print(f\"  {col}: æœ‰æ•ˆå€¼ {valid:,}, å¹³å‡ {mean_val:.6f}, æ¨™æº–å·® {std_val:.6f}\")\n",
    "\n",
    "# é¡¯ç¤ºæœ€å¾Œå¹¾ç­†\n",
    "print(f\"\\næœ€å¾Œ 5 ç­†è³‡æ–™:\")\n",
    "display_cols = [\n",
    "    'Close', 'Volume', 'Returns', \n",
    "    'Roll_Spread_60', 'Amihud_60', 'Kyle_Lambda_60', \n",
    "    'OFI_Volume_30', 'VIR_60', 'Realized_Spread_5min'\n",
    "]\n",
    "available_cols = [col for col in display_cols if col in df.columns]\n",
    "print(df[available_cols].tail())\n",
    "\n",
    "# # å„²å­˜çµæœ\n",
    "# output_file = 'NAS100_Microstructure_Features.csv'\n",
    "# df.to_csv(output_file)\n",
    "# print(f\"\\nâœ… å·²ä¿å­˜åˆ°: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0b0a1",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ7ï¼šML Modelæ§‹å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ7ï¼šML Model æ§‹å»ºèˆ‡è¨“ç·´ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "# =====================================================\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            roc_auc_score, precision_recall_curve, \n",
    "                            roc_curve, accuracy_score, f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    ç°¡å–®çš„æ¨¡å‹è¨“ç·´å™¨ï¼šæ”¯æ´å¤šç¨®æ¨¡å‹ï¼Œæ˜“æ–¼æ“´å±•\n",
    "    \n",
    "    åŠŸèƒ½:\n",
    "    -----\n",
    "    1. æ”¯æ´ XGBoost å’Œ Random Forest\n",
    "    2. ä½¿ç”¨ PurgedKFold äº¤å‰é©—è­‰\n",
    "    3. ä½¿ç”¨ clfHyperFit é€²è¡Œè¶…åƒæ•¸èª¿å„ª\n",
    "    4. æ¨¡å‹è©•ä¼°å’Œå¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'xgboost'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ¨¡å‹è¨“ç·´å™¨\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            æ¨¡å‹é¡å‹ï¼š'xgboost' æˆ– 'rf'\n",
    "        \"\"\"\n",
    "        self.model_type = model_type.lower()\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.cv_scores = None\n",
    "        \n",
    "    def _create_model(self, **kwargs):\n",
    "        \"\"\"\n",
    "        å‰µå»ºæ¨¡å‹å¯¦ä¾‹\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        **kwargs : dict\n",
    "            æ¨¡å‹åƒæ•¸\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        model : classifier\n",
    "            åˆ†é¡å™¨æ¨¡å‹\n",
    "        \"\"\"\n",
    "        if self.model_type == 'xgboost':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'eval_metric': 'logloss',\n",
    "                'use_label_encoder': False\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return XGBClassifier(**default_params)\n",
    "            \n",
    "        elif self.model_type == 'rf':\n",
    "            default_params = {\n",
    "                'random_state': 42\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return RandomForestClassifier(**default_params)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}\")\n",
    "    \n",
    "    def _get_param_grid(self, use_randomized: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å–å¾—åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        use_randomized : bool\n",
    "            æ˜¯å¦ä½¿ç”¨ RandomizedSearchCV\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        param_grid : dict\n",
    "            åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        if self.model_type == 'xgboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                    # ç§»é™¤ gamma, reg_alpha, reg_lambdaï¼ˆé€™äº›å¯èƒ½å°è‡´é æ¸¬åå‘æŸå€‹é¡åˆ¥ï¼‰\n",
    "                }\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    'n_estimators': [100, 200, 300, 500, 1000],\n",
    "                    'max_depth': [5, 10, 15, 20, None],\n",
    "                    'min_samples_split': logUniform(1e-3, 1e2),\n",
    "                    'min_samples_leaf': logUniform(1e-3, 1e1),\n",
    "                    'max_features': ['sqrt', 'log2', None],\n",
    "                    'bootstrap': [True, False]\n",
    "                }\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [5, 10, 15, 20, None],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'max_features': ['sqrt', 'log2', None]\n",
    "                }\n",
    "        \n",
    "        return param_grid\n",
    "    \n",
    "    def train(self, \n",
    "              X: pd.DataFrame,\n",
    "              y: pd.Series,\n",
    "              t1: pd.Series,\n",
    "              sample_weight: Optional[pd.Series] = None,\n",
    "              cv: int = 5,\n",
    "              pctEmbargo: float = 0.01,\n",
    "              use_hyperopt: bool = True,\n",
    "              use_randomized: bool = False,\n",
    "              rndSearchIter: int = 50,\n",
    "              n_jobs: int = -1,\n",
    "              **model_kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¨¡å‹\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µæ•¸æ“š\n",
    "        y : pd.Series\n",
    "            æ¨™ç±¤\n",
    "        t1 : pd.Series\n",
    "            æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        cv : int\n",
    "            K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "        pctEmbargo : float\n",
    "            ç¦åˆ¶æ¯”ä¾‹\n",
    "        use_hyperopt : bool\n",
    "            æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿å„ª\n",
    "        use_randomized : bool\n",
    "            æ˜¯å¦ä½¿ç”¨ RandomizedSearchCV\n",
    "        rndSearchIter : int\n",
    "            RandomizedSearchCV çš„è¿­ä»£æ¬¡æ•¸\n",
    "        n_jobs : int\n",
    "            ä¸¦è¡Œä½œæ¥­æ•¸\n",
    "        **model_kwargs : dict\n",
    "            æ¨¡å‹é¡å¤–åƒæ•¸\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        model : classifier\n",
    "            è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸš€ é–‹å§‹è¨“ç·´ {self.model_type.upper()} æ¨¡å‹\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # æº–å‚™ sample_weight\n",
    "        if sample_weight is None:\n",
    "            sample_weight = pd.Series(1.0, index=X.index)\n",
    "        \n",
    "        if use_hyperopt:\n",
    "            # ä½¿ç”¨è¶…åƒæ•¸èª¿å„ª\n",
    "            print(f\"\\nğŸ“Š ä½¿ç”¨ {'RandomizedSearchCV' if use_randomized else 'GridSearchCV'} é€²è¡Œè¶…åƒæ•¸èª¿å„ª...\")\n",
    "            \n",
    "            # å‰µå»º pipelineï¼ˆç”¨æ–¼ clfHyperFitï¼‰\n",
    "            pipe_clf = MyPipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                (self.model_type, self._create_model(**model_kwargs))\n",
    "            ])\n",
    "            \n",
    "            param_grid = self._get_param_grid(use_randomized=use_randomized)\n",
    "            # æ·»åŠ å‰ç¶´ä»¥ç¬¦åˆ pipeline æ ¼å¼\n",
    "            param_grid_prefixed = {f'{self.model_type}__{k}': v for k, v in param_grid.items()}\n",
    "            \n",
    "            fit_params = {f'{self.model_type}__sample_weight': sample_weight}\n",
    "            \n",
    "            # ä½¿ç”¨ clfHyperFit é€²è¡Œè¶…åƒæ•¸èª¿å„ª\n",
    "            best_pipeline = clfHyperFit(\n",
    "                feat=X,\n",
    "                lbl=y,\n",
    "                t1=t1,\n",
    "                pipe_clf=pipe_clf,\n",
    "                param_grid=param_grid_prefixed,\n",
    "                cv=cv,\n",
    "                bagging=[0, None, 1.],  # ä¸ä½¿ç”¨ bagging\n",
    "                rndSearchIter=rndSearchIter if use_randomized else 0,\n",
    "                n_jobs=n_jobs,\n",
    "                pctEmbargo=pctEmbargo,\n",
    "                **fit_params\n",
    "            )\n",
    "            \n",
    "            # æå–å¯¦éš›çš„åˆ†é¡å™¨æ¨¡å‹\n",
    "            if 'bag' in best_pipeline.named_steps:\n",
    "                # å¦‚æœæ˜¯ bagging pipeline\n",
    "                self.model = best_pipeline.named_steps['bag']\n",
    "            else:\n",
    "                # æå–åˆ†é¡å™¨\n",
    "                self.model = best_pipeline.named_steps[self.model_type]\n",
    "            \n",
    "            # æå–æœ€ä½³åƒæ•¸\n",
    "            if hasattr(best_pipeline, 'best_params_'):\n",
    "                self.best_params = best_pipeline.best_params_\n",
    "            \n",
    "            print(f\"\\nâœ… è¶…åƒæ•¸èª¿å„ªå®Œæˆ\")\n",
    "            if self.best_params:\n",
    "                print(f\"æœ€ä½³åƒæ•¸:\")\n",
    "                for key, value in self.best_params.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            # ä¸ä½¿ç”¨è¶…åƒæ•¸èª¿å„ªï¼Œç›´æ¥è¨“ç·´\n",
    "            print(f\"\\nğŸ“Š ç›´æ¥è¨“ç·´æ¨¡å‹ï¼ˆä¸ä½¿ç”¨è¶…åƒæ•¸èª¿å„ªï¼‰...\")\n",
    "            \n",
    "            # ä½¿ç”¨ cvScore é€²è¡Œäº¤å‰é©—è­‰\n",
    "            inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "            clf = self._create_model(**model_kwargs)\n",
    "            \n",
    "            self.cv_scores = cvScore(\n",
    "                clf=clf,\n",
    "                X=X,\n",
    "                y=y,\n",
    "                sample_weight=sample_weight,\n",
    "                scoring='f1',\n",
    "                cvGen=inner_cv\n",
    "            )\n",
    "            \n",
    "            print(f\"\\näº¤å‰é©—è­‰åˆ†æ•¸: {self.cv_scores}\")\n",
    "            print(f\"å¹³å‡åˆ†æ•¸: {self.cv_scores.mean():.4f} Â± {self.cv_scores.std():.4f}\")\n",
    "            \n",
    "            # åœ¨å…¨éƒ¨æ•¸æ“šä¸Šè¨“ç·´\n",
    "            self.model = clf.fit(X, y, sample_weight=sample_weight.values)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        é€²è¡Œé æ¸¬\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µæ•¸æ“š\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : np.ndarray\n",
    "            é¡åˆ¥é æ¸¬\n",
    "        probabilities : np.ndarray\n",
    "            æ¦‚ç‡é æ¸¬\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        predictions = self.model.predict(X)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def evaluate(self, \n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight: Optional[pd.Series] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        è©•ä¼°æ¨¡å‹æ€§èƒ½\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µæ•¸æ“š\n",
    "        y : pd.Series\n",
    "            çœŸå¯¦æ¨™ç±¤\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            è©•ä¼°æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        y_pred, y_proba = self.predict(X)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred, sample_weight=sample_weight),\n",
    "            'f1': f1_score(y, y_pred, sample_weight=sample_weight, average='weighted'),\n",
    "        }\n",
    "        \n",
    "        # å¦‚æœæ˜¯äºŒåˆ†é¡ï¼Œè¨ˆç®—æ›´å¤šæŒ‡æ¨™\n",
    "        if len(np.unique(y)) == 2:\n",
    "            metrics['roc_auc'] = roc_auc_score(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            metrics['f1_binary'] = f1_score(y, y_pred, sample_weight=sample_weight)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self, \n",
    "                    X: pd.DataFrame,\n",
    "                    y: pd.Series,\n",
    "                    sample_weight: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½æ¨¡å‹è©•ä¼°çµæœ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µæ•¸æ“š\n",
    "        y : pd.Series\n",
    "            çœŸå¯¦æ¨™ç±¤\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        y_pred, y_proba = self.predict(X)\n",
    "        metrics = self.evaluate(X, y, sample_weight)\n",
    "        \n",
    "        # ç¹ªè£½çµæœ\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. æ··æ·†çŸ©é™£\n",
    "        cm = confusion_matrix(y, y_pred, sample_weight=sample_weight)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "        axes[0, 0].set_ylabel('True Label', fontsize=12)\n",
    "        axes[0, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "        \n",
    "        # 2. ROC æ›²ç·šï¼ˆå¦‚æœæ˜¯äºŒåˆ†é¡ï¼‰\n",
    "        if len(np.unique(y)) == 2:\n",
    "            fpr, tpr, _ = roc_curve(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
    "            axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n",
    "            axes[0, 1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "            axes[0, 1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "            axes[0, 1].set_title('ROC Curve', fontsize=14)\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'ROC Curve\\n(Only for binary classification)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 1].set_title('ROC Curve', fontsize=14)\n",
    "        \n",
    "        # 3. Precision-Recall æ›²ç·šï¼ˆå¦‚æœæ˜¯äºŒåˆ†é¡ï¼‰\n",
    "        if len(np.unique(y)) == 2:\n",
    "            precision, recall, _ = precision_recall_curve(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            axes[1, 0].plot(recall, precision, linewidth=2)\n",
    "            axes[1, 0].set_xlabel('Recall', fontsize=12)\n",
    "            axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
    "            axes[1, 0].set_title('Precision-Recall Curve', fontsize=14)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Precision-Recall Curve\\n(Only for binary classification)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 0].set_title('Precision-Recall Curve', fontsize=14)\n",
    "        \n",
    "        # 4. è©•ä¼°æŒ‡æ¨™æ‘˜è¦\n",
    "        axes[1, 1].axis('off')\n",
    "        metrics_text = f\"\"\"\n",
    "        Model: {self.model_type.upper()}\n",
    "        \n",
    "        Evaluation Metrics:\n",
    "        --------------------\n",
    "        Accuracy: {metrics['accuracy']:.4f}\n",
    "        F1 Score: {metrics['f1']:.4f}\n",
    "        \"\"\"\n",
    "        if 'roc_auc' in metrics:\n",
    "            metrics_text += f\"ROC AUC: {metrics['roc_auc']:.4f}\\n\"\n",
    "        if 'f1_binary' in metrics:\n",
    "            metrics_text += f\"F1 (Binary): {metrics['f1_binary']:.4f}\\n\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, \n",
    "                        verticalalignment='center', family='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ‰“å°è©³ç´°å ±å‘Š\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š è©³ç´°åˆ†é¡å ±å‘Š\")\n",
    "        print(\"=\" * 60)\n",
    "        print(classification_report(y, y_pred, sample_weight=sample_weight))\n",
    "        print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc72f07",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ8ï¼šEmbargo Purged K-Fold Classäº¤å‰é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Purged K-Fold Cross-Validation (Snippet 7.3 & 7.4)\n",
    "# =====================================================\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Generator, Tuple\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "class PurgedKFold(_BaseKFold):\n",
    "    \"\"\"\n",
    "    Extend KFold class to work with labels that span intervals\n",
    "    \n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    é€™å€‹é¡åˆ¥æ“´å±•äº† scikit-learn çš„ KFoldï¼Œç”¨æ–¼è™•ç†æ¨™ç±¤é‡ç–Šçš„æƒ…æ³ã€‚\n",
    "    ç•¶æ¨™ç±¤è·¨è¶Šæ™‚é–“å€é–“æ™‚ï¼ˆä¾‹å¦‚ Triple Barrier Methodï¼‰ï¼Œæ¸¬è©¦é›†çš„æ¨™ç±¤\n",
    "    å¯èƒ½æœƒèˆ‡è¨“ç·´é›†çš„æ¨™ç±¤é‡ç–Šï¼Œå°è‡´æ•¸æ“šæ´©æ¼ã€‚\n",
    "    \n",
    "    è§£æ±ºæ–¹æ³•:\n",
    "    ---------\n",
    "    1. Purgingï¼ˆæ¸…é™¤ï¼‰: å¾è¨“ç·´é›†ä¸­ç§»é™¤èˆ‡æ¸¬è©¦é›†æ¨™ç±¤å€é–“é‡ç–Šçš„è§€æ¸¬å€¼\n",
    "    2. Embargoï¼ˆç¦åˆ¶ï¼‰: åœ¨æ¸¬è©¦é›†çµæŸå¾Œï¼Œé¡å¤–ç§»é™¤ä¸€æ®µæ™‚é–“çš„è¨“ç·´æ¨£æœ¬\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    n_splits : int\n",
    "        K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "    t1 : pd.Series\n",
    "        æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰ï¼Œindex å¿…é ˆèˆ‡ X çš„ index ä¸€è‡´\n",
    "    pctEmbargo : float\n",
    "        ç¦åˆ¶æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œä¾‹å¦‚ 0.01 è¡¨ç¤ºç¦åˆ¶ 1% çš„æ•¸æ“š\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 3, t1: Optional[pd.Series] = None, pctEmbargo: float = 0.0):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– PurgedKFold\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        n_splits : int\n",
    "            K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "        t1 : pd.Series, optional\n",
    "            æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰ï¼Œindex å¿…é ˆèˆ‡ X çš„ index ä¸€è‡´\n",
    "        pctEmbargo : float\n",
    "            ç¦åˆ¶æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œä¾‹å¦‚ 0.01 è¡¨ç¤ºç¦åˆ¶ 1% çš„æ•¸æ“š\n",
    "        \"\"\"\n",
    "        if t1 is not None and not isinstance(t1, pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pd.Series')\n",
    "        \n",
    "        super(PurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.t1 = t1\n",
    "        self.pctEmbargo = pctEmbargo\n",
    "    \n",
    "    def split(self, X: pd.DataFrame, y: Optional[pd.Series] = None, \n",
    "              groups: Optional[np.ndarray] = None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆè¨“ç·´/æ¸¬è©¦é›†çš„ç´¢å¼•\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µæ•¸æ“šï¼ˆindex å¿…é ˆèˆ‡ t1 çš„ index ä¸€è‡´ï¼‰\n",
    "        y : pd.Series, optional\n",
    "            æ¨™ç±¤ï¼ˆæœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ç¬¦åˆ scikit-learn æ¥å£ï¼‰\n",
    "        groups : np.ndarray, optional\n",
    "            åˆ†çµ„ï¼ˆæœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ç¬¦åˆ scikit-learn æ¥å£ï¼‰\n",
    "            \n",
    "        Yields:\n",
    "        -------\n",
    "        train_indices : np.ndarray\n",
    "            è¨“ç·´é›†ç´¢å¼•\n",
    "        test_indices : np.ndarray\n",
    "            æ¸¬è©¦é›†ç´¢å¼•\n",
    "        \"\"\"\n",
    "        if self.t1 is None:\n",
    "            raise ValueError('t1 (ThruDate) must be provided')\n",
    "        \n",
    "        # é©—è­‰ X å’Œ t1 çš„ index ä¸€è‡´\n",
    "        if (X.index == self.t1.index).sum() != len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "        \n",
    "        indices = np.arange(X.shape[0])\n",
    "        mbrg = int(X.shape[0] * self.pctEmbargo)  # embargo çš„æ¨£æœ¬æ•¸\n",
    "        \n",
    "        # å°‡æ•¸æ“šåˆ†æˆ n_splits å€‹é€£çºŒçš„æ¸¬è©¦é›†\n",
    "        test_starts = [(i[0], i[-1] + 1) for i in \n",
    "                      np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "        \n",
    "        for i, j in test_starts:\n",
    "            # æ¸¬è©¦é›†çš„èµ·å§‹æ™‚é–“\n",
    "            t0 = self.t1.index[i]  # start of test set\n",
    "            test_indices = indices[i:j]\n",
    "            \n",
    "            # æ‰¾åˆ°æ¸¬è©¦é›†ä¸­æœ€æ™šçš„çµæŸæ™‚é–“\n",
    "            maxT1Idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
    "            \n",
    "            # å·¦å´è¨“ç·´é›†ï¼šçµæŸæ™‚é–“ <= t0 çš„æ‰€æœ‰æ¨£æœ¬\n",
    "            train_indices = self.t1.index.searchsorted(\n",
    "                self.t1[self.t1 <= t0].index\n",
    "            )\n",
    "            \n",
    "            # å³å´è¨“ç·´é›†ï¼šåœ¨æ¸¬è©¦é›†çµæŸå¾Œï¼ŒåŠ ä¸Š embargo æœŸé–“\n",
    "            if maxT1Idx < X.shape[0]:\n",
    "                # right train (with embargo)\n",
    "                train_indices = np.concatenate((\n",
    "                    train_indices, \n",
    "                    indices[maxT1Idx + mbrg:]\n",
    "                ))\n",
    "            \n",
    "            yield train_indices, test_indices\n",
    "\n",
    "\n",
    "def cvScore(clf, X: pd.DataFrame, y: pd.Series, \n",
    "            sample_weight: Optional[pd.Series] = None,\n",
    "            scoring: str = 'f1',\n",
    "            t1: Optional[pd.Series] = None,\n",
    "            cv: Optional[int] = None,\n",
    "            cvGen: Optional[PurgedKFold] = None,\n",
    "            pctEmbargo: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ PurgedKFold é€²è¡Œäº¤å‰é©—è­‰è©•åˆ†\n",
    "    \n",
    "    é€™å€‹å‡½æ•¸é¿å…äº† scikit-learn çš„ cross_val_score åœ¨è™•ç†é‡ç–Šæ¨™ç±¤æ™‚çš„ bugã€‚\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    clf : sklearn classifier\n",
    "        åˆ†é¡å™¨æ¨¡å‹\n",
    "    X : pd.DataFrame\n",
    "        ç‰¹å¾µæ•¸æ“š\n",
    "    y : pd.Series\n",
    "        æ¨™ç±¤\n",
    "    sample_weight : pd.Series, optional\n",
    "        æ¨£æœ¬æ¬Šé‡\n",
    "    scoring : str\n",
    "        è©•åˆ†æ–¹æ³•ï¼š'neg_log_loss' æˆ– 'accuracy'\n",
    "    t1 : pd.Series, optional\n",
    "        æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆå¦‚æœ cvGen ç‚º Noneï¼Œå‰‡å¿…é ˆæä¾›ï¼‰\n",
    "    cv : int, optional\n",
    "        K æŠ˜æ•¸ï¼ˆå¦‚æœ cvGen ç‚º Noneï¼Œå‰‡å¿…é ˆæä¾›ï¼‰\n",
    "    cvGen : PurgedKFold, optional\n",
    "        è‡ªå®šç¾©çš„äº¤å‰é©—è­‰ç”Ÿæˆå™¨\n",
    "    pctEmbargo : float, optional\n",
    "        ç¦åˆ¶æ¯”ä¾‹ï¼ˆå¦‚æœ cvGen ç‚º Noneï¼Œå‰‡å¿…é ˆæä¾›ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    score : np.ndarray\n",
    "        æ¯æŠ˜çš„è©•åˆ†çµæœ\n",
    "    \"\"\"\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception('Wrong scoring method. Must be \"neg_log_loss\" or \"accuracy\".')\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰æä¾› cvGenï¼Œå‰µå»ºä¸€å€‹\n",
    "    if cvGen is None:\n",
    "        if t1 is None or cv is None or pctEmbargo is None:\n",
    "            raise ValueError('If cvGen is None, t1, cv, and pctEmbargo must be provided')\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰æä¾› sample_weightï¼Œä½¿ç”¨å‡å‹»æ¬Šé‡\n",
    "    if sample_weight is None:\n",
    "        sample_weight = pd.Series(1.0, index=X.index)\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    # å°æ¯ä¸€æŠ˜é€²è¡Œè¨“ç·´å’Œè©•ä¼°\n",
    "    for train_idx, test_idx in cvGen.split(X=X):\n",
    "        # è¨“ç·´æ¨¡å‹\n",
    "        fit = clf.fit(\n",
    "            X=X.iloc[train_idx, :],\n",
    "            y=y.iloc[train_idx],\n",
    "            sample_weight=sample_weight.iloc[train_idx].values\n",
    "        )\n",
    "        \n",
    "        # æ ¹æ“šè©•åˆ†æ–¹æ³•è¨ˆç®—åˆ†æ•¸\n",
    "        if scoring == 'neg_log_loss':\n",
    "            # ä½¿ç”¨è² å°æ•¸æå¤±ï¼ˆå› ç‚º cross_val_score è¿”å›è² å€¼ï¼‰\n",
    "            prob = fit.predict_proba(X.iloc[test_idx, :])\n",
    "            score_ = -log_loss(\n",
    "                y.iloc[test_idx],\n",
    "                prob,\n",
    "                sample_weight=sample_weight.iloc[test_idx].values,\n",
    "                labels=clf.classes_\n",
    "            )\n",
    "        else:  # accuracy\n",
    "            pred = fit.predict(X.iloc[test_idx, :])\n",
    "            score_ = accuracy_score(\n",
    "                y.iloc[test_idx],\n",
    "                pred,\n",
    "                sample_weight=sample_weight.iloc[test_idx].values\n",
    "            )\n",
    "        \n",
    "        score.append(score_)\n",
    "    \n",
    "    return np.array(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23efcb",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ9 ï¼š è¶…åƒæ•¸èª¿æ•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Hyperparameter Tuning with Purged K-Fold CV\n",
    "# Snippet 9.1, 9.2, 9.3, 9.4\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from scipy.stats import rv_continuous, kstest\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.2: Enhanced Pipeline Class\n",
    "# =====================================================\n",
    "\n",
    "class MyPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Enhanced Pipeline class that handles sample_weight argument\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    sklearn çš„ Pipeline çš„ fit æ–¹æ³•ä¸æ¥å— sample_weight åƒæ•¸ï¼Œ\n",
    "    è€Œæ˜¯æœŸæœ› fit_params é—œéµå­—åƒæ•¸ã€‚é€™æ˜¯ä¸€å€‹å·²çŸ¥çš„ bugã€‚\n",
    "    é€™å€‹é¡åˆ¥æ“´å±•äº† Pipelineï¼Œé‡å¯« fit æ–¹æ³•ä»¥è™•ç† sample_weightã€‚\n",
    "    \n",
    "    åƒè€ƒ:\n",
    "    -----\n",
    "    - GitHub issue: sklearn Pipeline sample_weight bug\n",
    "    - Stackoverflow: Understanding Python super with __init__ methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \"\"\"\n",
    "        é‡å¯« fit æ–¹æ³•ä»¥è™•ç† sample_weight\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            ç‰¹å¾µæ•¸æ“š\n",
    "        y : array-like\n",
    "            æ¨™ç±¤\n",
    "        sample_weight : array-like, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        **fit_params : dict\n",
    "            å…¶ä»– fit åƒæ•¸\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : MyPipeline\n",
    "            æ“¬åˆå¾Œçš„ pipeline\n",
    "        \"\"\"\n",
    "        if sample_weight is not None:\n",
    "            # å°‡ sample_weight å‚³éçµ¦æœ€å¾Œä¸€å€‹æ­¥é©Ÿï¼ˆé€šå¸¸æ˜¯åˆ†é¡å™¨ï¼‰\n",
    "            last_step_name = self.steps[-1][0]\n",
    "            fit_params[f'{last_step_name}__sample_weight'] = sample_weight\n",
    "        \n",
    "        return super(MyPipeline, self).fit(X, y, **fit_params)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.4: Log-Uniform Distribution\n",
    "# =====================================================\n",
    "\n",
    "class logUniform_gen(rv_continuous):\n",
    "    \"\"\"\n",
    "    Log-uniform distribution random variable generator\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    éš¨æ©Ÿè®Šæ•¸ x åœ¨ [a, b] å€é–“å…§éµå¾ª log-uniform åˆ†å¸ƒï¼Œ\n",
    "    ç•¶ä¸”åƒ…ç•¶ log[x] ~ U[log[a], log[b]]\n",
    "    \n",
    "    é€™å€‹åˆ†å¸ƒå°æ–¼æ¢ç´¢éç·šæ€§éŸ¿æ‡‰çš„åƒæ•¸ç©ºé–“ç‰¹åˆ¥æœ‰æ•ˆï¼Œ\n",
    "    ä¾‹å¦‚ SVC çš„ C åƒæ•¸å’Œ RBF kernel çš„ gamma åƒæ•¸ã€‚\n",
    "    \n",
    "    æ•¸å­¸å®šç¾©:\n",
    "    ---------\n",
    "    CDF: F[x] = log[x] - log[a] / log[b] - log[a]  for a â‰¤ x â‰¤ b\n",
    "    PDF: f[x] = 1 / (x * log[b/a])                 for a â‰¤ x â‰¤ b\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cdf(self, x):\n",
    "        \"\"\"ç´¯ç©åˆ†å¸ƒå‡½æ•¸ (CDF)\"\"\"\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def logUniform(a=1, b=np.exp(1)):\n",
    "    \"\"\"\n",
    "    å‰µå»º log-uniform åˆ†å¸ƒ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    a : float\n",
    "        ä¸‹ç•Œï¼ˆå¿…é ˆ > 0ï¼‰\n",
    "    b : float\n",
    "        ä¸Šç•Œï¼ˆå¿…é ˆ > aï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    logUniform_gen : logUniform_gen\n",
    "        Log-uniform åˆ†å¸ƒç”Ÿæˆå™¨\n",
    "    \"\"\"\n",
    "    return logUniform_gen(a=a, b=b, name='logUniform')\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.1 & 9.3: Hyperparameter Fitting Function\n",
    "# =====================================================\n",
    "\n",
    "def clfHyperFit(feat: pd.DataFrame, \n",
    "                lbl: pd.Series,\n",
    "                t1: pd.Series,\n",
    "                pipe_clf: Pipeline,\n",
    "                param_grid: Dict[str, List[Any]],\n",
    "                cv: int = 3,\n",
    "                bagging: List[Any] = [0, None, 1.],\n",
    "                rndSearchIter: int = 0,\n",
    "                n_jobs: int = -1,\n",
    "                pctEmbargo: float = 0.0,\n",
    "                **fit_params) -> Pipeline:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Purged K-Fold äº¤å‰é©—è­‰é€²è¡Œè¶…åƒæ•¸èª¿å„ª\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    é€™å€‹å‡½æ•¸å¯¦ç¾äº†å¸¶æœ‰ purging å’Œ embargo çš„ GridSearchCV æˆ– RandomizedSearchCVã€‚\n",
    "    å®ƒé¿å…äº† scikit-learn çš„ cross_val_score åœ¨è™•ç†é‡ç–Šæ¨™ç±¤æ™‚çš„ bugã€‚\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    feat : pd.DataFrame\n",
    "        ç‰¹å¾µæ•¸æ“š\n",
    "    lbl : pd.Series\n",
    "        æ¨™ç±¤\n",
    "    t1 : pd.Series\n",
    "        æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰ï¼Œindex å¿…é ˆèˆ‡ feat çš„ index ä¸€è‡´\n",
    "    pipe_clf : Pipeline\n",
    "        åˆ†é¡å™¨ pipelineï¼ˆæ‡‰ä½¿ç”¨ MyPipelineï¼‰\n",
    "    param_grid : dict\n",
    "        åƒæ•¸ç¶²æ ¼ï¼ˆGridSearchCVï¼‰æˆ–åƒæ•¸åˆ†å¸ƒï¼ˆRandomizedSearchCVï¼‰\n",
    "        - GridSearchCV: {'param1': [val1, val2], 'param2': [val3, val4]}\n",
    "        - RandomizedSearchCV: {'param1': logUniform(1e-3, 1e3), ...}\n",
    "    cv : int\n",
    "        K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "    bagging : list\n",
    "        Bagging åƒæ•¸ [n_estimators, max_samples, max_features]\n",
    "        - n_estimators: Bagging åˆ†é¡å™¨çš„æ•¸é‡\n",
    "        - max_samples: æ¯å€‹åˆ†é¡å™¨ä½¿ç”¨çš„æ¨£æœ¬æ¯”ä¾‹ï¼ˆNone è¡¨ç¤ºä¸ baggingï¼‰\n",
    "        - max_features: æ¯å€‹åˆ†é¡å™¨ä½¿ç”¨çš„ç‰¹å¾µæ¯”ä¾‹\n",
    "    rndSearchIter : int\n",
    "        å¦‚æœ > 0ï¼Œä½¿ç”¨ RandomizedSearchCVï¼Œå¦å‰‡ä½¿ç”¨ GridSearchCV\n",
    "    n_jobs : int\n",
    "        ä¸¦è¡Œä½œæ¥­æ•¸ï¼ˆ-1 è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ CPUï¼‰\n",
    "    pctEmbargo : float\n",
    "        ç¦åˆ¶æ¯”ä¾‹ï¼ˆ0-1ï¼‰\n",
    "    **fit_params : dict\n",
    "        å…¶ä»– fit åƒæ•¸ï¼ˆä¾‹å¦‚ sample_weightï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    gs : Pipeline\n",
    "        èª¿å„ªå¾Œçš„åˆ†é¡å™¨ pipelineï¼ˆå¯èƒ½åŒ…å« baggingï¼‰\n",
    "        \n",
    "    è©•åˆ†æ–¹æ³•:\n",
    "    ---------\n",
    "    - Meta-labeling (æ¨™ç±¤ç‚º {0, 1}): ä½¿ç”¨ 'f1' score\n",
    "    - å…¶ä»–æ‡‰ç”¨: ä½¿ç”¨ 'neg_log_loss'\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    1. å°æ–¼ meta-labelingï¼Œä½¿ç”¨ 'f1' è€Œä¸æ˜¯ 'accuracy' æˆ– 'neg_log_loss'ï¼Œ\n",
    "       å› ç‚ºåœ¨è² æ¨£æœ¬å¾ˆå¤šçš„æƒ…æ³ä¸‹ï¼Œé æ¸¬å…¨éƒ¨ç‚ºè² çš„æ¨¡å‹æœƒå¾—åˆ°é«˜åˆ†ï¼Œ\n",
    "       ä½†å¯¦éš›ä¸Šæ²’æœ‰å­¸åˆ°ä»»ä½•æœ‰ç”¨çš„æ¨¡å¼ã€‚\n",
    "    2. å°æ–¼æŠ•è³‡ç­–ç•¥ï¼Œä½¿ç”¨ 'neg_log_loss' è€Œä¸æ˜¯ 'accuracy'ï¼Œ\n",
    "       å› ç‚º log loss è€ƒæ…®äº†é æ¸¬æ¦‚ç‡ï¼Œæ›´é©åˆè©•ä¼°é«˜ç½®ä¿¡åº¦é æ¸¬çš„è¡¨ç¾ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # å›ºå®šä½¿ç”¨ F1 ä½œç‚ºè©•åˆ†æ¨™æº–\n",
    "    scoring = 'f1'\n",
    "    \n",
    "    # 1) è¶…åƒæ•¸æœå°‹ï¼Œåœ¨è¨“ç·´æ•¸æ“šä¸Š\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "    \n",
    "    if rndSearchIter == 0:\n",
    "        gs = GridSearchCV(\n",
    "            estimator=pipe_clf,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "    else:\n",
    "        gs = RandomizedSearchCV(\n",
    "            estimator=pipe_clf,\n",
    "            param_distributions=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs,\n",
    "            n_iter=rndSearchIter\n",
    "        )\n",
    "    \n",
    "    gs = gs.fit(feat, lbl, **fit_params).best_estimator_\n",
    "    \n",
    "    # 2) åœ¨å…¨éƒ¨æ•¸æ“šä¸Šæ“¬åˆé©—è­‰å¾Œçš„æ¨¡å‹\n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(\n",
    "            base_estimator=MyPipeline(gs.steps),\n",
    "            n_estimators=int(bagging[0]),\n",
    "            max_samples=float(bagging[1]),\n",
    "            max_features=float(bagging[2]),\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        \n",
    "        last_step_name = gs.base_estimator.steps[-1][0]\n",
    "        sample_weight_key = f'{last_step_name}__sample_weight'\n",
    "        sample_weight = fit_params.get(sample_weight_key)\n",
    "        \n",
    "        gs = gs.fit(feat, lbl, sample_weight=sample_weight)\n",
    "        gs = Pipeline([('bag', gs)])\n",
    "    \n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3b637",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ10 ï¼š åŸ·è¡Œé æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef156f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = final_weights_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a336f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ10ï¼šPredictionPipeline - æ•´åˆæ­¥é©Ÿ7ã€8ã€9\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any, Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PredictionPipeline:\n",
    "    \"\"\"\n",
    "    é æ¸¬ Pipelineï¼šæ•´åˆæ¨¡å‹è¨“ç·´ã€äº¤å‰é©—è­‰ã€è¶…åƒæ•¸èª¿æ•´\n",
    "    \n",
    "    åŠŸèƒ½:\n",
    "    -----\n",
    "    1. æ¨¡å‹è¨“ç·´ï¼ˆæ­¥é©Ÿ7ï¼‰\n",
    "    2. PurgedKFold äº¤å‰é©—è­‰ï¼ˆæ­¥é©Ÿ8ï¼‰\n",
    "    3. è¶…åƒæ•¸èª¿æ•´ï¼ˆæ­¥é©Ÿ9ï¼‰\n",
    "    4. æ¨¡å‹é æ¸¬\n",
    "    5. çµæœè©•ä¼°å’Œå¯è¦–åŒ–\n",
    "    \n",
    "    ä½¿ç”¨æ–¹å¼:\n",
    "    --------\n",
    "    pipeline = PredictionPipeline(\n",
    "        model_type='xgboost',  # æˆ– 'rf'\n",
    "        use_hyperopt=True,     # æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´\n",
    "        use_cv=True            # æ˜¯å¦ä½¿ç”¨äº¤å‰é©—è­‰\n",
    "    )\n",
    "    \n",
    "    # å‚³å…¥æº–å‚™å¥½çš„æ•¸æ“š\n",
    "    pipeline.fit(\n",
    "        X=X,                    # ç‰¹å¾µçŸ©é™£\n",
    "        y=y,                    # æ¨™ç±¤\n",
    "        t1=t1,                  # æ¨™ç±¤çµæŸæ™‚é–“\n",
    "        sample_weight=weights   # æ¨£æœ¬æ¬Šé‡ï¼ˆå¯é¸ï¼‰\n",
    "    )\n",
    "    \n",
    "    # é æ¸¬\n",
    "    predictions, probabilities = pipeline.predict(X_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_type: str = 'xgboost',\n",
    "                 use_hyperopt: bool = True,\n",
    "                 use_cv: bool = True,\n",
    "                 cv_splits: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 hyperopt_method: str = 'randomized',  # 'randomized' æˆ– 'grid'\n",
    "                 n_iter: int = 30,\n",
    "                 bagging: Optional[List[Any]] = None,\n",
    "                 n_jobs: int = -1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–é æ¸¬ Pipeline\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            æ¨¡å‹é¡å‹ï¼š'xgboost' æˆ– 'rf'\n",
    "        use_hyperopt : bool\n",
    "            æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_cv : bool\n",
    "            æ˜¯å¦ä½¿ç”¨äº¤å‰é©—è­‰ï¼ˆé è¨­ Trueï¼‰\n",
    "        cv_splits : int\n",
    "            äº¤å‰é©—è­‰æŠ˜æ•¸ï¼ˆé è¨­ 5ï¼‰\n",
    "        pct_embargo : float\n",
    "            ç¦åˆ¶æ¯”ä¾‹ï¼ˆé è¨­ 0.01ï¼‰\n",
    "        hyperopt_method : str\n",
    "            è¶…åƒæ•¸èª¿æ•´æ–¹æ³•ï¼š'randomized' æˆ– 'grid'ï¼ˆé è¨­ 'randomized'ï¼‰\n",
    "        n_iter : int\n",
    "            éš¨æ©Ÿæœå°‹è¿­ä»£æ¬¡æ•¸ï¼ˆé è¨­ 30ï¼‰\n",
    "        bagging : list, optional\n",
    "            Bagging åƒæ•¸ [n_estimators, max_samples, max_features]\n",
    "        n_jobs : int\n",
    "            ä¸¦è¡Œä½œæ¥­æ•¸ï¼ˆé è¨­ -1ï¼‰\n",
    "        \"\"\"\n",
    "        self.model_type = model_type.lower()\n",
    "        self.use_hyperopt = use_hyperopt\n",
    "        self.use_cv = use_cv\n",
    "        self.cv_splits = cv_splits\n",
    "        self.pct_embargo = pct_embargo\n",
    "        self.hyperopt_method = hyperopt_method\n",
    "        self.n_iter = n_iter\n",
    "        self.bagging = bagging if bagging is not None else [0, None, 1.]\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        # å…§éƒ¨è®Šé‡\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.best_params = None\n",
    "        self.cv_scores = None\n",
    "        self.best_cv_scores = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.t1_train = None\n",
    "        self.sample_weight_train = None\n",
    "        self.feature_columns = None\n",
    "        \n",
    "    def _create_pipeline(self, **model_kwargs) -> MyPipeline:\n",
    "        \"\"\"\n",
    "        å‰µå»ºæ¨¡å‹ pipeline\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        **model_kwargs : dict\n",
    "            æ¨¡å‹é¡å¤–åƒæ•¸\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pipeline : MyPipeline\n",
    "            åŒ…å« scaler å’Œåˆ†é¡å™¨çš„ pipeline\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        if self.model_type == 'xgboost':\n",
    "            from xgboost import XGBClassifier\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'eval_metric': 'mlogloss',  # å¤šåˆ†é¡ä½¿ç”¨ mlogloss\n",
    "                'use_label_encoder': False,\n",
    "                'objective': 'multi:softprob'  # æ˜ç¢ºæŒ‡å®šå¤šåˆ†é¡ç›®æ¨™\n",
    "            }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = XGBClassifier(**default_params)\n",
    "            \n",
    "        elif self.model_type == 'rf':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced'  # ä½¿ç”¨å¹³è¡¡çš„é¡åˆ¥æ¬Šé‡\n",
    "            }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = RandomForestClassifier(**default_params)\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}\")\n",
    "        \n",
    "        pipe = MyPipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            (self.model_type, clf)\n",
    "        ])\n",
    "        \n",
    "        return pipe\n",
    "    \n",
    "    def _get_param_grid(self, use_randomized: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å–å¾—åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        use_randomized : bool\n",
    "            æ˜¯å¦ä½¿ç”¨ RandomizedSearchCV\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        param_grid : dict\n",
    "            åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        if self.model_type == 'xgboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10, 15],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5, 7],\n",
    "                    f'{self.model_type}__gamma': logUniform(1e-5, 1e-1),\n",
    "                    f'{self.model_type}__reg_alpha': logUniform(1e-5, 1e-1),\n",
    "                    f'{self.model_type}__reg_lambda': logUniform(1e-5, 1e-1)\n",
    "                }\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                    f'{self.model_type}__gamma': [0, 0.1, 0.2],\n",
    "                    f'{self.model_type}__reg_alpha': [0, 0.01, 0.1],\n",
    "                    f'{self.model_type}__reg_lambda': [1, 1.5, 2]\n",
    "                }\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500, 1000],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': logUniform(1e-3, 1e2),\n",
    "                    f'{self.model_type}__min_samples_leaf': logUniform(1e-3, 1e1),\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                    f'{self.model_type}__bootstrap': [True, False]\n",
    "                }\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None]\n",
    "                }\n",
    "        \n",
    "        return param_grid\n",
    "    \n",
    "    def fit(self,\n",
    "            X: pd.DataFrame,\n",
    "            y: pd.Series,\n",
    "            t1: pd.Series,\n",
    "            sample_weight: Optional[pd.Series] = None,\n",
    "            **model_kwargs) -> 'PredictionPipeline':\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¨¡å‹ï¼ˆæ•´åˆæ­¥é©Ÿ7ã€8ã€9ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "        y : pd.Series\n",
    "            æ¨™ç±¤\n",
    "        t1 : pd.Series\n",
    "            æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        **model_kwargs : dict\n",
    "            æ¨¡å‹é¡å¤–åƒæ•¸ï¼ˆå¦‚æœä¸ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : PredictionPipeline\n",
    "            è¨“ç·´å¥½çš„ pipeline\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ ({self.model_type.upper()})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # ä¿å­˜è¨“ç·´æ•¸æ“š\n",
    "        self.X_train = X.copy()\n",
    "        self.y_train = y.copy()\n",
    "        self.t1_train = t1.copy()\n",
    "        self.sample_weight_train = sample_weight.copy() if sample_weight is not None else None\n",
    "        self.feature_columns = X.columns.tolist()\n",
    "        \n",
    "        # æ•¸æ“šé©—è­‰\n",
    "        common_idx = X.index.intersection(y.index).intersection(t1.index)\n",
    "        if len(common_idx) < len(X.index):\n",
    "            print(f\"âš ï¸ è­¦å‘Šï¼šæ•¸æ“šç´¢å¼•ä¸å®Œå…¨ä¸€è‡´ï¼Œä½¿ç”¨å…±åŒç´¢å¼•\")\n",
    "            print(f\"   X: {len(X.index):,}, y: {len(y.index):,}, t1: {len(t1.index):,}\")\n",
    "            print(f\"   å…±åŒç´¢å¼•: {len(common_idx):,}\")\n",
    "            self.X_train = X.loc[common_idx]\n",
    "            self.y_train = y.loc[common_idx]\n",
    "            self.t1_train = t1.loc[common_idx]\n",
    "            if sample_weight is not None:\n",
    "                self.sample_weight_train = sample_weight.loc[common_idx]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š è¨“ç·´æ•¸æ“šçµ±è¨ˆ:\")\n",
    "        print(f\"   ç‰¹å¾µæ•¸é‡: {len(self.X_train.columns):,}\")\n",
    "        print(f\"   æ¨£æœ¬æ•¸é‡: {len(self.X_train):,}\")\n",
    "        print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {self.y_train.value_counts().to_dict()}\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # æ­¥é©Ÿ8ï¼šäº¤å‰é©—è­‰ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        # =====================================================\n",
    "        if self.use_cv:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ8ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # å‰µå»ºåŸºæº–æ¨¡å‹é€²è¡Œäº¤å‰é©—è­‰\n",
    "            baseline_pipe = self._create_pipeline(**model_kwargs)\n",
    "            \n",
    "            # å‰µå»º PurgedKFold\n",
    "            cv = PurgedKFold(\n",
    "                n_splits=self.cv_splits,\n",
    "                t1=self.t1_train,\n",
    "                pctEmbargo=self.pct_embargo\n",
    "            )\n",
    "            \n",
    "            # ç¢ºå®šè©•åˆ†æ–¹æ³•\n",
    "            scoring = 'accuracy'\n",
    "            \n",
    "            print(f\"äº¤å‰é©—è­‰é…ç½®:\")\n",
    "            print(f\"  K æŠ˜æ•¸: {self.cv_splits}\")\n",
    "            print(f\"  ç¦åˆ¶æ¯”ä¾‹: {self.pct_embargo:.2%}\")\n",
    "            print(f\"  è©•åˆ†æ–¹æ³•: {scoring}\")\n",
    "            \n",
    "            # åŸ·è¡Œäº¤å‰é©—è­‰\n",
    "            self.cv_scores = cvScore(\n",
    "                clf=baseline_pipe,\n",
    "                X=self.X_train,\n",
    "                y=self.y_train,\n",
    "                sample_weight=self.sample_weight_train,\n",
    "                scoring=scoring,\n",
    "                cvGen=cv\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nâœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\")\n",
    "            print(f\"   åˆ†æ•¸: {self.cv_scores}\")\n",
    "            print(f\"   å¹³å‡åˆ†æ•¸: {self.cv_scores.mean():.4f} Â± {self.cv_scores.std():.4f}\")\n",
    "            print(f\"   æœ€å°åˆ†æ•¸: {self.cv_scores.min():.4f}\")\n",
    "            print(f\"   æœ€å¤§åˆ†æ•¸: {self.cv_scores.max():.4f}\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # æ­¥é©Ÿ9ï¼šè¶…åƒæ•¸èª¿æ•´ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        # =====================================================\n",
    "        if self.use_hyperopt:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ”§ æ­¥é©Ÿ9ï¼šåŸ·è¡Œè¶…åƒæ•¸èª¿æ•´\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # å‰µå»º pipeline\n",
    "            pipe_clf = self._create_pipeline(**model_kwargs)\n",
    "            \n",
    "            # å–å¾—åƒæ•¸ç¶²æ ¼\n",
    "            use_randomized = (self.hyperopt_method == 'randomized')\n",
    "            param_grid = self._get_param_grid(use_randomized=use_randomized)\n",
    "            \n",
    "            print(f\"è¶…åƒæ•¸èª¿æ•´é…ç½®:\")\n",
    "            print(f\"  æ–¹æ³•: {'RandomizedSearchCV' if use_randomized else 'GridSearchCV'}\")\n",
    "            print(f\"  è¿­ä»£æ¬¡æ•¸: {self.n_iter if use_randomized else 'å…¨éƒ¨çµ„åˆ'}\")\n",
    "            print(f\"  åƒæ•¸æ•¸é‡: {len(param_grid)}\")\n",
    "            \n",
    "            # æº–å‚™ fit_params\n",
    "            fit_params = {\n",
    "                f'{self.model_type}__sample_weight': self.sample_weight_train.values \n",
    "                if self.sample_weight_train is not None else None\n",
    "            }\n",
    "            \n",
    "            print(f\"\\né–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            \n",
    "            # åŸ·è¡Œè¶…åƒæ•¸èª¿æ•´\n",
    "            best_pipeline = clfHyperFit(\n",
    "                feat=self.X_train,\n",
    "                lbl=self.y_train,\n",
    "                t1=self.t1_train,\n",
    "                pipe_clf=pipe_clf,\n",
    "                param_grid=param_grid,\n",
    "                cv=self.cv_splits,\n",
    "                bagging=self.bagging,\n",
    "                rndSearchIter=self.n_iter if use_randomized else 0,\n",
    "                n_jobs=self.n_jobs,\n",
    "                pctEmbargo=self.pct_embargo,\n",
    "                **fit_params\n",
    "            )\n",
    "            \n",
    "            print(f\"çµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            \n",
    "            # æå–æœ€ä½³æ¨¡å‹\n",
    "            if 'bag' in best_pipeline.named_steps:\n",
    "                self.model = best_pipeline.named_steps['bag']\n",
    "            else:\n",
    "                self.model = best_pipeline.named_steps[self.model_type]\n",
    "            \n",
    "            # æå– scaler\n",
    "            if 'bag' in best_pipeline.named_steps:\n",
    "                self.scaler = best_pipeline.named_steps['bag'].base_estimator.named_steps['scaler']\n",
    "            else:\n",
    "                self.scaler = best_pipeline.named_steps['scaler']\n",
    "            \n",
    "            # æå–æœ€ä½³åƒæ•¸ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "            if hasattr(best_pipeline, 'best_params_'):\n",
    "                self.best_params = best_pipeline.best_params_\n",
    "            elif hasattr(best_pipeline, 'named_steps'):\n",
    "                if self.model_type in best_pipeline.named_steps:\n",
    "                    inner_model = best_pipeline.named_steps[self.model_type]\n",
    "                    if hasattr(inner_model, 'best_params_'):\n",
    "                        self.best_params = inner_model.best_params_\n",
    "            \n",
    "            print(f\"\\nâœ… è¶…åƒæ•¸èª¿æ•´å®Œæˆ\")\n",
    "            if self.best_params:\n",
    "                print(f\"æœ€ä½³åƒæ•¸:\")\n",
    "                for key, value in list(self.best_params.items())[:5]:  # åªé¡¯ç¤ºå‰5å€‹\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                if len(self.best_params) > 5:\n",
    "                    print(f\"  ... (å…± {len(self.best_params)} å€‹åƒæ•¸)\")\n",
    "            \n",
    "            # è©•ä¼°æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå•Ÿç”¨äº¤å‰é©—è­‰ï¼‰\n",
    "            if self.use_cv:\n",
    "                print(f\"\\nè©•ä¼°æœ€ä½³æ¨¡å‹...\")\n",
    "                cv_best = PurgedKFold(\n",
    "                    n_splits=self.cv_splits,\n",
    "                    t1=self.t1_train,\n",
    "                    pctEmbargo=self.pct_embargo\n",
    "                )\n",
    "                \n",
    "                scoring = 'accuracy' \n",
    "                \n",
    "                self.best_cv_scores = cvScore(\n",
    "                    clf=self.model,\n",
    "                    X=self.X_train,\n",
    "                    y=self.y_train,\n",
    "                    sample_weight=self.sample_weight_train,\n",
    "                    scoring=scoring,\n",
    "                    cvGen=cv_best\n",
    "                )\n",
    "                \n",
    "                print(f\"æœ€ä½³æ¨¡å‹äº¤å‰é©—è­‰åˆ†æ•¸:\")\n",
    "                print(f\"   åˆ†æ•¸: {self.best_cv_scores}\")\n",
    "                print(f\"   å¹³å‡åˆ†æ•¸: {self.best_cv_scores.mean():.4f} Â± {self.best_cv_scores.std():.4f}\")\n",
    "                \n",
    "                if self.cv_scores is not None:\n",
    "                    improvement = self.best_cv_scores.mean() - self.cv_scores.mean()\n",
    "                    print(f\"\\nèˆ‡åŸºæº–æ¨¡å‹æ¯”è¼ƒ:\")\n",
    "                    print(f\"   åŸºæº–æ¨¡å‹: {self.cv_scores.mean():.4f}\")\n",
    "                    print(f\"   æœ€ä½³æ¨¡å‹: {self.best_cv_scores.mean():.4f}\")\n",
    "                    print(f\"   æ”¹é€²: {improvement:+.4f} ({improvement/self.cv_scores.mean()*100:+.2f}%)\")\n",
    "        \n",
    "        else:\n",
    "            # ä¸ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´ï¼Œç›´æ¥è¨“ç·´\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ7ï¼šç›´æ¥è¨“ç·´æ¨¡å‹ï¼ˆä¸ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´ï¼‰\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            pipe_clf = self._create_pipeline(**model_kwargs)\n",
    "            \n",
    "            # æº–å‚™ fit_params\n",
    "            fit_params = {\n",
    "                f'{self.model_type}__sample_weight': self.sample_weight_train.values \n",
    "                if self.sample_weight_train is not None else None\n",
    "            }\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            self.model = pipe_clf.named_steps[self.model_type]\n",
    "            self.scaler = pipe_clf.named_steps['scaler']\n",
    "            \n",
    "            # æ¨™æº–åŒ–ç‰¹å¾µ\n",
    "            X_scaled = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                index=self.X_train.index,\n",
    "                columns=self.X_train.columns\n",
    "            )\n",
    "            \n",
    "            # è¨“ç·´åˆ†é¡å™¨\n",
    "            self.model.fit(\n",
    "                X_scaled,\n",
    "                self.y_train,\n",
    "                sample_weight=self.sample_weight_train.values if self.sample_weight_train is not None else None\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"âœ… PredictionPipeline è¨“ç·´å®Œæˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œé æ¸¬\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£ï¼ˆå¿…é ˆèˆ‡è¨“ç·´æ™‚çš„ç‰¹å¾µé †åºä¸€è‡´ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : np.ndarray\n",
    "            é¡åˆ¥é æ¸¬\n",
    "        probabilities : np.ndarray\n",
    "            æ¦‚ç‡é æ¸¬\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆåŸ·è¡Œ fit() è¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ”® åŸ·è¡Œæ¨¡å‹é æ¸¬\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # ç¢ºä¿ç‰¹å¾µé †åºä¸€è‡´\n",
    "        if self.feature_columns is not None:\n",
    "            missing_cols = [col for col in self.feature_columns if col not in X.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"âš ï¸ è­¦å‘Šï¼šç¼ºå°‘ {len(missing_cols)} å€‹ç‰¹å¾µï¼Œå°‡å¡«å……ç‚º 0\")\n",
    "                for col in missing_cols:\n",
    "                    X[col] = 0\n",
    "            X = X[self.feature_columns]\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.transform(X),\n",
    "            index=X.index,\n",
    "            columns=X.columns\n",
    "        )\n",
    "        \n",
    "        # é æ¸¬\n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        probabilities = self.model.predict_proba(X_scaled)\n",
    "        \n",
    "        print(f\"âœ… é æ¸¬å®Œæˆ\")\n",
    "        print(f\"   é æ¸¬æ¨£æœ¬æ•¸: {len(predictions):,}\")\n",
    "        print(f\"   é æ¸¬é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "        unique, counts = np.unique(predictions, return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            print(f\"     é¡åˆ¥ {u}: {c:,} ({c/len(predictions):.2%})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def evaluate(self,\n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight: Optional[pd.Series] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        è©•ä¼°æ¨¡å‹æ€§èƒ½\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "        y : pd.Series\n",
    "            çœŸå¯¦æ¨™ç±¤\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            è©•ä¼°æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import (accuracy_score, f1_score, \n",
    "                                   precision_score, recall_score,\n",
    "                                   roc_auc_score)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆåŸ·è¡Œ fit() è¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # é æ¸¬\n",
    "        predictions, probabilities = self.predict(X)\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ¨™\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, predictions, sample_weight=sample_weight),\n",
    "            'precision': precision_score(y, predictions, average='weighted', \n",
    "                                        sample_weight=sample_weight, zero_division=0),\n",
    "            'recall': recall_score(y, predictions, average='weighted', \n",
    "                                  sample_weight=sample_weight, zero_division=0),\n",
    "            'f1': f1_score(y, predictions, average='weighted', \n",
    "                          sample_weight=sample_weight, zero_division=0),\n",
    "        }\n",
    "        \n",
    "        # å¦‚æœæ˜¯äºŒåˆ†é¡ï¼Œè¨ˆç®—æ›´å¤šæŒ‡æ¨™\n",
    "        if len(np.unique(y)) == 2:\n",
    "            metrics['f1_binary'] = f1_score(y, predictions, sample_weight=sample_weight, zero_division=0)\n",
    "            metrics['roc_auc'] = roc_auc_score(y, probabilities[:, 1], sample_weight=sample_weight)\n",
    "        \n",
    "        # æ‰“å°çµæœ\n",
    "        print(f\"\\nè©•ä¼°æŒ‡æ¨™:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # æ··æ·†çŸ©é™£\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y, predictions, sample_weight=sample_weight)\n",
    "        print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self,\n",
    "                    X: pd.DataFrame = None,\n",
    "                    y: pd.Series = None,\n",
    "                    sample_weight: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½çµæœï¼ˆäº¤å‰é©—è­‰åˆ†æ•¸ã€é æ¸¬çµæœç­‰ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame, optional\n",
    "            ç‰¹å¾µçŸ©é™£ï¼ˆç”¨æ–¼é æ¸¬å’Œè©•ä¼°ï¼‰\n",
    "        y : pd.Series, optional\n",
    "            çœŸå¯¦æ¨™ç±¤ï¼ˆç”¨æ–¼è©•ä¼°ï¼‰\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. äº¤å‰é©—è­‰åˆ†æ•¸å°æ¯”\n",
    "        if self.cv_scores is not None and self.best_cv_scores is not None:\n",
    "            axes[0, 0].bar(range(len(self.cv_scores)), self.cv_scores, alpha=0.6, \n",
    "                          label='Baseline Model', color='blue', width=0.4)\n",
    "            axes[0, 0].bar([i + 0.4 for i in range(len(self.best_cv_scores))], \n",
    "                          self.best_cv_scores, alpha=0.6, \n",
    "                          label='Best Model (After Tuning)', color='red', width=0.4)\n",
    "            axes[0, 0].axhline(self.cv_scores.mean(), color='blue', linestyle='--', \n",
    "                             label=f'Baseline Mean: {self.cv_scores.mean():.4f}', linewidth=1.5)\n",
    "            axes[0, 0].axhline(self.best_cv_scores.mean(), color='red', linestyle='--', \n",
    "                             label=f'Best Mean: {self.best_cv_scores.mean():.4f}', linewidth=1.5)\n",
    "            axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "            axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores: Baseline vs Best Model', fontsize=14)\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_xticks(range(len(self.cv_scores)))\n",
    "            axes[0, 0].set_xticklabels([f'Fold {i+1}' for i in range(len(self.cv_scores))])\n",
    "        elif self.cv_scores is not None:\n",
    "            axes[0, 0].bar(range(len(self.cv_scores)), self.cv_scores, alpha=0.7, color='blue')\n",
    "            axes[0, 0].axhline(self.cv_scores.mean(), color='red', linestyle='--', \n",
    "                             label=f'Mean: {self.cv_scores.mean():.4f}', linewidth=2)\n",
    "            axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "            axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores', fontsize=14)\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, 'No CV scores available', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores', fontsize=14)\n",
    "        \n",
    "        # 2. åˆ†æ•¸åˆ†å¸ƒå°æ¯”\n",
    "        if self.cv_scores is not None and self.best_cv_scores is not None:\n",
    "            axes[0, 1].boxplot([self.cv_scores, self.best_cv_scores], \n",
    "                             labels=['Baseline', 'Best Model'])\n",
    "            axes[0, 1].scatter([1] * len(self.cv_scores), self.cv_scores, \n",
    "                             alpha=0.3, s=30, color='blue')\n",
    "            axes[0, 1].scatter([2] * len(self.best_cv_scores), self.best_cv_scores, \n",
    "                             alpha=0.3, s=30, color='red')\n",
    "            axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution Comparison', fontsize=14)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        elif self.cv_scores is not None:\n",
    "            axes[0, 1].boxplot([self.cv_scores], labels=['Baseline'])\n",
    "            axes[0, 1].scatter([1] * len(self.cv_scores), self.cv_scores, \n",
    "                             alpha=0.3, s=30, color='blue')\n",
    "            axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution', fontsize=14)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No CV scores available', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution', fontsize=14)\n",
    "        \n",
    "        # 3. é æ¸¬çµæœï¼ˆå¦‚æœæœ‰ X å’Œ yï¼‰\n",
    "        if X is not None and y is not None:\n",
    "            predictions, probabilities = self.predict(X)\n",
    "            \n",
    "            # æ··æ·†çŸ©é™£\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(y, predictions, sample_weight=sample_weight)\n",
    "            sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 0])  # ä½¿ç”¨æµ®é»æ•¸æ ¼å¼\n",
    "            axes[1, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "            axes[1, 0].set_ylabel('True Label', fontsize=12)\n",
    "            axes[1, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "            \n",
    "            # é æ¸¬åˆ†å¸ƒ\n",
    "            pred_counts = pd.Series(predictions).value_counts().sort_index()\n",
    "            axes[1, 1].bar(pred_counts.index, pred_counts.values, alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('Predicted Label', fontsize=12)\n",
    "            axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "            axes[1, 1].set_title('Prediction Distribution', fontsize=14)\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No prediction data\\n(provide X and y)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "            axes[1, 1].text(0.5, 0.5, 'No prediction data\\n(provide X and y)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 1].set_title('Prediction Distribution', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ10ï¼šåŸ·è¡Œé æ¸¬ - ä¿®æ­£ç‰ˆï¼ˆä¿®æ­£ç´¢å¼•éŒ¯èª¤ï¼‰\n",
    "# =====================================================\n",
    "\n",
    "# ç¬¬0æ­¥ï¼šæŒ‡å®šæ•¸æ“š\n",
    "y = bins['bin']\n",
    "t1 = events['t1']\n",
    "sample_weight = final_weights\n",
    "\n",
    "# æº–å‚™ç‰¹å¾µçŸ©é™£ X\n",
    "event_indices = events.index\n",
    "features_list = []\n",
    "\n",
    "if 'fracdiff_features_all' in globals():\n",
    "    fracdiff_selected = fracdiff_features_all.loc[event_indices]\n",
    "    selected_cols = [col for col in fracdiff_selected.columns if 'NAS100' in col or 'SPX500' in col or 'US2000' in col or 'XAUUSD' in col or 'UK100' in col or 'HKG33' in col or 'SPX500' in col] \n",
    "    if len(selected_cols) > 0:\n",
    "        features_list.append(fracdiff_selected[selected_cols])\n",
    "\n",
    "strategy_features = pd.DataFrame(index=event_indices)\n",
    "strategy_aligned = df_filtered.loc[event_indices]\n",
    "if 'HL115%' in strategy_aligned.columns:\n",
    "    strategy_features['HL115%'] = strategy_aligned['HL115%']\n",
    "if 'çªç ´signalåƒ¹%' in strategy_aligned.columns:\n",
    "    strategy_features['çªç ´signalåƒ¹%'] = strategy_aligned['çªç ´signalåƒ¹%']\n",
    "features_list.append(strategy_features)\n",
    "\n",
    "X = pd.concat(features_list, axis=1).fillna(method='ffill').fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# æ•¸æ“šå°é½Š\n",
    "common_idx = X.index.intersection(y.index).intersection(t1.index)\n",
    "X = X.loc[common_idx]\n",
    "y = y.loc[common_idx]\n",
    "t1 = t1.loc[common_idx]\n",
    "sample_weight = sample_weight.loc[common_idx] if sample_weight is not None else None\n",
    "\n",
    "# æ¨™ç±¤æ˜ å°„ï¼šå°‡ -1, 0, 1 æ˜ å°„ç‚º 0, 1, 2\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "reverse_mapping = {0: -1, 1: 0, 2: 1}\n",
    "y_mapped = y.map(label_mapping)\n",
    "\n",
    "# ç¬¬0.5æ­¥ï¼šæŒ‰æŒ‡å®šæ™‚é–“åˆ‡åˆ†è¨“ç·´/æ¸¬è©¦é›†\n",
    "train_start = '2015-11-01'\n",
    "test_start = '2021-01-01'\n",
    "\n",
    "X_train = X.loc[(X.index >= train_start) & (X.index < test_start)]\n",
    "X_test = X.loc[X.index >= test_start]\n",
    "y_train = y_mapped.loc[(y_mapped.index >= train_start) & (y_mapped.index < test_start)]\n",
    "y_test = y_mapped.loc[y_mapped.index >= test_start]  # ä¿®æ­£ï¼šä½¿ç”¨ y_mapped.index\n",
    "t1_train = t1.loc[(t1.index >= train_start) & (t1.index < test_start)]\n",
    "t1_test = t1.loc[t1.index >= test_start]\n",
    "sample_weight_train = sample_weight.loc[(sample_weight.index >= train_start) & (sample_weight.index < test_start)] if sample_weight is not None else None\n",
    "sample_weight_test = sample_weight.loc[sample_weight.index >= test_start] if sample_weight is not None else None\n",
    "\n",
    "# ç¬¬1æ­¥ï¼šåˆå§‹åŒ– Pipeline\n",
    "pipeline = PredictionPipeline(\n",
    "    model_type='rf',\n",
    "    use_hyperopt=True,\n",
    "    use_cv=True,\n",
    "    cv_splits=5,\n",
    "    pct_embargo=0.01,\n",
    "    hyperopt_method='grid',\n",
    "    n_iter=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ç¬¬2æ­¥ï¼šè¨“ç·´æ¨¡å‹\n",
    "pipeline.fit(X_train, y_train, t1_train, sample_weight_train)\n",
    "\n",
    "# ç¬¬3æ­¥ï¼šå¯è¦–åŒ–\n",
    "pipeline.plot_results(X=X_test, y=y_test, sample_weight=sample_weight_test)\n",
    "\n",
    "# ç¬¬4æ­¥ï¼šé æ¸¬\n",
    "predictions_test_mapped, probabilities_test = pipeline.predict(X_test)\n",
    "predictions_test = pd.Series(predictions_test_mapped).map(reverse_mapping).values\n",
    "\n",
    "# å¯è¦–åŒ–æ¦‚ç‡åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].hist(probabilities_test[:, 0], bins=50, alpha=0.7, label='Class 0 (åŸ-1)', color='red')\n",
    "axes[0, 0].hist(probabilities_test[:, 1], bins=50, alpha=0.7, label='Class 1 (åŸ0)', color='blue')\n",
    "axes[0, 0].hist(probabilities_test[:, 2], bins=50, alpha=0.7, label='Class 2 (åŸ1)', color='green')\n",
    "axes[0, 0].set_xlabel('Probability', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Probability Distribution by Class', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "mean_probs = probabilities_test.mean(axis=0)\n",
    "axes[0, 1].bar(range(3), mean_probs, alpha=0.7, color=['red', 'blue', 'green'])\n",
    "axes[0, 1].set_xticks(range(3))\n",
    "axes[0, 1].set_xticklabels(['Class 0\\n(åŸ-1)', 'Class 1\\n(åŸ0)', 'Class 2\\n(åŸ1)'])\n",
    "axes[0, 1].set_ylabel('Mean Probability', fontsize=12)\n",
    "axes[0, 1].set_title('Mean Probability by Class', fontsize=14)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "pred_counts = pd.Series(predictions_test_mapped).value_counts().sort_index()\n",
    "axes[1, 0].bar(pred_counts.index, pred_counts.values, alpha=0.7, color=['red', 'blue', 'green'])\n",
    "axes[1, 0].set_xlabel('Predicted Class', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Count', fontsize=12)\n",
    "axes[1, 0].set_title('Predicted Class Distribution', fontsize=14)\n",
    "axes[1, 0].set_xticks(range(3))\n",
    "axes[1, 0].set_xticklabels(['Class 0\\n(åŸ-1)', 'Class 1\\n(åŸ0)', 'Class 2\\n(åŸ1)'])\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "true_counts = y_test.value_counts().sort_index()\n",
    "axes[1, 1].bar(true_counts.index, true_counts.values, alpha=0.7, color=['red', 'blue', 'green'])\n",
    "axes[1, 1].set_xlabel('True Class', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[1, 1].set_title('True Class Distribution (Test Set)', fontsize=14)\n",
    "axes[1, 1].set_xticks(range(3))\n",
    "axes[1, 1].set_xticklabels(['Class 0\\n(åŸ-1)', 'Class 1\\n(åŸ0)', 'Class 2\\n(åŸ1)'])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ‰“å°æ¦‚ç‡çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š é æ¸¬æ¦‚ç‡çµ±è¨ˆ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Class 0 (åŸ-1) æ¦‚ç‡: å¹³å‡={probabilities_test[:, 0].mean():.4f}, ä¸­ä½æ•¸={np.median(probabilities_test[:, 0]):.4f}\")\n",
    "print(f\"Class 1 (åŸ0) æ¦‚ç‡: å¹³å‡={probabilities_test[:, 1].mean():.4f}, ä¸­ä½æ•¸={np.median(probabilities_test[:, 1]):.4f}\")\n",
    "print(f\"Class 2 (åŸ1) æ¦‚ç‡: å¹³å‡={probabilities_test[:, 2].mean():.4f}, ä¸­ä½æ•¸={np.median(probabilities_test[:, 2]):.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç¬¬5æ­¥ï¼šè©•ä¼°\n",
    "y_test_original = y.loc[y_test.index]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "metrics_test = {\n",
    "    'accuracy': accuracy_score(y_test_original, predictions_test, sample_weight=sample_weight_test),\n",
    "    'precision': precision_score(y_test_original, predictions_test, average='weighted', \n",
    "                                 sample_weight=sample_weight_test, zero_division=0),\n",
    "    'recall': recall_score(y_test_original, predictions_test, average='weighted', \n",
    "                          sample_weight=sample_weight_test, zero_division=0),\n",
    "    'f1': f1_score(y_test_original, predictions_test, average='weighted', \n",
    "                  sample_weight=sample_weight_test, zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š æ¸¬è©¦é›†è©•ä¼°çµæœ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"è©•ä¼°æ¨£æœ¬æ•¸: {len(y_test_original):,}\")\n",
    "for key, value in metrics_test.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test_original, predictions_test, sample_weight=sample_weight_test)\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç¬¬6æ­¥ï¼šå‰µå»ºé æ¸¬çµæœ DataFrame\n",
    "pred_df = pd.DataFrame(index=X_test.index)\n",
    "pred_df['t1'] = t1_test\n",
    "pred_df['side'] = events.loc[X_test.index, 'side']\n",
    "pred_df['true_label'] = y_test_original\n",
    "pred_df['predicted_label'] = predictions_test\n",
    "pred_df['prob_class_0'] = probabilities_test[:, 0]\n",
    "pred_df['prob_class_1'] = probabilities_test[:, 1]\n",
    "pred_df['prob_class_2'] = probabilities_test[:, 2]\n",
    "pred_df['prediction_confidence'] = np.max(probabilities_test, axis=1)\n",
    "pred_df['is_correct'] = (pred_df['predicted_label'] == pred_df['true_label'])\n",
    "pred_df['trade_signal'] = np.where(\n",
    "    (pred_df['predicted_label'] == 1) & (pred_df['side'] == 1), 1,\n",
    "    np.where((pred_df['predicted_label'] == 1) & (pred_df['side'] == -1), -1, 0)\n",
    ")\n",
    "\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73fdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392d205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ee390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
