{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca34831",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ0ï¼šè³‡æ–™é è™•ç†èˆ‡é‡æ¡æ¨£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c95a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2014-11-01'\n",
    "test_start = '2023-01-01'\n",
    "test_end = '2026-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®€å–NAS100è³‡æ–™\n",
    "def load_nas100_data(file_path):\n",
    "    \"\"\"è¼‰å…¥ä¸¦é è™•ç†NAS100è³‡æ–™\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # è½‰æ›æ™‚é–“æ ¼å¼\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # ä½¿ç”¨BidCloseä½œç‚ºä¸»è¦åƒ¹æ ¼\n",
    "    df['Close'] = df['BidClose']\n",
    "    df['High'] = df['BidHigh'] \n",
    "    df['Low'] = df['BidLow']\n",
    "    df['Open'] = df['BidOpen']\n",
    "    \n",
    "    return df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "# è³‡æ–™é‡æ¡æ¨£\n",
    "def resample_data(df, interval='1H'):\n",
    "    \"\"\"å°‡15åˆ†é˜è³‡æ–™é‡æ¡æ¨£ç‚ºæŒ‡å®šé–“éš”\"\"\"\n",
    "    resampled = df.resample(interval).agg({\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min', \n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum'\n",
    "    }).dropna()\n",
    "    \n",
    "    return resampled\n",
    "\n",
    "# åŸ·è¡Œè³‡æ–™è¼‰å…¥å’Œé‡æ¡æ¨£\n",
    "nas100_raw = load_nas100_data('equity_prices/NAS100.csv')\n",
    "\n",
    "# # åƒ…ä¿ç•™ 2015-11-01 ä¹‹å¾Œ\n",
    "nas100_raw = nas100_raw.loc[nas100_raw.index >= train_start]\n",
    "nas100_raw = nas100_raw.loc[nas100_raw.index <= test_end]\n",
    "nas100_raw\n",
    "\n",
    "# nas100_hourly = resample_data(nas100_raw, '1H')\n",
    "\n",
    "# print(f\"åŸå§‹è³‡æ–™ç­†æ•¸: {len(nas100_raw)}\")\n",
    "# print(f\"é‡æ¡æ¨£å¾Œç­†æ•¸: {len(nas100_hourly)}\")\n",
    "# print(f\"è³‡æ–™æœŸé–“: {nas100_hourly.index[0]} è‡³ {nas100_hourly.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1932e",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ1ï¼šè³‡æ–™é›œè¨Šéæ¿¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9270ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1ï¸âƒ£ é›œè¨Šéæ¿¾ Class\n",
    "# =====================================================\n",
    "class NoiseFilter:\n",
    "    \"\"\"\n",
    "    é›œè¨Šéæ¿¾å™¨ï¼šè¨ˆç®—æ³¢å‹•ç‡ã€CUSUM éæ¿¾ç­‰\n",
    "    \n",
    "    CUSUM Filter èªªæ˜ï¼š\n",
    "    ==================\n",
    "    CUSUM (Cumulative Sum) Filter æ˜¯ä¸€ç¨®å“è³ªæ§åˆ¶æ–¹æ³•ï¼Œç”¨æ–¼æª¢æ¸¬æ¸¬é‡å€¼\n",
    "    ç›¸å°æ–¼ç›®æ¨™å€¼çš„å‡å€¼åç§»ã€‚\n",
    "    \n",
    "    ç”¨é€”ï¼š\n",
    "    -----\n",
    "    1. éæ¿¾å¸‚å ´é›œè¨Šï¼Œåªä¿ç•™é¡¯è‘—çš„åƒ¹æ ¼è®Šå‹•äº‹ä»¶\n",
    "    2. é¿å…åœ¨åƒ¹æ ¼åœ¨é–¾å€¼é™„è¿‘å¾˜å¾Šæ™‚è§¸ç™¼å¤šå€‹äº‹ä»¶ï¼ˆé€™æ˜¯ Bollinger Bands ç­‰\n",
    "       å¸¸è¦‹å¸‚å ´ä¿¡è™Ÿçš„ç¼ºé»ï¼‰\n",
    "    3. éœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦ h æ‰æœƒè§¸ç™¼äº‹ä»¶ï¼Œä½¿äº‹ä»¶æ›´å…·æ„ç¾©\n",
    "    \n",
    "    æ•¸å­¸åŸç†ï¼š\n",
    "    ---------\n",
    "    å°æ–¼è§€æ¸¬å€¼ {y_t}_{t=1,...,T}ï¼Œå®šç¾©ç´¯ç©å’Œï¼š\n",
    "    \n",
    "    S^+_t = max{0, S^+_{t-1} + y_t - E_{t-1}[y_t]},  S^+_0 = 0\n",
    "    S^-_t = min{0, S^-_{t-1} + y_t - E_{t-1}[y_t]},  S^-_0 = 0\n",
    "    S_t = max{S^+_t, -S^-_t}\n",
    "    \n",
    "    å…¶ä¸­ E_{t-1}[y_t] = y_{t-1}ï¼ˆä½¿ç”¨å‰ä¸€æœŸå€¼ä½œç‚ºæœŸæœ›å€¼ï¼‰\n",
    "    \n",
    "    ç•¶ S_t â‰¥ hï¼ˆé–¾å€¼ï¼‰æ™‚ï¼Œè§¸ç™¼äº‹ä»¶ä¸¦é‡ç½® S_t = 0ã€‚\n",
    "    \n",
    "    å°ç¨± CUSUM Filter çš„å„ªå‹¢ï¼š\n",
    "    -------------------------\n",
    "    - åŒæ™‚æ•æ‰ä¸Šæ¼²å’Œä¸‹è·Œçš„ç´¯ç©åå·®\n",
    "    - é›¶åº•ç·šæ©Ÿåˆ¶ï¼šç•¶ y_t â‰¤ E_{t-1}[y_t] - S_{t-1} æ™‚ï¼ŒS_t = 0\n",
    "    - é¿å…åœ¨é–¾å€¼é™„è¿‘åè¦†è§¸ç™¼äº‹ä»¶\n",
    "    - éœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦æ‰æœƒè§¸ç™¼ï¼Œä½¿äº‹ä»¶æ›´å…·çµ±è¨ˆæ„ç¾©\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Lam and Yam (1997): æå‡ºäº¤æ›¿è²·è³£ä¿¡è™Ÿçš„æŠ•è³‡ç­–ç•¥\n",
    "    - Fama and Blume (1966): Filter trading strategy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vol_span: int = 100):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        vol_span : int\n",
    "            æ—¥æ³¢å‹•ç‡è¨ˆç®—çš„ EWM span\n",
    "        \"\"\"\n",
    "        self.vol_span = vol_span\n",
    "        self.daily_vol = None\n",
    "        self.total_bars = None\n",
    "        self.filtered_bars = None\n",
    "        \n",
    "    def calculate_daily_vol(self, close: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ—¥æ³¢å‹•ç‡\n",
    "        \n",
    "        ä½¿ç”¨æŒ‡æ•¸åŠ æ¬Šç§»å‹•æ¨™æº–å·® (EWM) è¨ˆç®—æ³¢å‹•ç‡ï¼Œå°è¿‘æœŸæ•¸æ“šçµ¦äºˆæ›´é«˜æ¬Šé‡ã€‚\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        daily_vol : pd.Series\n",
    "            æ—¥æ³¢å‹•ç‡åºåˆ—\n",
    "        \"\"\"\n",
    "        # è¨˜éŒ„ç¸½ bar æ•¸\n",
    "        self.total_bars = len(close)\n",
    "        \n",
    "        # è¨ˆç®—æ—¥æ”¶ç›Šç‡\n",
    "        df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))\n",
    "        df0 = df0[df0 > 0]\n",
    "        df0 = pd.Series(close.index[df0 - 1], \n",
    "                        index=close.index[close.shape[0] - df0.shape[0]:])\n",
    "        df0 = close.loc[df0.index] / close.loc[df0.values].values - 1\n",
    "        \n",
    "        # ä½¿ç”¨ EWM è¨ˆç®—æ¨™æº–å·®ï¼ˆæ³¢å‹•ç‡ï¼‰\n",
    "        df0 = df0.ewm(span=self.vol_span).std()\n",
    "        \n",
    "        self.daily_vol = df0\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_vol_count = df0.notna().sum()\n",
    "        vol_stats = self.get_vol_stats()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ³¢å‹•ç‡è¨ˆç®—çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½ K ç·šæ•¸ (Total Bars): {self.total_bars:,}\")\n",
    "        print(f\"æœ‰æ•ˆæ³¢å‹•ç‡æ•¸æ“šæ•¸: {valid_vol_count:,}\")\n",
    "        print(f\"æ³¢å‹•ç‡è¨ˆç®—æ™‚é–“çª—å£ (vol_span): {self.vol_span} æœŸ\")\n",
    "        print(f\"  (è¨»: EWM ä½¿ç”¨æŒ‡æ•¸åŠ æ¬Šï¼Œä¸æ˜¯å›ºå®šçª—å£)\")\n",
    "        print(f\"\\næ³¢å‹•ç‡çµ±è¨ˆ:\")\n",
    "        if vol_stats:\n",
    "            print(f\"  å¹³å‡å€¼ (Mean): {vol_stats['mean']:.6f}\")\n",
    "            print(f\"  æ¨™æº–å·® (Std): {vol_stats['std']:.6f}\")\n",
    "            print(f\"  æœ€å°å€¼ (Min): {vol_stats['min']:.6f}\")\n",
    "            print(f\"  æœ€å¤§å€¼ (Max): {vol_stats['max']:.6f}\")\n",
    "            print(f\"  ä¸­ä½æ•¸ (Median): {vol_stats['median']:.6f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return df0\n",
    "    \n",
    "    def cusum_filter(self, close: pd.Series, threshold: float) -> pd.DatetimeIndex:\n",
    "        \"\"\"\n",
    "        å°ç¨± CUSUM éæ¿¾å™¨ (Symmetric CUSUM Filter)\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°åƒ¹æ ¼åºåˆ—çš„å·®åˆ†ï¼ˆdiff = y_t - y_{t-1}ï¼‰æ‡‰ç”¨å°ç¨± CUSUM éæ¿¾ï¼š\n",
    "        \n",
    "        S^+_t = max{0, S^+_{t-1} + diff_t}  (ä¸Šæ¼²ç´¯ç©)\n",
    "        S^-_t = min{0, S^-_{t-1} + diff_t}  (ä¸‹è·Œç´¯ç©)\n",
    "        \n",
    "        ç•¶ S^-_t < -h æˆ– S^+_t > h æ™‚ï¼š\n",
    "        - è§¸ç™¼äº‹ä»¶ï¼ˆè¨˜éŒ„æ™‚é–“é»ï¼‰\n",
    "        - é‡ç½®å°æ‡‰çš„ç´¯ç©å’Œç‚º 0\n",
    "        \n",
    "        é€™æ¨£è¨­è¨ˆçš„å„ªå‹¢ï¼š\n",
    "        --------------\n",
    "        1. é¿å…åœ¨é–¾å€¼é™„è¿‘åè¦†è§¸ç™¼ï¼ˆéœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦ hï¼‰\n",
    "        2. åŒæ™‚æ•æ‰ä¸Šæ¼²å’Œä¸‹è·Œçš„ç´¯ç©åå·®\n",
    "        3. é›¶åº•ç·šæ©Ÿåˆ¶ç¢ºä¿åªæ•æ‰å–®æ–¹å‘çš„æŒçºŒåå·®\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        threshold : float\n",
    "            CUSUM é–¾å€¼ (h)\n",
    "            é€šå¸¸ä½¿ç”¨æ³¢å‹•ç‡çš„å¹³å‡å€¼æˆ–æ¨™æº–å·®\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            éæ¿¾å¾Œçš„äº‹ä»¶æ™‚é–“é»\n",
    "            \n",
    "        åƒè€ƒå¯¦ç¾ï¼š\n",
    "        ---------\n",
    "        åŸºæ–¼ Snippet 2.4 (Advances in Financial Machine Learning, Chapter 2)\n",
    "        \"\"\"\n",
    "        # è¨˜éŒ„ç¸½ bar æ•¸\n",
    "        self.total_bars = len(close)\n",
    "        \n",
    "        tEvents = []\n",
    "        sPos, sNeg = 0, 0  # S^+_0 = 0, S^-_0 = 0\n",
    "        diff = close.diff()  # y_t - y_{t-1}\n",
    "        \n",
    "        # éæ­·æ¯å€‹æ™‚é–“é»ï¼ˆå¾ç¬¬äºŒå€‹é–‹å§‹ï¼Œå› ç‚ºç¬¬ä¸€å€‹ diff æ˜¯ NaNï¼‰\n",
    "        for i in diff.index[1:]:\n",
    "            # æ›´æ–°ç´¯ç©å’Œ\n",
    "            # S^+_t = max{0, S^+_{t-1} + diff_t}\n",
    "            sPos = max(0, sPos + diff.loc[i])\n",
    "            # S^-_t = min{0, S^-_{t-1} + diff_t}\n",
    "            sNeg = min(0, sNeg + diff.loc[i])\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦è§¸ç™¼äº‹ä»¶\n",
    "            if sNeg < -threshold:\n",
    "                # ä¸‹è·Œç´¯ç©è¶…éé–¾å€¼ï¼šè§¸ç™¼äº‹ä»¶ä¸¦é‡ç½®\n",
    "                sNeg = 0\n",
    "                tEvents.append(i)\n",
    "            elif sPos > threshold:\n",
    "                # ä¸Šæ¼²ç´¯ç©è¶…éé–¾å€¼ï¼šè§¸ç™¼äº‹ä»¶ä¸¦é‡ç½®\n",
    "                sPos = 0\n",
    "                tEvents.append(i)\n",
    "        \n",
    "        filtered_events = pd.DatetimeIndex(tEvents)\n",
    "        self.filtered_bars = len(filtered_events)\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        filter_rate = self.filtered_bars / self.total_bars if self.total_bars > 0 else 0\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š CUSUM éæ¿¾çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½ K ç·šæ•¸ (Total Bars): {self.total_bars:,}\")\n",
    "        print(f\"ç¯©é¸å¾Œ K ç·šæ•¸ (Filtered Bars): {self.filtered_bars:,}\")\n",
    "        print(f\"éæ¿¾æ¯”ä¾‹: {filter_rate:.4%}\")\n",
    "        print(f\"CUSUM é–¾å€¼ (Threshold h): {threshold:.6f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - åªä¿ç•™åƒ¹æ ¼ç´¯ç©è®Šå‹•è¶…éé–¾å€¼ {threshold:.6f} çš„æ™‚é–“é»\")\n",
    "        print(f\"  - é¿å…åœ¨é–¾å€¼é™„è¿‘åè¦†è§¸ç™¼äº‹ä»¶\")\n",
    "        print(f\"  - éœ€è¦å®Œæ•´çš„é‹è¡Œé•·åº¦æ‰æœƒè§¸ç™¼ï¼Œä½¿äº‹ä»¶æ›´å…·çµ±è¨ˆæ„ç¾©\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return filtered_events\n",
    "    \n",
    "    def get_vol_stats(self) -> dict:\n",
    "        \"\"\"å–å¾—æ³¢å‹•ç‡çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if self.daily_vol is None:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'mean': self.daily_vol.mean(),\n",
    "            'std': self.daily_vol.std(),\n",
    "            'min': self.daily_vol.min(),\n",
    "            'max': self.daily_vol.max(),\n",
    "            'median': self.daily_vol.median()\n",
    "        }\n",
    "    \n",
    "    def plot_volatility(self):\n",
    "        \"\"\"ç¹ªè£½æ³¢å‹•ç‡åœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.daily_vol is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_daily_vol()\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        self.daily_vol.plot(ax=ax, title='Daily Volatility', linewidth=1)\n",
    "        ax.axhline(self.daily_vol.mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {self.daily_vol.mean():.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. é›œè¨Šéæ¿¾\n",
    "noise_filter = NoiseFilter(vol_span=500)\n",
    "daily_vol = noise_filter.calculate_daily_vol(nas100_raw['Close'])\n",
    "\n",
    "# 2. CUSUM ç¯©é¸æ•¸æ“š\n",
    "# ä½¿ç”¨æ³¢å‹•ç‡å¹³å‡å€¼ä½œç‚ºé–¾å€¼\n",
    "# cusum_threshold = daily_vol.mean()\n",
    "cusum_threshold = 0.001\n",
    "cusum_events = noise_filter.cusum_filter(nas100_raw['Close'], cusum_threshold)\n",
    "df_filtered = nas100_raw.loc[cusum_events].copy()  # âœ… éæ¿¾å¾Œçš„æ•¸æ“š\n",
    "print(f\"éæ¿¾å¾Œçš„è³‡æ–™ç­†æ•¸: {len(df_filtered)}\")\n",
    "noise_filter.plot_volatility()\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a23019",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ2 ï¼šç­–ç•¥è¨Šè™Ÿè¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae722d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2ï¸âƒ£ L1-OLD ç­–ç•¥ Class\n",
    "# =====================================================\n",
    "class L1Strategy:\n",
    "    \"\"\"L1-OLD çªç ´ç­–ç•¥\"\"\"\n",
    "    \n",
    "    def __init__(self, entry_param: float = 0.5, rolling_window: int = 460):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        entry_param : float\n",
    "            çªç ´åƒæ•¸ï¼ˆé è¨­ 0.5ï¼‰\n",
    "        rolling_window : int\n",
    "            è¨ˆç®— HL115 å’Œçªç ´åƒ¹çš„æ»¾å‹•çª—å£ï¼ˆé è¨­ 460ï¼‰\n",
    "        \"\"\"\n",
    "        self.entry_param = entry_param\n",
    "        self.rolling_window = rolling_window\n",
    "        self.signals = None\n",
    "        \n",
    "    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—ç­–ç•¥æŒ‡æ¨™\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            éœ€åŒ…å« 'High', 'Low', 'Open', 'Close' æ¬„ä½\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        df : pd.DataFrame\n",
    "            æ–°å¢ 'HL115', 'çªç ´signalåƒ¹', 'signal', 'side' æ¬„ä½\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # è¨ˆç®— HL115ï¼ˆéå»Næ ¹Kç·šçš„åƒ¹æ ¼ç¯„åœï¼‰\n",
    "        df['HL115'] = (\n",
    "            df['High'].shift(1).rolling(self.rolling_window, \n",
    "                                           min_periods=self.rolling_window).max() - \n",
    "            df['Low'].shift(1).rolling(self.rolling_window, \n",
    "                                         min_periods=self.rolling_window).min()\n",
    "        )\n",
    "        df['HL115%'] = df['HL115'] / df['Low'].shift(self.rolling_window+1)\n",
    "        \n",
    "        # è¨ˆç®—çªç ´signalåƒ¹\n",
    "        df['çªç ´signalåƒ¹'] = (\n",
    "            df['Low'].shift(1).rolling(self.rolling_window).min() + \n",
    "            (self.entry_param * df['HL115'])\n",
    "        )\n",
    "        df['çªç ´signalåƒ¹%'] = df['çªç ´signalåƒ¹']/df['Open']\n",
    "        \n",
    "        # ç”Ÿæˆçªç ´ä¿¡è™Ÿ\n",
    "        df['signal'] = False\n",
    "        df['signal'] = (\n",
    "            (df['signal'].shift(1) == False) & \n",
    "            (df['High'] > df['çªç ´signalåƒ¹']) & \n",
    "            (df['Open'] < df['çªç ´signalåƒ¹'])\n",
    "        )\n",
    "        \n",
    "        # è½‰æ›ç‚º sideï¼ˆåšå¤šæ–¹å‘ï¼‰\n",
    "        df['side'] = np.nan\n",
    "        df.loc[df['signal'] == True, 'side'] = 1.0\n",
    "        df['side'] = df['side'].ffill()\n",
    "        \n",
    "        self.signals = df\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        total_bars = len(df)\n",
    "        signal_count = df['signal'].sum()\n",
    "        signal_rate = signal_count / total_bars if total_bars > 0 else 0\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š L1-OLD ç­–ç•¥çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½ K ç·šæ•¸ (Bars): {total_bars:,}\")\n",
    "        print(f\"ä¿¡è™Ÿæ•¸é‡ (Signals): {signal_count:,}\")\n",
    "        print(f\"ä¿¡è™Ÿæ¯”ä¾‹: {signal_rate:.4%}\")\n",
    "        print(f\"ç­–ç•¥åƒæ•¸: entry_param={self.entry_param}, rolling_window={self.rolling_window}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_signal_events(self) -> pd.DatetimeIndex:\n",
    "        \"\"\"å–å¾—ä¿¡è™Ÿç™¼ç”Ÿçš„æ™‚é–“é»\"\"\"\n",
    "        if self.signals is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_indicators()\")\n",
    "            return None\n",
    "        \n",
    "        return self.signals[self.signals['signal'] == True].index\n",
    "    \n",
    "    def get_signal_stats(self) -> dict:\n",
    "        \"\"\"å–å¾—ä¿¡è™Ÿçµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if self.signals is None:\n",
    "            return None\n",
    "        \n",
    "        signal_count = self.signals['signal'].sum()\n",
    "        total_bars = len(self.signals)\n",
    "        \n",
    "        return {\n",
    "            'total_signals': signal_count,\n",
    "            'signal_rate': signal_count / total_bars,\n",
    "            'avg_HL115': self.signals['HL115'].mean(),\n",
    "            'avg_price_to_signal': (self.signals['Close'] / self.signals['çªç ´signalåƒ¹']).mean()\n",
    "        }\n",
    "    \n",
    "    def plot_strategy(self, start_idx: Optional[int] = None, end_idx: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ç­–ç•¥åœ–\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_idx : int, optional\n",
    "            èµ·å§‹ç´¢å¼•\n",
    "        end_idx : int, optional\n",
    "            çµæŸç´¢å¼•\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.signals is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_indicators()\")\n",
    "            return\n",
    "        \n",
    "        df = self.signals.iloc[start_idx:end_idx] if start_idx or end_idx else self.signals\n",
    "        signal_events = df[df['signal'] == True].index\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # 1. åƒ¹æ ¼èˆ‡çªç ´ä¿¡è™Ÿ\n",
    "        axes[0].plot(df.index, df['Close'], label='Close Price', alpha=0.7, linewidth=1)\n",
    "        axes[0].plot(df.index, df['çªç ´signalåƒ¹'], label='Breakout Signal', \n",
    "                    linestyle='--', alpha=0.7, color='orange')\n",
    "        axes[0].scatter(signal_events, df.loc[signal_events, 'Close'], \n",
    "                       color='red', s=50, label='Entry Signal', zorder=5, marker='^')\n",
    "        axes[0].set_title(f'L1-OLD Strategy (entry_param={self.entry_param})', fontsize=12)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. HL115 æ³¢å‹•æŒ‡æ¨™\n",
    "        axes[1].plot(df.index, df['HL115'], label='HL115', color='orange', linewidth=1)\n",
    "        axes[1].set_title(f'Price Range Indicator (window={self.rolling_window})', fontsize=12)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# =====================================================\n",
    "# 3ï¸âƒ£ S1 ç­–ç•¥ Class (åšç©º)\n",
    "# =====================================================\n",
    "class S1Strategy:\n",
    "    \"\"\"S1 åšç©ºç­–ç•¥\"\"\"\n",
    "    \n",
    "    def __init__(self, entry_param: float = 0.5, entry_param2: float = 0.4, rolling_window: int = 460):\n",
    "        self.entry_param = entry_param\n",
    "        self.entry_param2 = entry_param2\n",
    "        self.rolling_window = rolling_window\n",
    "        self.signals = None\n",
    "        \n",
    "    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        \n",
    "        # è¨ˆç®— HL115\n",
    "        df['HL115'] = (\n",
    "            df['High'].shift(1).rolling(self.rolling_window, min_periods=self.rolling_window).max() - \n",
    "            df['Low'].shift(1).rolling(self.rolling_window, min_periods=self.rolling_window).min()\n",
    "        )\n",
    "        \n",
    "        # è¨ˆç®— lowpoint\n",
    "        prev_low_min = df['Low'].shift(1).rolling(self.rolling_window).min()\n",
    "        prev_high_max = df['High'].shift(1).rolling(self.rolling_window).max()\n",
    "        \n",
    "        # é¿å…é™¤ä»¥é›¶\n",
    "        denom = prev_high_max - prev_low_min\n",
    "        denom = denom.replace(0, np.nan)\n",
    "        \n",
    "        df['lowpoint'] = (df['Low'] - prev_low_min) / denom\n",
    "        \n",
    "        # è¨ˆç®— avg_lowpoint\n",
    "        df['avg_lowpoint'] = df['lowpoint'].shift(1).rolling(self.rolling_window).mean()\n",
    "        \n",
    "        # è¨ˆç®— çªç ´signalåƒ¹\n",
    "        df['çªç ´signalåƒ¹'] = prev_low_min + (self.entry_param * df['HL115'])\n",
    "        \n",
    "        # ç”Ÿæˆä¿¡è™Ÿ\n",
    "        # æ¢ä»¶: Low < çªç ´åƒ¹ & Open > çªç ´åƒ¹ & avg_lowpoint < entry_param2\n",
    "        condition = (\n",
    "            (df['Low'] < df['çªç ´signalåƒ¹']) & \n",
    "            (df['Open'] > df['çªç ´signalåƒ¹'])  \n",
    "            # (df['avg_lowpoint'] < self.entry_param2)\n",
    "        )\n",
    "        \n",
    "        df['signal'] = condition\n",
    "        df.loc[df.index[:115], 'signal'] = False\n",
    "        \n",
    "        # è½‰æ›ç‚º side (-1 åšç©º)\n",
    "        df['side'] = np.nan\n",
    "        df.loc[df['signal'] == True, 'side'] = -1.0\n",
    "        df['side'] = df['side'].ffill()\n",
    "        \n",
    "        self.signals = df\n",
    "        \n",
    "        # çµ±è¨ˆ\n",
    "        total_bars = len(df)\n",
    "        signal_count = df['signal'].sum()\n",
    "        signal_rate = signal_count / total_bars if total_bars > 0 else 0\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š S1 ç­–ç•¥çµ±è¨ˆ (Short)\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½ K ç·šæ•¸: {total_bars:,}\")\n",
    "        print(f\"ä¿¡è™Ÿæ•¸é‡: {signal_count:,}\")\n",
    "        print(f\"ä¿¡è™Ÿæ¯”ä¾‹: {signal_rate:.4%}\")\n",
    "        print(f\"ç­–ç•¥åƒæ•¸: entry_param={self.entry_param}, entry_param2={self.entry_param2}, rolling_window={self.rolling_window}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_signal_events(self) -> pd.DatetimeIndex:\n",
    "        if self.signals is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_indicators()\")\n",
    "            return None\n",
    "        return self.signals[self.signals['signal'] == True].index\n",
    "    \n",
    "    def plot_strategy(self, start_idx: Optional[int] = None, end_idx: Optional[int] = None):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if self.signals is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ calculate_indicators()\")\n",
    "            return\n",
    "        \n",
    "        df = self.signals.iloc[start_idx:end_idx] if start_idx or end_idx else self.signals\n",
    "        signal_events = df[df['signal'] == True].index\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
    "        \n",
    "        # 1. åƒ¹æ ¼èˆ‡çªç ´ä¿¡è™Ÿ\n",
    "        axes[0].plot(df.index, df['Close'], label='Close Price', alpha=0.7, linewidth=1)\n",
    "        axes[0].plot(df.index, df['çªç ´signalåƒ¹'], label='Breakout Signal', linestyle='--', alpha=0.7, color='orange')\n",
    "        axes[0].scatter(signal_events, df.loc[signal_events, 'Close'], color='green', s=50, label='Short Signal', zorder=5, marker='v')\n",
    "        axes[0].set_title(f'S1 Strategy (Short)', fontsize=12)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. lowpoint & avg_lowpoint\n",
    "        axes[1].plot(df.index, df['lowpoint'], label='lowpoint', alpha=0.5, linewidth=0.5)\n",
    "        axes[1].plot(df.index, df['avg_lowpoint'], label='avg_lowpoint', color='red', linewidth=1)\n",
    "        axes[1].axhline(y=self.entry_param2, color='gray', linestyle='--', label='Threshold')\n",
    "        axes[1].set_title('Lowpoint Indicator', fontsize=12)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. HL115\n",
    "        axes[2].plot(df.index, df['HL115'], label='HL115', color='orange', linewidth=1)\n",
    "        axes[2].set_title('HL115', fontsize=12)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fc79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. åœ¨ç¯©é¸å¾Œçš„æ•¸æ“šä¸Šè¨ˆç®—ç­–ç•¥ä¿¡è™Ÿ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ˆ åœ¨ç¯©é¸å¾Œçš„æ•¸æ“šä¸Šè¨ˆç®—ç­–ç•¥ä¿¡è™Ÿ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "strategy = S1Strategy(entry_param=0.5, rolling_window=500)\n",
    "df = strategy.calculate_indicators(df_filtered)  # âœ… ä½¿ç”¨ df_filtered\n",
    "tEvents = strategy.get_signal_events()  # ç­–ç•¥ä¿¡è™Ÿçš„äº‹ä»¶é»\n",
    "strategy.plot_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce1d23",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ3ï¼šMeta Labeling ç›®æ¨™è®Šæ•¸ç”Ÿæˆèˆ‡æ­¢ç›ˆæ­¢æè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8780be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3ï¸âƒ£ Meta-Labeling Class\n",
    "# =====================================================\n",
    "class MetaLabeling:\n",
    "    \"\"\"\n",
    "    Meta-Labelingï¼šTriple Barrier å’Œæ¨™ç±¤ç”Ÿæˆ\n",
    "    \n",
    "    Triple Barrier Method èªªæ˜ï¼š\n",
    "    ============================\n",
    "    Triple Barrier Method æ˜¯ä¸€ç¨®æ¨™ç±¤æ–¹æ³•ï¼Œæ ¹æ“šä¸‰å€‹å±éšœä¸­ç¬¬ä¸€å€‹è¢«è§¸åŠçš„\n",
    "    å±éšœä¾†æ¨™è¨˜è§€æ¸¬å€¼ã€‚\n",
    "    \n",
    "    ç”¨é€”ï¼š\n",
    "    -----\n",
    "    1. å‹•æ…‹è¨­å®šæ­¢ç›ˆæ­¢æï¼šæ ¹æ“šä¼°è¨ˆçš„æ³¢å‹•ç‡ï¼ˆå·²å¯¦ç¾æˆ–éš±å«ï¼‰å‹•æ…‹èª¿æ•´\n",
    "    2. é¿å…ç›®æ¨™éé«˜æˆ–éä½ï¼šè€ƒæ…®ç•¶å‰æ³¢å‹•ç‡ï¼Œè¨­å®šåˆç†çš„æ­¢ç›ˆæ­¢ææ°´å¹³\n",
    "    3. è·¯å¾‘ä¾è³´æ¨™ç±¤ï¼šè€ƒæ…®å¾ [t_i,0, t_i,0 + h] çš„å®Œæ•´åƒ¹æ ¼è·¯å¾‘\n",
    "    \n",
    "    ä¸‰å€‹å±éšœï¼š\n",
    "    --------\n",
    "    1. ä¸Šæ–¹æ°´å¹³å±éšœï¼ˆProfit Takingï¼‰ï¼šæ­¢ç›ˆé™åˆ¶\n",
    "    2. ä¸‹æ–¹æ°´å¹³å±éšœï¼ˆStop Lossï¼‰ï¼šæ­¢æé™åˆ¶\n",
    "    3. å‚ç›´å±éšœï¼ˆVertical Barrierï¼‰ï¼šæ™‚é–“åˆ°æœŸé™åˆ¶ï¼ˆæŒæœ‰æœŸï¼‰\n",
    "    \n",
    "    å±éšœé…ç½®ï¼š\n",
    "    --------\n",
    "    ç”¨ä¸‰å…ƒçµ„ [pt, sl, t1] è¡¨ç¤ºï¼Œå…¶ä¸­ï¼š\n",
    "    - 0 è¡¨ç¤ºå±éšœæœªå•Ÿç”¨\n",
    "    - 1 è¡¨ç¤ºå±éšœå•Ÿç”¨\n",
    "    \n",
    "    å¸¸ç”¨é…ç½®ï¼š\n",
    "    - [1,1,1]: æ¨™æº–è¨­å®šï¼ˆæ­¢ç›ˆã€æ­¢æã€æ™‚é–“åˆ°æœŸï¼‰\n",
    "    - [0,1,1]: åªæœ‰æ­¢æå’Œæ™‚é–“åˆ°æœŸ\n",
    "    - [1,1,0]: åªæœ‰æ­¢ç›ˆå’Œæ­¢æï¼ˆç„¡æ™‚é–“é™åˆ¶ï¼‰\n",
    "    \n",
    "    æ•¸å­¸åŸç†ï¼š\n",
    "    ---------\n",
    "    å°æ–¼æ¯å€‹äº‹ä»¶æ™‚é–“é» t_i,0ï¼š\n",
    "    \n",
    "    1. ä¸Šæ–¹å±éšœï¼ˆæ­¢ç›ˆï¼‰ï¼š\n",
    "       PT = t_i,0 + pt Ã— Ïƒ_t_i,0\n",
    "       å…¶ä¸­ pt æ˜¯æ­¢ç›ˆå€æ•¸ï¼ŒÏƒ_t_i,0 æ˜¯ç›®æ¨™æ³¢å‹•ç‡\n",
    "    \n",
    "    2. ä¸‹æ–¹å±éšœï¼ˆæ­¢æï¼‰ï¼š\n",
    "       SL = t_i,0 - sl Ã— Ïƒ_t_i,0\n",
    "       å…¶ä¸­ sl æ˜¯æ­¢æå€æ•¸\n",
    "    \n",
    "    3. å‚ç›´å±éšœï¼ˆæ™‚é–“åˆ°æœŸï¼‰ï¼š\n",
    "       t_i,1 = t_i,0 + h\n",
    "       å…¶ä¸­ h æ˜¯æŒæœ‰æœŸï¼ˆæœŸæ•¸ï¼‰\n",
    "    \n",
    "    4. ç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼š\n",
    "       t_i,1 = min{PT_touch, SL_touch, t_i,0 + h}\n",
    "    \n",
    "    5. æ¨™ç±¤ç”Ÿæˆï¼š\n",
    "       - bin = 1:  è§¸åŠä¸Šæ–¹å±éšœï¼ˆæˆåŠŸï¼‰\n",
    "       - bin = -1: è§¸åŠä¸‹æ–¹å±éšœï¼ˆå¤±æ•—ï¼‰\n",
    "       - bin = 0:  è§¸åŠå‚ç›´å±éšœï¼ˆæ™‚é–“åˆ°æœŸï¼‰\n",
    "    \n",
    "    è·¯å¾‘ä¾è³´æ€§ï¼š\n",
    "    ----------\n",
    "    ç‚ºäº†æ¨™è¨˜è§€æ¸¬å€¼ï¼Œå¿…é ˆè€ƒæ…®å¾ t_i,0 åˆ° t_i,0 + h çš„å®Œæ•´åƒ¹æ ¼è·¯å¾‘ã€‚\n",
    "    é€™ä½¿å¾—æ¨™ç±¤æ˜¯è·¯å¾‘ä¾è³´çš„ï¼Œè€Œéåƒ…ä¾è³´çµ‚é»åƒ¹æ ¼ã€‚\n",
    "    \n",
    "    å‹•æ…‹é–¾å€¼ï¼š\n",
    "    --------\n",
    "    ä½¿ç”¨ getDailyVol() è¨ˆç®—æ—¥æ³¢å‹•ç‡ï¼Œä½œç‚ºå‹•æ…‹èª¿æ•´æ­¢ç›ˆæ­¢æçš„åŸºç¤ï¼š\n",
    "    \n",
    "    Ïƒ_t = EWM_std(returns, span=span0)\n",
    "    \n",
    "    å…¶ä¸­ returns æ˜¯æ—¥æ”¶ç›Šç‡åºåˆ—ã€‚\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Advances in Financial Machine Learning, Chapter 3\n",
    "    - Snippet 3.1: getDailyVol - è¨ˆç®—æ—¥æ³¢å‹•ç‡\n",
    "    - Snippet 3.2: applyPtSlOnT1 - Triple Barrier æ¨™ç±¤æ–¹æ³•\n",
    "    - Snippet 3.3: getEvents - å–å¾—ç¬¬ä¸€å€‹å±éšœè§¸åŠæ™‚é–“\n",
    "    - Snippet 3.4: getVerticalBarriers - è¨­å®šå‚ç›´å±éšœ\n",
    "    - Snippet 3.5: getBins - ç”Ÿæˆæ¨™ç±¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ptSl: List[float] = [1, 2], numPeriods: int = 100, \n",
    "                 minRet: float = 0.01, min_label_pct: float = 0.05):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        ptSl : list\n",
    "            [æ­¢ç›ˆå€æ•¸, æ­¢æå€æ•¸]\n",
    "            - ptSl[0]: ä¸Šæ–¹å±éšœçš„å€æ•¸ï¼ˆProfit Takingï¼‰\n",
    "            - ptSl[1]: ä¸‹æ–¹å±éšœçš„å€æ•¸ï¼ˆStop Lossï¼‰\n",
    "            - å¦‚æœç‚º 0ï¼Œå‰‡è©²å±éšœä¸å•Ÿç”¨\n",
    "        numPeriods : int\n",
    "            å‚ç›´å±éšœæœŸæ•¸ï¼ˆä¾‹å¦‚ï¼š100 æ ¹ K ç·šï¼‰\n",
    "            å°æ‡‰è«–æ–‡ä¸­çš„ hï¼ˆæŒæœ‰æœŸï¼‰\n",
    "        minRet : float\n",
    "            æœ€å°æ³¢å‹•ç‡é–¾å€¼\n",
    "            åªæœ‰ç•¶ trgt > minRet æ™‚æ‰æœƒç”Ÿæˆäº‹ä»¶\n",
    "        min_label_pct : float\n",
    "            æœ€å°æ¨™ç±¤æ¯”ä¾‹ï¼ˆç”¨æ–¼ dropLabelsï¼‰\n",
    "            ä½æ–¼æ­¤æ¯”ä¾‹çš„æ¨™ç±¤æœƒè¢«ç§»é™¤\n",
    "        \"\"\"\n",
    "        self.ptSl = ptSl\n",
    "        self.numPeriods = numPeriods\n",
    "        self.minRet = minRet\n",
    "        self.min_label_pct = min_label_pct\n",
    "        self.events = None\n",
    "        self.bins = None\n",
    "        \n",
    "    def get_vertical_barriers(self, close: pd.Series, tEvents: pd.DatetimeIndex) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨­å®šå‚ç›´æ™‚é–“å±éšœï¼ˆä»¥æœŸæ•¸ç‚ºå–®ä½ï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼ˆåŸºæ–¼ Snippet 3.4ï¼‰ï¼š\n",
    "        ----------------------------\n",
    "        å°æ¯å€‹äº‹ä»¶æ™‚é–“é» t_i,0ï¼Œæ‰¾åˆ° numPeriods æœŸå¾Œçš„æ™‚é–“é»ã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        t_i,1 = close.index[position(t_i,0) + numPeriods]\n",
    "        \n",
    "        å¦‚æœè¶…å‡ºæ•¸æ“šç¯„åœï¼Œå‰‡ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“é»ã€‚\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        t1 : pd.Series\n",
    "            å‚ç›´å±éšœæ™‚é–“é»ï¼ˆindex=äº‹ä»¶èµ·é», value=å±éšœæ™‚é–“é»ï¼‰\n",
    "        \"\"\"\n",
    "        t1 = []\n",
    "        close_index = close.index\n",
    "        \n",
    "        for tEvent in tEvents:\n",
    "            # æ‰¾åˆ° tEvent åœ¨ close ä¸­çš„ä½ç½®\n",
    "            try:\n",
    "                event_pos = close_index.get_loc(tEvent)\n",
    "                # è¨ˆç®— numPeriods æœŸå¾Œçš„ä½ç½®\n",
    "                barrier_pos = event_pos + self.numPeriods\n",
    "                \n",
    "                # ç¢ºä¿ä¸è¶…éæ•¸æ“šç¯„åœ\n",
    "                if barrier_pos < len(close_index):\n",
    "                    t1.append(close_index[barrier_pos])\n",
    "                else:\n",
    "                    # å¦‚æœè¶…å‡ºç¯„åœï¼Œä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“é»\n",
    "                    t1.append(close_index[-1])\n",
    "            except KeyError:\n",
    "                # å¦‚æœæ‰¾ä¸åˆ°è©²æ™‚é–“é»ï¼Œè·³é\n",
    "                continue\n",
    "        \n",
    "        if len(t1) == 0:\n",
    "            return pd.Series(dtype='datetime64[ns]')\n",
    "        \n",
    "        # ç¢ºä¿ t1 çš„ç´¢å¼•èˆ‡ tEvents å°æ‡‰\n",
    "        t1_series = pd.Series(t1, index=tEvents[:len(t1)])\n",
    "        return t1_series\n",
    "    \n",
    "    def apply_pt_sl_on_t1(self, close: pd.Series, events: pd.DataFrame, \n",
    "                          molecule: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        å¥—ç”¨æ­¢ç›ˆæ­¢æï¼ˆåŸºæ–¼ Snippet 3.2ï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ï¼Œè¨ˆç®—å¾äº‹ä»¶é–‹å§‹åˆ°å‚ç›´å±éšœæœŸé–“çš„åƒ¹æ ¼è·¯å¾‘ï¼š\n",
    "        \n",
    "        path_prices = close[loc:t1]\n",
    "        path_returns = (path_prices / close[loc] - 1) Ã— side\n",
    "        \n",
    "        ç„¶å¾Œæ‰¾å‡ºï¼š\n",
    "        - æœ€æ—©è§¸åŠæ­¢ç›ˆçš„æ™‚é–“ï¼špath_returns > pt Ã— trgt\n",
    "        - æœ€æ—©è§¸åŠæ­¢æçš„æ™‚é–“ï¼špath_returns < -sl Ã— trgt\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        - PT_level = pt Ã— trgt\n",
    "        - SL_level = -sl Ã— trgt\n",
    "        - path_returns = (close[t] / close[loc] - 1) Ã— side\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        events : pd.DataFrame\n",
    "            äº‹ä»¶ DataFrameï¼ˆåŒ…å« t1, trgt, sideï¼‰\n",
    "        molecule : pd.DatetimeIndex\n",
    "            è¦è™•ç†çš„äº‹ä»¶å­é›†ï¼ˆç”¨æ–¼ä¸¦è¡Œè¨ˆç®—ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        out : pd.DataFrame\n",
    "            åŒ…å«æ¯å€‹å±éšœè§¸åŠæ™‚é–“çš„ DataFrame\n",
    "        \"\"\"\n",
    "        events_ = events.loc[molecule]\n",
    "        out = events_[['t1']].copy(deep=True)\n",
    "        \n",
    "        # è¨ˆç®—æ­¢ç›ˆæ°´å¹³\n",
    "        if self.ptSl[0] > 0:\n",
    "            pt = self.ptSl[0] * events_['trgt']  # PT = pt Ã— Ïƒ\n",
    "        else:\n",
    "            pt = pd.Series(index=events.index)  # æœªå•Ÿç”¨\n",
    "        \n",
    "        # è¨ˆç®—æ­¢ææ°´å¹³\n",
    "        if self.ptSl[1] > 0:\n",
    "            sl = -self.ptSl[1] * events_['trgt']  # SL = -sl Ã— Ïƒ\n",
    "        else:\n",
    "            sl = pd.Series(index=events.index)  # æœªå•Ÿç”¨\n",
    "        \n",
    "        # å°æ¯å€‹äº‹ä»¶è¨ˆç®—è·¯å¾‘\n",
    "        for loc, t1 in events_['t1'].fillna(close.index[-1]).items():\n",
    "            # å–å¾—åƒ¹æ ¼è·¯å¾‘\n",
    "            df0 = close[loc:t1]\n",
    "            # è¨ˆç®—è·¯å¾‘å ±é…¬ï¼ˆè€ƒæ…®æ–¹å‘ï¼‰\n",
    "            df0 = (df0 / close[loc] - 1) * events_.at[loc, 'side']\n",
    "            \n",
    "            # æ‰¾å‡ºæœ€æ—©è§¸åŠæ­¢æçš„æ™‚é–“\n",
    "            out.loc[loc, 'sl'] = df0[df0 < sl[loc]].index.min()\n",
    "            # æ‰¾å‡ºæœ€æ—©è§¸åŠæ­¢ç›ˆçš„æ™‚é–“\n",
    "            out.loc[loc, 'pt'] = df0[df0 > pt[loc]].index.min()\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_events(self, close: pd.Series, tEvents: pd.DatetimeIndex, \n",
    "                   trgt: pd.Series, side: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆäº‹ä»¶ä¸¦å¥—ç”¨ Triple Barrierï¼ˆåŸºæ–¼ Snippet 3.3ï¼‰\n",
    "        \n",
    "        å¯¦ç¾æµç¨‹ï¼š\n",
    "        ---------\n",
    "        1. éæ¿¾ç›®æ¨™ï¼šåªä¿ç•™ trgt > minRet çš„äº‹ä»¶\n",
    "        2. è¨­å®šå‚ç›´å±éšœï¼šè¨ˆç®—æ¯å€‹äº‹ä»¶çš„æ™‚é–“åˆ°æœŸé»\n",
    "        3. å¥—ç”¨ Triple Barrierï¼šè¨ˆç®—æ­¢ç›ˆæ­¢æè§¸åŠæ™‚é–“\n",
    "        4. æ‰¾å‡ºç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼št1 = min{pt_touch, sl_touch, vertical_barrier}\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»ï¼ˆä¾†è‡ª CUSUM éæ¿¾æˆ–ç­–ç•¥ä¿¡è™Ÿï¼‰\n",
    "        trgt : pd.Series\n",
    "            ç›®æ¨™æ³¢å‹•ç‡ï¼ˆdaily_volï¼Œç”¨æ–¼å‹•æ…‹èª¿æ•´æ­¢ç›ˆæ­¢æï¼‰\n",
    "        side : pd.Series\n",
    "            æ–¹å‘ï¼ˆ1 ç‚ºåšå¤šï¼Œ-1 ç‚ºåšç©ºï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        events : pd.DataFrame\n",
    "            åŒ…å« t1, trgt, side, pt, sl çš„äº‹ä»¶ DataFrame\n",
    "            - t1: ç¬¬ä¸€å€‹å±éšœè§¸åŠæ™‚é–“\n",
    "            - trgt: ç›®æ¨™æ³¢å‹•ç‡\n",
    "            - side: æ–¹å‘\n",
    "            - pt: æ­¢ç›ˆå€æ•¸\n",
    "            - sl: æ­¢æå€æ•¸\n",
    "        \"\"\"\n",
    "        # 1. éæ¿¾ç›®æ¨™ï¼ˆåªä¿ç•™æ³¢å‹•ç‡è¶³å¤ å¤§çš„äº‹ä»¶ï¼‰\n",
    "        trgt = trgt.loc[tEvents]\n",
    "        trgt = trgt[trgt > self.minRet]\n",
    "        \n",
    "        # 2. è¨­å®šå‚ç›´å±éšœ\n",
    "        t1 = self.get_vertical_barriers(close, tEvents)\n",
    "        \n",
    "        # 3. å–å¾—æ–¹å‘\n",
    "        side_ = side.loc[trgt.index]\n",
    "        \n",
    "        # 4. çµ„åˆäº‹ä»¶ç‰©ä»¶\n",
    "        events = pd.concat({\n",
    "            't1': t1.loc[trgt.index], \n",
    "            'trgt': trgt, \n",
    "            'side': side_\n",
    "        }, axis=1)\n",
    "        events = events.dropna(subset=['trgt'])\n",
    "        \n",
    "        # 5. è¨ˆç®—æ­¢ç›ˆæ­¢æè§¸åŠæ™‚é–“\n",
    "        df0 = self.apply_pt_sl_on_t1(close, events, events.index)\n",
    "        \n",
    "        # 6. æ‰¾å‡ºç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼ˆæœ€æ—©çš„æ™‚é–“ï¼‰\n",
    "        events['t1'] = df0.dropna(how='all').min(axis=1)\n",
    "        \n",
    "        # 7. å„²å­˜å±éšœé…ç½®\n",
    "        events['pt'] = self.ptSl[0]\n",
    "        events['sl'] = self.ptSl[1]\n",
    "        \n",
    "        self.events = events\n",
    "        return events\n",
    "    \n",
    "    def get_bins(self, close: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ¨™ç±¤ï¼ˆåŸºæ–¼ Snippet 3.5ï¼Œä¸¦æ“´å±•ç‚ºæ”¯æ´ Meta-Labelingï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        1. å°é½Šåƒ¹æ ¼ï¼šå–å¾—äº‹ä»¶é–‹å§‹å’ŒçµæŸæ™‚é–“çš„æ‰€æœ‰åƒ¹æ ¼é»\n",
    "        2. è¨ˆç®—å¯¦éš›å ±é…¬ï¼š\n",
    "           ret = (close[t1] / close[t0] - 1) Ã— side\n",
    "        3. åˆ¤æ–·è§¸åŠçš„å±éšœï¼š\n",
    "           - å¦‚æœ ret > 0 ä¸” ret > pt Ã— trgt â†’ bin = 1ï¼ˆè§¸åŠæ­¢ç›ˆï¼‰\n",
    "           - å¦‚æœ ret < 0 ä¸” ret < -sl Ã— trgt â†’ bin = -1ï¼ˆè§¸åŠæ­¢æï¼‰\n",
    "           - å¦å‰‡ â†’ bin = 0ï¼ˆè§¸åŠå‚ç›´å±éšœï¼‰\n",
    "        \n",
    "        æ¨™ç±¤å®šç¾©ï¼š\n",
    "        --------\n",
    "        - bin = 1:  æˆåŠŸï¼ˆè§¸åŠæ­¢ç›ˆå±éšœï¼‰\n",
    "        - bin = 0:  æ™‚é–“åˆ°æœŸï¼ˆè§¸åŠå‚ç›´å±éšœï¼‰\n",
    "        - bin = -1: è™§æï¼ˆè§¸åŠæ­¢æå±éšœï¼‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        bins : pd.DataFrame\n",
    "            åŒ…å« ret, trgt, bin, side çš„æ¨™ç±¤ DataFrame\n",
    "        \"\"\"\n",
    "        if self.events is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_events()\")\n",
    "            return None\n",
    "        \n",
    "        events_ = self.events.dropna(subset=['t1'])\n",
    "        px = events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "        px = close.reindex(px, method='bfill')\n",
    "        \n",
    "        out = pd.DataFrame(index=events_.index)\n",
    "        out['ret'] = px.loc[events_['t1'].values].values / px.loc[events_.index] - 1\n",
    "        out['ret'] *= events_['side']\n",
    "        out['trgt'] = events_['trgt']\n",
    "        \n",
    "        # åˆ¤æ–·è§¸åŠå“ªå€‹å±éšœ\n",
    "        out['bin'] = 0  # é è¨­ç‚ºæ™‚é–“åˆ°æœŸ\n",
    "        for date_time, values in out.iterrows():\n",
    "            ret = values['ret']\n",
    "            target = values['trgt']\n",
    "            pt_level = ret > target * self.events.loc[date_time, 'pt']\n",
    "            sl_level = ret < -target * self.events.loc[date_time, 'sl']\n",
    "            \n",
    "            if ret > 0.0 and pt_level:\n",
    "                # è§¸åŠæ­¢ç›ˆå±éšœ â†’ bin = 1 (æˆåŠŸ)\n",
    "                out.loc[date_time, 'bin'] = 1\n",
    "            elif ret < 0.0 and sl_level:\n",
    "                # è§¸åŠæ­¢æå±éšœ â†’ bin = -1 (è™§æ)\n",
    "                out.loc[date_time, 'bin'] = -1\n",
    "            else:\n",
    "                # æ™‚é–“åˆ°æœŸï¼ˆå‚ç›´å±éšœï¼‰â†’ bin = 0\n",
    "                out.loc[date_time, 'bin'] = 0\n",
    "        \n",
    "        # å¦‚æœæœ‰ sideï¼ˆMeta-Labelingï¼‰ï¼Œä¿ç•™ side è³‡è¨Š\n",
    "        if 'side' in events_:\n",
    "            out['side'] = events_['side']\n",
    "        \n",
    "        self.bins = out\n",
    "        return out\n",
    "    \n",
    "    def drop_rare_labels(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç§»é™¤ç¨€æœ‰æ¨™ç±¤ï¼ˆåŸºæ–¼ Snippet 3.8ï¼‰\n",
    "        \n",
    "        ç›®çš„ï¼š\n",
    "        -----\n",
    "        ç§»é™¤æ¨£æœ¬æ•¸éå°‘çš„æ¨™ç±¤é¡åˆ¥ï¼Œé¿å…æ¨¡å‹å­¸ç¿’åæ–œã€‚\n",
    "        \n",
    "        æµç¨‹ï¼š\n",
    "        -----\n",
    "        1. è¨ˆç®—æ¯å€‹æ¨™ç±¤çš„æ¯”ä¾‹\n",
    "        2. å¦‚æœæœ€å°æ¯”ä¾‹ < min_label_pctï¼Œç§»é™¤è©²æ¨™ç±¤\n",
    "        3. é‡è¤‡ç›´åˆ°æ‰€æœ‰æ¨™ç±¤æ¯”ä¾‹éƒ½ >= min_label_pct\n",
    "        \"\"\"\n",
    "        if self.bins is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_bins()\")\n",
    "            return None\n",
    "        \n",
    "        bins = self.bins.copy()\n",
    "        \n",
    "        while True:\n",
    "            df0 = bins['bin'].value_counts(normalize=True)\n",
    "            if df0.min() > self.min_label_pct or df0.shape[0] < 3:\n",
    "                break\n",
    "            print(f\"Dropped label {df0.idxmin()}, percentage: {df0.min():.4f}\")\n",
    "            bins = bins[bins['bin'] != df0.idxmin()]\n",
    "        \n",
    "        self.bins = bins\n",
    "        self.events = self.events.loc[bins.index]\n",
    "        \n",
    "        return bins\n",
    "    \n",
    "    def get_label_stats(self) -> dict:\n",
    "        \"\"\"å–å¾—æ¨™ç±¤çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if self.bins is None:\n",
    "            return None\n",
    "        \n",
    "        label_counts = self.bins['bin'].value_counts()\n",
    "        \n",
    "        return {\n",
    "            'total_labels': len(self.bins),\n",
    "            'label_distribution': label_counts.to_dict(),\n",
    "            'avg_return': self.bins['ret'].mean(),\n",
    "            'win_rate': (self.bins['bin'] == 1).sum() / len(self.bins),\n",
    "            'loss_rate': (self.bins['bin'] == -1).sum() / len(self.bins),\n",
    "            'timeout_rate': (self.bins['bin'] == 0).sum() / len(self.bins)\n",
    "        }\n",
    "    \n",
    "    def plot_labels(self):\n",
    "        \"\"\"ç¹ªè£½æ¨™ç±¤åˆ†å¸ƒåœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.bins is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_bins()\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # æ¨™ç±¤åˆ†å¸ƒ\n",
    "        label_counts = self.bins['bin'].value_counts().sort_index()\n",
    "        colors = {-1: 'red', 0: 'gray', 1: 'green'}\n",
    "        label_colors = [colors.get(idx, 'steelblue') for idx in label_counts.index]\n",
    "        \n",
    "        label_counts.plot(kind='bar', ax=axes[0], color=label_colors)\n",
    "        axes[0].set_title('Label Distribution', fontsize=12)\n",
    "        axes[0].set_xlabel('Label (-1: Loss, 0: Timeout, 1: Win)')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # å›å ±åˆ†å¸ƒ\n",
    "        self.bins['ret'].hist(bins=50, ax=axes[1], alpha=0.7, color='steelblue')\n",
    "        axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Return')\n",
    "        axes[1].set_title('Return Distribution', fontsize=12)\n",
    "        axes[1].set_xlabel('Return')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5226ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Meta-Labelingï¼ˆä½¿ç”¨åŸå§‹å®Œæ•´æ•¸æ“šï¼‰\n",
    "meta = MetaLabeling(ptSl=[0.25, 0.25], numPeriods=32)\n",
    "events = meta.get_events(\n",
    "    nas100_raw['Close'],  # âœ… åŸå§‹å®Œæ•´æ•¸æ“š\n",
    "    tEvents,              # ç­–ç•¥ä¿¡è™Ÿçš„äº‹ä»¶é»\n",
    "    daily_vol,            # åŸºæ–¼åŸå§‹æ•¸æ“šè¨ˆç®—\n",
    "    df['side']            # ç­–ç•¥ä¿¡è™Ÿçš„æ–¹å‘ï¼ˆä¾†è‡ªéæ¿¾å¾Œçš„æ•¸æ“šï¼‰\n",
    ")\n",
    "bins = meta.get_bins(nas100_raw['Close'])  # âœ… åŸå§‹å®Œæ•´æ•¸æ“š\n",
    "meta.plot_labels()\n",
    "meta.drop_rare_labels()\n",
    "meta.get_label_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e442d",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ4 ï¼š æ¨£æœ¬æ¬Šé‡å¹³è¡¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c766862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4ï¸âƒ£ æ¨£æœ¬æ¬Šé‡ Classï¼ˆä¸¦ç™¼åº¦ã€å”¯ä¸€æ€§ã€æ™‚é–“è¡°æ¸›ã€é¡åˆ¥å¹³è¡¡ï¼‰\n",
    "# =====================================================\n",
    "class SampleWeight:\n",
    "    \"\"\"\n",
    "    æ¨£æœ¬æ¬Šé‡è¨ˆç®—ï¼šä¸¦ç™¼åº¦ã€å”¯ä¸€æ€§ã€æ™‚é–“è¡°æ¸›ã€é¡åˆ¥å¹³è¡¡\n",
    "    \n",
    "    Chapter 4: Sample Weights èªªæ˜\n",
    "    ===============================\n",
    "    \n",
    "    å•é¡ŒèƒŒæ™¯ï¼š\n",
    "    --------\n",
    "    åœ¨é‡‘èæ‡‰ç”¨ä¸­ï¼Œè§€æ¸¬å€¼ä¸æ˜¯ç”±ç¨ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰éç¨‹ç”Ÿæˆçš„ã€‚\n",
    "    ç•¶å…©å€‹æ¨™ç±¤ y_i å’Œ y_j çš„æ™‚é–“å€é–“æœ‰é‡ç–Šæ™‚ï¼ˆt_i,1 > t_j,0ï¼‰ï¼Œ\n",
    "    å®ƒå€‘æœƒä¾è³´å…±åŒçš„å ±é…¬ r_{t_j,0, min{t_i,1, t_j,1}}ã€‚\n",
    "    \n",
    "    é€™å°è‡´æ¨™ç±¤åºåˆ— {y_i}_{i=1,...,I} ä¸æ˜¯ IIDï¼Œé•åäº†å¤§å¤šæ•¸ ML ç®—æ³•çš„å‡è¨­ã€‚\n",
    "    \n",
    "    è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "    --------\n",
    "    é€šéè¨­è¨ˆæ¡æ¨£å’ŒåŠ æ¬Šæ–¹æ¡ˆä¾†ç³¾æ­£é‡ç–Šçµæœçš„ä¸ç•¶å½±éŸ¿ï¼š\n",
    "    1. è¨ˆç®—ä¸¦ç™¼åº¦ï¼ˆConcurrencyï¼‰ï¼šæ¯å€‹æ™‚é–“é»æœ‰å¤šå°‘æ¨™ç±¤åŒæ™‚å­˜æ´»\n",
    "    2. è¨ˆç®—å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰ï¼šæ¯å€‹æ¨™ç±¤çš„å¹³å‡å”¯ä¸€æ€§\n",
    "    3. æ‡‰ç”¨æ™‚é–“è¡°æ¸›ï¼ˆTime Decayï¼‰ï¼šè®“èˆŠæ¨£æœ¬æ¬Šé‡é™ä½\n",
    "    4. é¡åˆ¥å¹³è¡¡ï¼ˆClass Balanceï¼‰ï¼šå¹³è¡¡ä¸åŒé¡åˆ¥çš„æ¨£æœ¬æ¬Šé‡\n",
    "    5. çµ„åˆæ¬Šé‡ï¼šæœ€çµ‚æ¬Šé‡ = å”¯ä¸€æ€§ Ã— æ™‚é–“è¡°æ¸› Ã— é¡åˆ¥å¹³è¡¡ï¼ˆå¯é¸ï¼šÃ— å ±é…¬æ­¸å› ï¼‰\n",
    "    \n",
    "    ä¸¦ç™¼åº¦ï¼ˆConcurrencyï¼‰ï¼š\n",
    "    ---------------------\n",
    "    å®šç¾©ï¼šå…©å€‹æ¨™ç±¤ y_i å’Œ y_j åœ¨æ™‚é–“ t æ˜¯ä¸¦ç™¼çš„ï¼Œç•¶å®ƒå€‘éƒ½ä¾è³´è‡³å°‘ä¸€å€‹\n",
    "    å…±åŒçš„å ±é…¬ r_{t-1,t} = p_t / p_{t-1} - 1ã€‚\n",
    "    \n",
    "    è¨ˆç®—æ–¹å¼ï¼š\n",
    "    å°æ¯å€‹æ™‚é–“é» t = 1,...,Tï¼Œå½¢æˆäºŒå…ƒé™£åˆ— {1_{t,i}}_{i=1,...,I}ï¼š\n",
    "    - 1_{t,i} = 1ï¼šå¦‚æœ [t_i,0, t_i,1] èˆ‡ [t-1, t] é‡ç–Š\n",
    "    - 1_{t,i} = 0ï¼šå¦å‰‡\n",
    "    \n",
    "    ä¸¦ç™¼åº¦ï¼šc_t = Î£_{i=1}^I 1_{t,i}\n",
    "    \n",
    "    å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰ï¼š\n",
    "    --------------------\n",
    "    å®šç¾©ï¼šæ¨™ç±¤ i åœ¨æ™‚é–“ t çš„å”¯ä¸€æ€§ç‚º u_{t,i} = 1_{t,i} / c_t\n",
    "    \n",
    "    å¹³å‡å”¯ä¸€æ€§ï¼šÅ«_i = (Î£_{t=1}^T 1_{t,i})^{-1} Ã— (Î£_{t=1}^T u_{t,i})\n",
    "    \n",
    "    ä¹Ÿå¯ä»¥è§£é‡‹ç‚ºï¼šæ¨™ç±¤ i å­˜æ´»æœŸé–“çš„ä¸¦ç™¼åº¦å€’æ•¸çš„èª¿å’Œå¹³å‡ã€‚\n",
    "    \n",
    "    å ±é…¬æ­¸å› ï¼ˆReturn Attributionï¼‰ï¼š\n",
    "    -----------------------------\n",
    "    ç•¶æ¨™ç±¤æ˜¯å ±é…¬ç¬¦è™Ÿçš„å‡½æ•¸æ™‚ï¼ˆ{-1,1} æˆ– {0,1}ï¼‰ï¼Œæ¨£æœ¬æ¬Šé‡å¯ä»¥å®šç¾©ç‚ºï¼š\n",
    "    \n",
    "    Ìƒw_i = |Î£_{t=t_i,0}^{t_i,1} (r_{t-1,t} / c_t)|\n",
    "    \n",
    "    å…¶ä¸­ r_{t-1,t} æ˜¯å°æ•¸å ±é…¬ï¼Œc_t æ˜¯æ™‚é–“ t çš„ä¸¦ç™¼åº¦ã€‚\n",
    "    \n",
    "    ç„¶å¾Œæ¨™æº–åŒ–ï¼šw_i = Ìƒw_i Ã— I / (Î£_{j=1}^I Ìƒw_j)\n",
    "    \n",
    "    æ™‚é–“è¡°æ¸›ï¼ˆTime Decayï¼‰ï¼š\n",
    "    ----------------------\n",
    "    å¸‚å ´æ˜¯é©æ‡‰æ€§ç³»çµ±ï¼ŒèˆŠæ¨£æœ¬ä¸å¦‚æ–°æ¨£æœ¬ç›¸é—œã€‚\n",
    "    \n",
    "    ç·šæ€§æ™‚é–“è¡°æ¸›ï¼š\n",
    "    d[x] = max{0, a + bx}\n",
    "    \n",
    "    é‚Šç•Œæ¢ä»¶ï¼š\n",
    "    1. d[Î£_{i=1}^I Å«_i] = 1ï¼ˆæœ€æ–°æ¨£æœ¬æ¬Šé‡ç‚º 1ï¼‰\n",
    "    2. d[0] = cï¼ˆæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º cï¼Œc âˆˆ [0,1]ï¼‰\n",
    "    \n",
    "    åƒæ•¸ c çš„æ„ç¾©ï¼š\n",
    "    - c = 1ï¼šç„¡æ™‚é–“è¡°æ¸›\n",
    "    - 0 < c < 1ï¼šç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\n",
    "    - c = 0ï¼šæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "    - c < 0ï¼šæœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "    \n",
    "    æ³¨æ„ï¼šæ™‚é–“è¡°æ¸›æ˜¯åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ï¼Œè€Œéæ™‚é–“é †åºï¼Œå› ç‚ºåœ¨å­˜åœ¨å†—é¤˜\n",
    "    è§€æ¸¬å€¼çš„æƒ…æ³ä¸‹ï¼ŒæŒ‰æ™‚é–“é †åºè¡°æ¸›æœƒä½¿æ¬Šé‡é™ä½å¤ªå¿«ã€‚\n",
    "    \n",
    "    é¡åˆ¥å¹³è¡¡ï¼ˆClass Balanceï¼‰ï¼š\n",
    "    ------------------------\n",
    "    ç•¶æ¨™ç±¤åˆ†å¸ƒä¸å¹³è¡¡æ™‚ï¼ˆä¾‹å¦‚ï¼šä¸Šæ¼²æ¨™ç±¤å¤šæ–¼ä¸‹è·Œæ¨™ç±¤ï¼‰ï¼Œéœ€è¦å°å°‘æ•¸é¡åˆ¥\n",
    "    çµ¦äºˆæ›´é«˜çš„æ¬Šé‡ï¼Œä»¥å¹³è¡¡æ¨¡å‹è¨“ç·´ã€‚\n",
    "    \n",
    "    è¨ˆç®—æ–¹å¼ï¼š\n",
    "    w_class = n_samples / (n_classes Ã— n_class_samples)\n",
    "    \n",
    "    é€™æ¨£å¯ä»¥è®“å°‘æ•¸é¡åˆ¥çš„æ¨£æœ¬æ¬Šé‡æ›´é«˜ï¼Œå¹³è¡¡é¡åˆ¥åˆ†å¸ƒã€‚\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Advances in Financial Machine Learning, Chapter 4\n",
    "    - Snippet 4.1: mpNumCoEvents - è¨ˆç®—ä¸¦ç™¼åº¦\n",
    "    - Snippet 4.2: mpSampleTW - è¨ˆç®—å¹³å‡å”¯ä¸€æ€§\n",
    "    - Snippet 4.10: mpSampleW - å ±é…¬æ­¸å› æ¬Šé‡\n",
    "    - Snippet 4.11: getTimeDecay - æ™‚é–“è¡°æ¸›å› å­\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_co_events = None\n",
    "        self.uniqueness = None\n",
    "        self.time_decay = None\n",
    "        self.class_balance_weights = None\n",
    "        self.sample_weights = None\n",
    "        \n",
    "    def compute_num_co_events(self, close_idx: pd.DatetimeIndex, \n",
    "                              t1: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸ï¼ˆConcurrencyï¼‰\n",
    "        åŸºæ–¼ Snippet 4.1: mpNumCoEvents\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹æ™‚é–“é» tï¼Œè¨ˆç®—æœ‰å¤šå°‘å€‹äº‹ä»¶çš„å­˜æ´»æœŸé–“ [t_i,0, t_i,1] \n",
    "        èˆ‡æ™‚é–“é» t é‡ç–Šã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        c_t = Î£_{i=1}^I 1_{t,i}\n",
    "        \n",
    "        å…¶ä¸­ 1_{t,i} = 1 å¦‚æœ [t_i,0, t_i,1] èˆ‡ [t-1, t] é‡ç–Šï¼Œå¦å‰‡ç‚º 0ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        close_idx : pd.DatetimeIndex\n",
    "            åƒ¹æ ¼æ•¸æ“šçš„ç´¢å¼•\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        num_co_events : pd.Series\n",
    "            æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸\n",
    "        \"\"\"\n",
    "        t1 = t1.fillna(close_idx[-1])  # æœªçµæŸäº‹ä»¶ä»è¦è¨ˆç®—\n",
    "        \n",
    "        # åˆå§‹åŒ–è¨ˆæ•¸åºåˆ—\n",
    "        iloc = close_idx.searchsorted(np.array([t1.index[0], t1.max()]))\n",
    "        count = pd.Series(0, index=close_idx[iloc[0]:iloc[1]+1])\n",
    "        \n",
    "        # å°æ¯å€‹äº‹ä»¶ï¼Œåœ¨å…¶å­˜æ´»æœŸé–“ +1\n",
    "        for tIn, tOut in t1.items():\n",
    "            count.loc[tIn:tOut] += 1.0\n",
    "        \n",
    "        self.num_co_events = count\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š ä¸¦ç™¼åº¦çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½æ™‚é–“é»æ•¸: {len(count):,}\")\n",
    "        print(f\"å¹³å‡ä¸¦ç™¼äº‹ä»¶æ•¸: {count.mean():.2f}\")\n",
    "        print(f\"æœ€å¤§ä¸¦ç™¼äº‹ä»¶æ•¸: {count.max():.0f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - ä¸¦ç™¼åº¦è¡¡é‡æ¯å€‹æ™‚é–“é»æœ‰å¤šå°‘å€‹æ¨™ç±¤åŒæ™‚å­˜æ´»\")\n",
    "        print(f\"  - é«˜ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ä¹‹é–“é‡ç–Šå¤šï¼Œè³‡è¨Šå†—é¤˜\")\n",
    "        print(f\"  - ä½ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ç›¸å°ç¨ç«‹\")\n",
    "        print(f\"ä¸¦ç™¼åº¦åˆ†å¸ƒ:\")\n",
    "        print(count.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return count\n",
    "\n",
    "    def compute_uniqueness(self, t1: pd.Series, \n",
    "                          num_co_events: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¯å€‹äº‹ä»¶çš„å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰\n",
    "        åŸºæ–¼ Snippet 4.2: mpSampleTW\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ iï¼Œè¨ˆç®—å…¶åœ¨å­˜æ´»æœŸé–“çš„å¹³å‡å”¯ä¸€æ€§ï¼š\n",
    "        \n",
    "        u_{t,i} = 1_{t,i} / c_t  ï¼ˆæ™‚é–“ t çš„å”¯ä¸€æ€§ï¼‰\n",
    "        Å«_i = (1 / T_i) Ã— Î£_{t=t_i,0}^{t_i,1} u_{t,i}\n",
    "        \n",
    "        å…¶ä¸­ T_i æ˜¯äº‹ä»¶ i çš„å­˜æ´»æœŸæ•¸ã€‚\n",
    "        \n",
    "        ä¹Ÿå¯ä»¥è§£é‡‹ç‚ºï¼šäº‹ä»¶å­˜æ´»æœŸé–“çš„ä¸¦ç™¼åº¦å€’æ•¸çš„èª¿å’Œå¹³å‡ã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        Å«_i = mean(1 / c_t) for t âˆˆ [t_i,0, t_i,1]\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "        num_co_events : pd.Series\n",
    "            æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        uniqueness : pd.Series\n",
    "            æ¯å€‹äº‹ä»¶çš„å”¯ä¸€æ€§ï¼ˆ1 / å¹³å‡ä¸¦ç™¼åº¦ï¼‰\n",
    "        \"\"\"\n",
    "        wght = pd.Series(index=t1.index)\n",
    "        \n",
    "        for tIn, tOut in t1.items():\n",
    "            # è¨ˆç®—äº‹ä»¶å­˜æ´»æœŸé–“çš„å¹³å‡ä¸¦ç™¼åº¦å€’æ•¸\n",
    "            avg_co_events = num_co_events.loc[tIn:tOut].mean()\n",
    "            wght.loc[tIn] = 1.0 / avg_co_events if avg_co_events > 0 else 0\n",
    "        \n",
    "        self.uniqueness = wght\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å”¯ä¸€æ€§çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½äº‹ä»¶æ•¸: {len(wght):,}\")\n",
    "        print(f\"å¹³å‡å”¯ä¸€æ€§: {wght.mean():.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¡¡é‡æ¯å€‹æ¨™ç±¤çš„éé‡ç–Šç¨‹åº¦\")\n",
    "        print(f\"  - å”¯ä¸€æ€§ = 1 / å¹³å‡ä¸¦ç™¼åº¦\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¶Šé«˜ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤è¶Šç¨ç«‹ï¼Œæ¬Šé‡æ‡‰è©²è¶Šå¤§\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¶Šä½ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤èˆ‡å…¶ä»–æ¨™ç±¤é‡ç–Šå¤šï¼Œæ¬Šé‡æ‡‰è©²è¶Šå°\")\n",
    "        print(f\"å”¯ä¸€æ€§åˆ†å¸ƒ:\")\n",
    "        print(wght.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return wght\n",
    "\n",
    "    def compute_return_attribution(self, \n",
    "                                  close: pd.Series,\n",
    "                                  t1: pd.Series,\n",
    "                                  num_co_events: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆReturn Attributionï¼‰\n",
    "        åŸºæ–¼ Snippet 4.10: mpSampleW\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ï¼Œè¨ˆç®—å…¶å­˜æ´»æœŸé–“çš„æ­¸å› å ±é…¬ï¼š\n",
    "        \n",
    "        Ìƒw_i = |Î£_{t=t_i,0}^{t_i,1} (r_{t-1,t} / c_t)|\n",
    "        \n",
    "        å…¶ä¸­ï¼š\n",
    "        - r_{t-1,t} = log(close_t / close_{t-1}) æ˜¯å°æ•¸å ±é…¬\n",
    "        - c_t æ˜¯æ™‚é–“ t çš„ä¸¦ç™¼åº¦\n",
    "        \n",
    "        ç„¶å¾Œæ¨™æº–åŒ–ï¼šw_i = Ìƒw_i Ã— I / (Î£_{j=1}^I Ìƒw_j)\n",
    "        \n",
    "        é€™æ¨£è¨­è¨ˆçš„æ„ç¾©ï¼š\n",
    "        - å°‡å ±é…¬æŒ‰ä¸¦ç™¼åº¦åˆ†é…ï¼Œé¿å…é‡ç–Šäº‹ä»¶é‡è¤‡è¨ˆç®—å ±é…¬\n",
    "        - çµ•å°å ±é…¬å¤§çš„äº‹ä»¶æ¬Šé‡æ›´é«˜\n",
    "        - æ¨™æº–åŒ–å¾Œæ¬Šé‡ç¸½å’Œç‚º Iï¼ˆæ¨£æœ¬æ•¸ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“\n",
    "        num_co_events : pd.Series\n",
    "            ä¸¦ç™¼åº¦åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        return_weights : pd.Series\n",
    "            å ±é…¬æ­¸å› æ¬Šé‡\n",
    "        \"\"\"\n",
    "        ret = np.log(close).diff()  # å°æ•¸å ±é…¬\n",
    "        wght = pd.Series(index=t1.index)\n",
    "        \n",
    "        for tIn, tOut in t1.items():\n",
    "            # è¨ˆç®—æ­¸å› å ±é…¬ï¼šå ±é…¬ / ä¸¦ç™¼åº¦\n",
    "            attributed_ret = (ret.loc[tIn:tOut] / \n",
    "                            num_co_events.loc[tIn:tOut]).sum()\n",
    "            wght.loc[tIn] = abs(attributed_ret)\n",
    "        \n",
    "        # æ¨™æº–åŒ–ï¼šæ¬Šé‡ç¸½å’Œ = æ¨£æœ¬æ•¸\n",
    "        wght = wght * len(wght) / wght.sum()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å ±é…¬æ­¸å› æ¬Šé‡çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½äº‹ä»¶æ•¸: {len(wght):,}\")\n",
    "        print(f\"å¹³å‡æ¬Šé‡: {wght.mean():.4f}\")\n",
    "        print(f\"æ¬Šé‡ç¸½å’Œ: {wght.sum():.0f} (æ‡‰ç­‰æ–¼æ¨£æœ¬æ•¸)\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - æ ¹æ“šæ­¸å› çš„çµ•å°å ±é…¬ä¾†åŠ æ¬Š\")\n",
    "        print(f\"  - å ±é…¬æŒ‰ä¸¦ç™¼åº¦åˆ†é…ï¼Œé¿å…é‡ç–Šäº‹ä»¶é‡è¤‡è¨ˆç®—\")\n",
    "        print(f\"  - çµ•å°å ±é…¬å¤§çš„äº‹ä»¶æ¬Šé‡æ›´é«˜\")\n",
    "        print(f\"æ¬Šé‡åˆ†å¸ƒ:\")\n",
    "        print(wght.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return wght\n",
    "\n",
    "    def compute_class_balance_weight(self, labels: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹é¡åˆ¥ï¼Œè¨ˆç®—å…¶æ¬Šé‡ç‚ºï¼š\n",
    "        w_class = n_samples / (n_classes Ã— n_class_samples)\n",
    "        \n",
    "        é€™æ¨£å¯ä»¥è®“å°‘æ•¸é¡åˆ¥çš„æ¨£æœ¬æ¬Šé‡æ›´é«˜ï¼Œå¹³è¡¡é¡åˆ¥åˆ†å¸ƒã€‚\n",
    "        \n",
    "        ä½¿ç”¨ sklearn çš„ compute_sample_weight('balanced', labels) å¯¦ç¾ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        labels : pd.Series\n",
    "            æ¨™ç±¤åºåˆ—ï¼ˆindex å¿…é ˆèˆ‡æ¨£æœ¬æ¬Šé‡çš„ index ä¸€è‡´ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        class_weights : pd.Series\n",
    "            æ¯å€‹æ¨£æœ¬çš„é¡åˆ¥å¹³è¡¡æ¬Šé‡\n",
    "        \"\"\"\n",
    "        from sklearn.utils.class_weight import compute_sample_weight\n",
    "        \n",
    "        # è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡\n",
    "        class_weights = compute_sample_weight('balanced', labels)\n",
    "        class_weights_series = pd.Series(class_weights, index=labels.index)\n",
    "        \n",
    "        self.class_balance_weights = class_weights_series\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š é¡åˆ¥å¹³è¡¡æ¬Šé‡çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "        label_counts = labels.value_counts().sort_index()\n",
    "        print(label_counts)\n",
    "        print(f\"\\né¡åˆ¥å¹³è¡¡æ¬Šé‡:\")\n",
    "        for label in sorted(labels.unique()):\n",
    "            mask = labels == label\n",
    "            avg_weight = class_weights_series[mask].mean()\n",
    "            count = mask.sum()\n",
    "            pct = count / len(labels) * 100\n",
    "            print(f\"  æ¨™ç±¤ {label}: æ¨£æœ¬æ•¸ = {count:,} ({pct:.2f}%), å¹³å‡æ¬Šé‡ = {avg_weight:.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - é¡åˆ¥å¹³è¡¡æ¬Šé‡ç”¨æ–¼å¹³è¡¡ä¸åŒé¡åˆ¥çš„æ¨£æœ¬åˆ†å¸ƒ\")\n",
    "        print(f\"  - å°‘æ•¸é¡åˆ¥çš„æ¨£æœ¬æœƒç²å¾—æ›´é«˜çš„æ¬Šé‡\")\n",
    "        print(f\"  - å¤šæ•¸é¡åˆ¥çš„æ¨£æœ¬æœƒç²å¾—è¼ƒä½çš„æ¬Šé‡\")\n",
    "        print(f\"  - æ¬Šé‡è¨ˆç®—å…¬å¼: w = n_samples / (n_classes Ã— n_class_samples)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return class_weights_series\n",
    "\n",
    "    def get_time_decay_linear(self, tW: pd.Series, \n",
    "                              clf_last_w: float = 1.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        ç·šæ€§æ™‚é–“è¡°æ¸›ï¼ˆåŸºæ–¼ Snippet 4.11: getTimeDecayï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        æ‡‰ç”¨åˆ†æ®µç·šæ€§è¡°æ¸›åˆ°è§€æ¸¬çš„å”¯ä¸€æ€§ï¼ˆtWï¼‰ï¼š\n",
    "        \n",
    "        d[x] = max{0, a + bx}\n",
    "        \n",
    "        é‚Šç•Œæ¢ä»¶ï¼š\n",
    "        1. d[Î£_{i=1}^I Å«_i] = 1  ï¼ˆæœ€æ–°æ¨£æœ¬æ¬Šé‡ç‚º 1ï¼‰\n",
    "        2. d[0] = clf_last_w    ï¼ˆæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º clf_last_wï¼‰\n",
    "        \n",
    "        æ±‚è§£ï¼š\n",
    "        a = 1 - b Ã— Î£_{i=1}^I Å«_i\n",
    "        b = (1 - clf_last_w) / Î£_{i=1}^I Å«_i  ï¼ˆç•¶ clf_last_w >= 0ï¼‰\n",
    "        \n",
    "        æ³¨æ„ï¼šæ™‚é–“è¡°æ¸›æ˜¯åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ x âˆˆ [0, Î£_{i=1}^I Å«_i]ï¼Œ\n",
    "        è€Œéæ™‚é–“é †åºï¼Œå› ç‚ºåœ¨å­˜åœ¨å†—é¤˜è§€æ¸¬å€¼çš„æƒ…æ³ä¸‹ï¼ŒæŒ‰æ™‚é–“é †åº\n",
    "        è¡°æ¸›æœƒä½¿æ¬Šé‡é™ä½å¤ªå¿«ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        tW : pd.Series\n",
    "            å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé€šå¸¸æ˜¯ uniquenessï¼‰\n",
    "        clf_last_w : float\n",
    "            æœ€èˆŠæ¨£æœ¬çš„æ¬Šé‡ï¼ˆé è¨­ 1.0ï¼Œè¡¨ç¤ºä¸è¡°æ¸›ï¼‰\n",
    "            - c = 1: ç„¡æ™‚é–“è¡°æ¸›\n",
    "            - 0 < c < 1: ç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\n",
    "            - c = 0: æœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "            - c < 0: æœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        time_decay : pd.Series\n",
    "            æ™‚é–“è¡°æ¸›æ¬Šé‡\n",
    "        \"\"\"\n",
    "        clfW = tW.sort_index().cumsum()\n",
    "        \n",
    "        if clfW.iloc[-1] > 0:\n",
    "            if clf_last_w >= 0:\n",
    "                # ç·šæ€§è¡°æ¸›ï¼šd[0] = clf_last_w, d[Î£Å«] = 1\n",
    "                slope = (1.0 - clf_last_w) / clfW.iloc[-1]\n",
    "            else:\n",
    "                # ç•¶ clf_last_w < 0 æ™‚çš„ç‰¹æ®Šè™•ç†\n",
    "                slope = 1 / ((clf_last_w + 1) * clfW.iloc[-1])\n",
    "        else:\n",
    "            slope = 0\n",
    "        \n",
    "        const = 1.0 - slope * clfW.iloc[-1]\n",
    "        clfW = const + slope * clfW\n",
    "        clfW[clfW < 0] = 0\n",
    "        \n",
    "        self.time_decay = clfW\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š ç·šæ€§æ™‚é–“è¡°æ¸›\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"æ™‚é–“è¡°æ¸›åƒæ•¸: const={const:.4f}, slope={slope:.6f}\")\n",
    "        print(f\"æœ€èˆŠæ¨£æœ¬æ¬Šé‡ (c): {clf_last_w:.4f}\")\n",
    "        print(f\"æœ€æ–°æ¨£æœ¬æ¬Šé‡: 1.0\")\n",
    "        print(f\"å¹³å‡æ™‚é–“è¡°æ¸›æ¬Šé‡: {clfW.mean():.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        if clf_last_w == 1.0:\n",
    "            print(f\"  - c = 1: ç„¡æ™‚é–“è¡°æ¸›\")\n",
    "        elif 0 < clf_last_w < 1:\n",
    "            print(f\"  - 0 < c < 1: ç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\")\n",
    "        elif clf_last_w == 0:\n",
    "            print(f\"  - c = 0: æœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\")\n",
    "        elif clf_last_w < 0:\n",
    "            print(f\"  - c < 0: æœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\")\n",
    "        print(f\"  - è¡°æ¸›åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ï¼Œè€Œéæ™‚é–“é †åº\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return clfW\n",
    "    \n",
    "    def get_time_decay_exp(self, tW: pd.Series, \n",
    "                          decay_rate: float = 1.0,\n",
    "                          percent_of_zero_wts: float = 0.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        æŒ‡æ•¸æ™‚é–“è¡°æ¸›\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        ä½¿ç”¨æŒ‡æ•¸å‡½æ•¸é€²è¡Œæ™‚é–“è¡°æ¸›ï¼š\n",
    "        \n",
    "        d[x] = exp((decay_rate - 1) Ã— (Î£Å« - x))\n",
    "        \n",
    "        å…¶ä¸­ x æ˜¯ç´¯ç©å”¯ä¸€æ€§ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        tW : pd.Series\n",
    "            å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé€šå¸¸æ˜¯ uniquenessï¼‰\n",
    "        decay_rate : float\n",
    "            è¡°æ¸›ç‡ï¼ˆ>1 è¡¨ç¤ºè¡°æ¸›æ›´å¿«ï¼Œ<1 è¡¨ç¤ºè¡°æ¸›æ›´æ…¢ï¼‰\n",
    "        percent_of_zero_wts : float\n",
    "            æœ€èˆŠæ¨£æœ¬ä¸­è¨­ç‚º 0 çš„æ¯”ä¾‹\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        time_decay : pd.Series\n",
    "            æ™‚é–“è¡°æ¸›æ¬Šé‡\n",
    "        \"\"\"\n",
    "        clf_w = tW.sort_index().cumsum()\n",
    "        last_value = clf_w.iloc[-1]\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ•¸è¡°æ¸›æ¬Šé‡\n",
    "        out_wts = []\n",
    "        zero_threshold = int(round(len(clf_w) * percent_of_zero_wts))\n",
    "        \n",
    "        for i in range(len(clf_w)):\n",
    "            if i < zero_threshold:\n",
    "                out_wts.append(0.0)\n",
    "            else:\n",
    "                # æŒ‡æ•¸è¡°æ¸›ï¼šexp((decay_rate - 1) * (last_value - current_value))\n",
    "                decay_factor = np.exp((decay_rate - 1.0) * (last_value - clf_w.iloc[i]))\n",
    "                out_wts.append(decay_factor)\n",
    "        \n",
    "        time_decay = pd.Series(out_wts, index=clf_w.index)\n",
    "        self.time_decay = time_decay\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æŒ‡æ•¸æ™‚é–“è¡°æ¸›çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"è¡°æ¸›ç‡ (decay_rate): {decay_rate}\")\n",
    "        print(f\"é›¶æ¬Šé‡æ¨£æœ¬æ¯”ä¾‹: {percent_of_zero_wts:.2%}\")\n",
    "        print(f\"å¹³å‡æ™‚é–“è¡°æ¸›æ¬Šé‡: {time_decay.mean():.4f}\")\n",
    "        print(f\"æ™‚é–“è¡°æ¸›åˆ†å¸ƒ:\")\n",
    "        print(time_decay.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return time_decay\n",
    "    \n",
    "    def compute_sample_weights(self, \n",
    "                              t1: pd.Series,\n",
    "                              close_idx: pd.DatetimeIndex,\n",
    "                              labels: Optional[pd.Series] = None,\n",
    "                              close: Optional[pd.Series] = None,\n",
    "                              use_uniqueness: bool = True,\n",
    "                              use_time_decay: bool = True,\n",
    "                              use_return_attribution: bool = False,\n",
    "                              use_class_balance: bool = False,\n",
    "                              time_decay_type: str = 'linear',\n",
    "                              time_decay_params: dict = None) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æœ€çµ‚æ¨£æœ¬æ¬Šé‡ï¼ˆå¯é¸æ“‡ä½¿ç”¨å“ªäº›æ©Ÿåˆ¶ï¼‰\n",
    "        \n",
    "        æ¬Šé‡çµ„åˆæ–¹å¼ï¼š\n",
    "        ------------\n",
    "        1. åŸºç¤æ¬Šé‡ = å”¯ä¸€æ€§ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        2. å¯é¸ï¼šÃ— å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        3. å¯é¸ï¼šÃ— æ™‚é–“è¡°æ¸›ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        4. å¯é¸ï¼šÃ— é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        \n",
    "        æœ€çµ‚æ¬Šé‡ = uniqueness Ã— (return_attribution) Ã— (time_decay) Ã— (class_balance)\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "        close_idx : pd.DatetimeIndex\n",
    "            åƒ¹æ ¼æ•¸æ“šçš„ç´¢å¼•\n",
    "        labels : pd.Series, optional\n",
    "            æ¨™ç±¤åºåˆ—ï¼ˆç”¨æ–¼è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼Œindex å¿…é ˆèˆ‡ t1 ä¸€è‡´ï¼‰\n",
    "        close : pd.Series, optional\n",
    "            åƒ¹æ ¼åºåˆ—ï¼ˆç”¨æ–¼è¨ˆç®—å ±é…¬æ­¸å› ï¼‰\n",
    "        use_uniqueness : bool\n",
    "            æ˜¯å¦ä½¿ç”¨å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_time_decay : bool\n",
    "            æ˜¯å¦ä½¿ç”¨æ™‚é–“è¡°æ¸›ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_return_attribution : bool\n",
    "            æ˜¯å¦ä½¿ç”¨å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆé è¨­ Falseï¼‰\n",
    "        use_class_balance : bool\n",
    "            æ˜¯å¦ä½¿ç”¨é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼ˆé è¨­ Falseï¼‰\n",
    "        time_decay_type : str\n",
    "            æ™‚é–“è¡°æ¸›é¡å‹ï¼š'linear' æˆ– 'exp'ï¼ˆé è¨­ 'linear'ï¼‰\n",
    "        time_decay_params : dict, optional\n",
    "            æ™‚é–“è¡°æ¸›åƒæ•¸\n",
    "            - linear: {'clf_last_w': 0.5}\n",
    "            - exp: {'decay_rate': 1.2, 'percent_of_zero_wts': 0.0}\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        sample_weights : pd.Series\n",
    "            æœ€çµ‚æ¨£æœ¬æ¬Šé‡ï¼ˆå¦‚æœéƒ½ä¸ä½¿ç”¨ï¼Œè¿”å›å…¨ç‚º 1 çš„æ¬Šé‡ï¼‰\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡è¨ˆç®—é…ç½®\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ä½¿ç”¨å”¯ä¸€æ€§: {use_uniqueness}\")\n",
    "        print(f\"ä½¿ç”¨å ±é…¬æ­¸å› : {use_return_attribution}\")\n",
    "        print(f\"ä½¿ç”¨æ™‚é–“è¡°æ¸›: {use_time_decay}\")\n",
    "        print(f\"ä½¿ç”¨é¡åˆ¥å¹³è¡¡: {use_class_balance}\")\n",
    "        if use_time_decay:\n",
    "            print(f\"æ™‚é–“è¡°æ¸›é¡å‹: {time_decay_type}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # å¦‚æœéƒ½ä¸ä½¿ç”¨ï¼Œè¿”å›å‡å‹»æ¬Šé‡\n",
    "        if not use_uniqueness and not use_time_decay and not use_return_attribution and not use_class_balance:\n",
    "            sample_weights = pd.Series(1.0, index=t1.index)\n",
    "            self.sample_weights = sample_weights\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡çµ±è¨ˆï¼ˆå‡å‹»æ¬Šé‡ï¼‰\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"ç¸½æ¨£æœ¬æ•¸: {len(sample_weights):,}\")\n",
    "            print(f\"æ‰€æœ‰æ¨£æœ¬æ¬Šé‡: 1.0\")\n",
    "            print(\"=\" * 60)\n",
    "            return sample_weights\n",
    "        \n",
    "        # è¨ˆç®—ä¸¦ç™¼åº¦ï¼ˆå”¯ä¸€æ€§å’Œå ±é…¬æ­¸å› éƒ½éœ€è¦ï¼‰\n",
    "        if use_uniqueness or use_return_attribution:\n",
    "            num_co_events = self.compute_num_co_events(close_idx, t1)\n",
    "        else:\n",
    "            num_co_events = None\n",
    "        \n",
    "        # è¨ˆç®—å”¯ä¸€æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_uniqueness:\n",
    "            uniqueness = self.compute_uniqueness(t1, num_co_events)\n",
    "        else:\n",
    "            uniqueness = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨å”¯ä¸€æ€§ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—å ±é…¬æ­¸å› ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_return_attribution:\n",
    "            if close is None:\n",
    "                raise ValueError(\"ä½¿ç”¨å ±é…¬æ­¸å› æ™‚éœ€è¦æä¾› close åƒæ•¸\")\n",
    "            return_weights = self.compute_return_attribution(close, t1, num_co_events)\n",
    "        else:\n",
    "            return_weights = pd.Series(1.0, index=t1.index)\n",
    "        \n",
    "        # è¨ˆç®—æ™‚é–“è¡°æ¸›ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_time_decay:\n",
    "            if time_decay_params is None:\n",
    "                time_decay_params = {}\n",
    "            \n",
    "            if time_decay_type == 'linear':\n",
    "                clf_last_w = time_decay_params.get('clf_last_w', 1.0)\n",
    "                time_decay = self.get_time_decay_linear(uniqueness, clf_last_w)\n",
    "            elif time_decay_type == 'exp':\n",
    "                decay_rate = time_decay_params.get('decay_rate', 1.0)\n",
    "                percent_of_zero_wts = time_decay_params.get('percent_of_zero_wts', 0.0)\n",
    "                time_decay = self.get_time_decay_exp(uniqueness, decay_rate, percent_of_zero_wts)\n",
    "            else:\n",
    "                raise ValueError(f\"ä¸æ”¯æ´çš„æ™‚é–“è¡°æ¸›é¡å‹: {time_decay_type}\")\n",
    "        else:\n",
    "            time_decay = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨æ™‚é–“è¡°æ¸›ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_class_balance:\n",
    "            if labels is None:\n",
    "                raise ValueError(\"ä½¿ç”¨é¡åˆ¥å¹³è¡¡æ™‚éœ€è¦æä¾› labels åƒæ•¸\")\n",
    "            # ç¢ºä¿ labels çš„ index èˆ‡ t1 ä¸€è‡´\n",
    "            if not labels.index.equals(t1.index):\n",
    "                # å˜—è©¦å°é½Š\n",
    "                common_idx = labels.index.intersection(t1.index)\n",
    "                if len(common_idx) == 0:\n",
    "                    raise ValueError(\"labels å’Œ t1 çš„ index æ²’æœ‰äº¤é›†\")\n",
    "                labels = labels.loc[common_idx]\n",
    "                t1 = t1.loc[common_idx]\n",
    "                uniqueness = uniqueness.loc[common_idx]\n",
    "                return_weights = return_weights.loc[common_idx]\n",
    "                time_decay = time_decay.loc[common_idx]\n",
    "            class_balance_weights = self.compute_class_balance_weight(labels)\n",
    "        else:\n",
    "            class_balance_weights = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨é¡åˆ¥å¹³è¡¡ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—æœ€çµ‚æ¬Šé‡ï¼ˆæ‰€æœ‰æ¬Šé‡ç›¸ä¹˜ï¼‰\n",
    "        common_idx = (uniqueness.index\n",
    "                     .intersection(time_decay.index)\n",
    "                     .intersection(return_weights.index)\n",
    "                     .intersection(class_balance_weights.index))\n",
    "        \n",
    "        sample_weights = (uniqueness.loc[common_idx] * \n",
    "                          return_weights.loc[common_idx] * \n",
    "                          time_decay.loc[common_idx] *\n",
    "                          class_balance_weights.loc[common_idx])\n",
    "        \n",
    "        self.sample_weights = sample_weights\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡çµ±è¨ˆï¼ˆæœ€çµ‚ï¼‰\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½æ¨£æœ¬æ•¸: {len(sample_weights):,}\")\n",
    "        print(f\"å¹³å‡æ¬Šé‡: {sample_weights.mean():.4f}\")\n",
    "        print(f\"æ¬Šé‡åˆ†å¸ƒ:\")\n",
    "        print(sample_weights.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return sample_weights\n",
    "    \n",
    "    def plot_concurrency_vs_volatility(self, daily_vol: pd.Series):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ä¸¦ç™¼åº¦ vs æ³¢å‹•ç‡çš„é—œä¿‚åœ–\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        daily_vol : pd.Series\n",
    "            æ—¥æ³¢å‹•ç‡åºåˆ—\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        if self.num_co_events is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_num_co_events()\")\n",
    "            return\n",
    "        \n",
    "        # å°é½Šæ•¸æ“šï¼ˆä½¿ç”¨åŸå§‹æ™‚é–“é»ï¼‰\n",
    "        to_plot = pd.DataFrame({\n",
    "            'conc_events': self.num_co_events,\n",
    "            'daily_vol': daily_vol\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(to_plot) == 0:\n",
    "            print(\"âš ï¸ å°é½Šå¾Œç„¡æœ‰æ•ˆæ•¸æ“š\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # 1. æ™‚é–“åºåˆ—åœ–\n",
    "        ax1 = axes[0]\n",
    "        ax1_twin = ax1.twinx()\n",
    "        \n",
    "        line1 = ax1.plot(to_plot.index, to_plot['conc_events'], \n",
    "                        label='Concurrent Events', color='blue', linewidth=1.5)\n",
    "        line2 = ax1_twin.plot(to_plot.index, to_plot['daily_vol'], \n",
    "                            label='Daily Volatility', color='red', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        ax1.set_xlabel('Time', fontsize=12)\n",
    "        ax1.set_ylabel('Concurrent Events', color='blue', fontsize=12)\n",
    "        ax1_twin.set_ylabel('Daily Volatility', color='red', fontsize=12)\n",
    "        ax1.set_title('Concurrent Events vs Daily Volatility', fontsize=14)\n",
    "        ax1.tick_params(axis='y', labelcolor='blue')\n",
    "        ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1_twin.legend(loc='upper right')\n",
    "        \n",
    "        # 2. æ•£é»åœ–\n",
    "        sns.regplot(x=to_plot['conc_events'], y=to_plot['daily_vol'], \n",
    "                ax=axes[1], scatter_kws={'alpha': 0.6, 's': 50})\n",
    "        axes[1].set_xlabel('Concurrent Events', fontsize=12)\n",
    "        axes[1].set_ylabel('Daily Volatility', fontsize=12)\n",
    "        axes[1].set_title('Concurrent Events vs Daily Volatility (Scatter Plot)', fontsize=14)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ‰“å°ç›¸é—œæ€§\n",
    "        correlation = to_plot['conc_events'].corr(to_plot['daily_vol'])\n",
    "        print(f\"\\nğŸ“Š ä¸¦ç™¼åº¦èˆ‡æ³¢å‹•ç‡çš„ç›¸é—œä¿‚æ•¸: {correlation:.4f}\")\n",
    "    \n",
    "    def plot_weights_distribution(self):\n",
    "        \"\"\"ç¹ªè£½æ¬Šé‡åˆ†å¸ƒåœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.sample_weights is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_sample_weights()\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. å”¯ä¸€æ€§åˆ†å¸ƒ\n",
    "        if self.uniqueness is not None:\n",
    "            self.uniqueness.hist(bins=50, ax=axes[0, 0], alpha=0.7, color='blue')\n",
    "            axes[0, 0].set_title('Uniqueness Distribution')\n",
    "            axes[0, 0].set_xlabel('Uniqueness')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. æ™‚é–“è¡°æ¸›åˆ†å¸ƒ\n",
    "        if self.time_decay is not None:\n",
    "            self.time_decay.hist(bins=50, ax=axes[0, 1], alpha=0.7, color='green')\n",
    "            axes[0, 1].set_title('Time Decay Distribution')\n",
    "            axes[0, 1].set_xlabel('Time Decay Weight')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. æ¨£æœ¬æ¬Šé‡åˆ†å¸ƒ\n",
    "        self.sample_weights.hist(bins=50, ax=axes[1, 0], alpha=0.7, color='orange')\n",
    "        axes[1, 0].set_title('Sample Weights Distribution')\n",
    "        axes[1, 0].set_xlabel('Sample Weight')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. æ¬Šé‡éš¨æ™‚é–“è®ŠåŒ–\n",
    "        if len(self.sample_weights) > 0:\n",
    "            sorted_weights = self.sample_weights.sort_index()\n",
    "            axes[1, 1].plot(sorted_weights.index, sorted_weights.values, \n",
    "                           linewidth=1, alpha=0.7, color='purple')\n",
    "            axes[1, 1].set_title('Sample Weights Over Time')\n",
    "            axes[1, 1].set_xlabel('Time')\n",
    "            axes[1, 1].set_ylabel('Sample Weight')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_uniqueness_autocorr(self, lag: int = 1) -> float:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å”¯ä¸€æ€§çš„åºåˆ—ç›¸é—œæ€§ï¼ˆAR(1)ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        lag : int\n",
    "            æ»¯å¾ŒæœŸæ•¸ï¼ˆé è¨­ 1ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        autocorr : float\n",
    "            åºåˆ—ç›¸é—œæ€§\n",
    "        \"\"\"\n",
    "        if self.uniqueness is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_uniqueness()\")\n",
    "            return None\n",
    "        \n",
    "        autocorr = self.uniqueness.autocorr(lag=lag)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å”¯ä¸€æ€§åºåˆ—ç›¸é—œæ€§\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"AR({lag}) è‡ªç›¸é—œä¿‚æ•¸: {autocorr:.4f}\")\n",
    "        \n",
    "        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "        n = len(self.uniqueness)\n",
    "        critical_value = 1.96 / np.sqrt(n)  # 95% ä¿¡å¿ƒæ°´æº–\n",
    "        \n",
    "        if abs(autocorr) > critical_value:\n",
    "            print(f\"âœ… çµ±è¨ˆé¡¯è‘— (|{autocorr:.4f}| > {critical_value:.4f})\")\n",
    "            print(f\"\\nè§£é‡‹:\")\n",
    "            print(f\"  - å”¯ä¸€æ€§å­˜åœ¨åºåˆ—ç›¸é—œæ€§ï¼Œè¡¨ç¤ºå¸‚å ´ç‹€æ…‹å…·æœ‰æŒçºŒæ€§\")\n",
    "            print(f\"  - é€™åœ¨é‡‘èæ•¸æ“šä¸­æ˜¯å¸¸è¦‹ä¸”é æœŸçš„ç¾è±¡\")\n",
    "            print(f\"  - æ™‚é–“è¡°æ¸›æ©Ÿåˆ¶æœ‰åŠ©æ–¼é™ä½èˆŠæ•¸æ“šçš„å½±éŸ¿\")\n",
    "        else:\n",
    "            print(f\"âŒ çµ±è¨ˆä¸é¡¯è‘— (|{autocorr:.4f}| <= {critical_value:.4f})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return autocorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# å®Œæ•´ç¯„ä¾‹ï¼šæ¨£æœ¬æ¬Šé‡è¨ˆç®—ï¼ˆæ‰€æœ‰é¸é …ï¼‰\n",
    "# =====================================================\n",
    "\n",
    "sample_weight = SampleWeight()\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … A: æ¨™æº–é…ç½®ï¼ˆå”¯ä¸€æ€§ + ç·šæ€§æ™‚é–“è¡°æ¸›ï¼‰\n",
    "# =====================================================\n",
    "final_weights_A = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=True,\n",
    "    use_time_decay=True,\n",
    "    time_decay_type='linear',\n",
    "    labels=bins['bin'],  # âœ… åŠ å…¥ labels åƒæ•¸\n",
    "    use_class_balance=True,  # âœ… å•Ÿç”¨é¡åˆ¥å¹³è¡¡\n",
    "    time_decay_params={'clf_last_w': 0.5}\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … B: å®Œæ•´é…ç½®ï¼ˆå”¯ä¸€æ€§ + å ±é…¬æ­¸å›  + æ™‚é–“è¡°æ¸›ï¼‰\n",
    "# =====================================================\n",
    "final_weights_B = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    close=nas100_raw['Close'],\n",
    "    use_uniqueness=True,\n",
    "    use_time_decay=True,\n",
    "    use_return_attribution=True,  # åŠ å…¥å ±é…¬æ­¸å› \n",
    "    labels=bins['bin'],  # âœ… åŠ å…¥ labels åƒæ•¸\n",
    "    use_class_balance=True,  # âœ… å•Ÿç”¨é¡åˆ¥å¹³è¡¡\n",
    "    time_decay_type='linear',\n",
    "    time_decay_params={'clf_last_w': 0.5}\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … C: æŒ‡æ•¸æ™‚é–“è¡°æ¸›\n",
    "# =====================================================\n",
    "final_weights_C = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=False,\n",
    "    use_time_decay=False,\n",
    "    labels=bins['bin'],  # âœ… åŠ å…¥ labels åƒæ•¸\n",
    "    use_class_balance=True,  # âœ… å•Ÿç”¨é¡åˆ¥å¹³è¡¡\n",
    "    time_decay_type='linear',\n",
    "    time_decay_params={'clf_last_w': 0.5}\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … D: åªä½¿ç”¨å”¯ä¸€æ€§ï¼ˆæœ€ç°¡å–®ï¼‰\n",
    "# =====================================================\n",
    "final_weights_D = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=True,\n",
    "    use_time_decay=False,\n",
    "    labels=bins['bin'],  # âœ… åŠ å…¥ labels åƒæ•¸\n",
    "    use_class_balance=True,  # âœ… å•Ÿç”¨é¡åˆ¥å¹³è¡¡\n",
    "    use_return_attribution=False\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … E: åªä½¿ç”¨å ±é…¬æ­¸å› \n",
    "# =====================================================\n",
    "final_weights_E = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    close=nas100_raw['Close'],\n",
    "    use_uniqueness=False,\n",
    "    use_time_decay=False,\n",
    "    labels=bins['bin'],  # âœ… åŠ å…¥ labels åƒæ•¸\n",
    "    use_class_balance=True,  # âœ… å•Ÿç”¨é¡åˆ¥å¹³è¡¡\n",
    "    use_return_attribution=True\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# é¸é … F: å‡å‹»æ¬Šé‡ï¼ˆåŸºæº–å°ç…§ï¼‰\n",
    "# =====================================================\n",
    "final_weights_F = sample_weight.compute_sample_weights(\n",
    "    t1=events['t1'],\n",
    "    close_idx=nas100_raw['Close'].index,\n",
    "    use_uniqueness=False,\n",
    "    use_time_decay=False,\n",
    "    labels=bins['bin'],  # âœ… åŠ å…¥ labels åƒæ•¸\n",
    "    use_class_balance=True,  # âœ… å•Ÿç”¨é¡åˆ¥å¹³è¡¡\n",
    "    use_return_attribution=False\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# æ¯”è¼ƒä¸åŒé…ç½®çš„æ¬Šé‡çµ±è¨ˆ\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ä¸åŒé…ç½®çš„æ¬Šé‡æ¯”è¼ƒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'A: å”¯ä¸€æ€§+ç·šæ€§è¡°æ¸›': final_weights_A.describe(),\n",
    "    'B: å”¯ä¸€æ€§+å ±é…¬æ­¸å› +ç·šæ€§è¡°æ¸›': final_weights_B.describe(),\n",
    "    'C: å”¯ä¸€æ€§+æŒ‡æ•¸è¡°æ¸›': final_weights_C.describe(),\n",
    "    'D: åªå”¯ä¸€æ€§': final_weights_D.describe(),\n",
    "    'E: åªå ±é…¬æ­¸å› ': final_weights_E.describe(),\n",
    "    'F: å‡å‹»æ¬Šé‡': final_weights_F.describe()\n",
    "})\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "# =====================================================\n",
    "# è¦–è¦ºåŒ–æ¯”è¼ƒ\n",
    "# =====================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. æ¬Šé‡åˆ†å¸ƒæ¯”è¼ƒ\n",
    "axes[0, 0].hist(final_weights_A, bins=50, alpha=0.5, label='A: Uniqueness+Linear', color='blue')\n",
    "axes[0, 0].hist(final_weights_B, bins=50, alpha=0.5, label='B: å®Œæ•´é…ç½®', color='red')\n",
    "axes[0, 0].hist(final_weights_D, bins=50, alpha=0.5, label='D: åªå”¯ä¸€æ€§', color='green')\n",
    "axes[0, 0].set_title('Weight Distribution Comparison')\n",
    "axes[0, 0].set_xlabel('Sample Weight')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. æ¬Šé‡éš¨æ™‚é–“è®ŠåŒ–\n",
    "sorted_A = final_weights_A.sort_index()\n",
    "axes[0, 1].plot(sorted_A.index, sorted_A.values, label='A: å”¯ä¸€æ€§+ç·šæ€§', alpha=0.7)\n",
    "axes[0, 1].set_title('Weights Over Time')\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Weight')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. æ¬Šé‡åˆ†ä½æ•¸æ¯”è¼ƒ\n",
    "weights_list = [final_weights_A, final_weights_B, final_weights_D]\n",
    "labels_list = ['A: å”¯ä¸€æ€§+ç·šæ€§', 'B: å®Œæ•´é…ç½®', 'D: åªå”¯ä¸€æ€§']\n",
    "percentiles = [25, 50, 75, 90, 95]\n",
    "\n",
    "for i, (w, label) in enumerate(zip(weights_list, labels_list)):\n",
    "    values = [w.quantile(p/100) for p in percentiles]\n",
    "    axes[1, 0].plot(percentiles, values, marker='o', label=label)\n",
    "\n",
    "axes[1, 0].set_title('Weight Percentiles Comparison')\n",
    "axes[1, 0].set_xlabel('Percentile')\n",
    "axes[1, 0].set_ylabel('Weight Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. æ¬Šé‡çµ±è¨ˆæ‘˜è¦\n",
    "stats_data = {\n",
    "    'Mean': [w.mean() for w in weights_list],\n",
    "    'Std': [w.std() for w in weights_list],\n",
    "    'Min': [w.min() for w in weights_list],\n",
    "    'Max': [w.max() for w in weights_list]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats_data, index=labels_list)\n",
    "stats_df.plot(kind='bar', ax=axes[1, 1], alpha=0.7)\n",
    "axes[1, 1].set_title('Weight Statistics Comparison')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =====================================================\n",
    "# æœ€çµ‚é¸æ“‡ï¼ˆæ ¹æ“šéœ€æ±‚é¸æ“‡ä¸€å€‹ï¼‰\n",
    "# =====================================================\n",
    "# æ¨è–¦ï¼šä½¿ç”¨é¸é … A æˆ– B\n",
    "final_weights = final_weights_B  # æˆ– final_weights_B\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… æœ€çµ‚é¸æ“‡çš„æ¬Šé‡é…ç½®\")\n",
    "print(\"=\"*60)\n",
    "print(f\"é…ç½®: å”¯ä¸€æ€§ + ç·šæ€§æ™‚é–“è¡°æ¸›\")\n",
    "print(f\"æ¨£æœ¬æ•¸: {len(final_weights):,}\")\n",
    "print(f\"å¹³å‡æ¬Šé‡: {final_weights.mean():.4f}\")\n",
    "print(f\"æ¬Šé‡ç¯„åœ: [{final_weights.min():.4f}, {final_weights.max():.4f}]\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f61e80",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ5 ï¼š åˆ†éšå·®å¹³ç©©åƒ¹æ ¼ç‰¹å¾µæå–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7e886",
   "metadata": {},
   "source": [
    "ä¸»å•†å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# å®Œæ•´ä»£ç¢¼ï¼šç‚ºæ‰€æœ‰å•†å“ç”¢ç”Ÿåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# 2. ä¿®æ­£å¾Œçš„ FractionalDiff classï¼ˆç¢ºä¿ä¿ç•™æ‰€æœ‰ indexï¼‰\n",
    "# =====================================================\n",
    "\n",
    "class FractionalDiff:\n",
    "    \"\"\"åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimal_d = None\n",
    "        self.fracdiff_series = None\n",
    "        self.features = None\n",
    "        \n",
    "    def get_weights(self, d: float, size: int) -> np.ndarray:\n",
    "        \"\"\"è¨ˆç®—æ¬Šé‡ï¼ˆæ“´å±•çª—å£ï¼‰\"\"\"\n",
    "        w = [1.0]\n",
    "        for k in range(1, size):\n",
    "            w_ = -w[-1] / k * (d - k + 1)\n",
    "            w.append(float(w_))\n",
    "        return np.array(w[::-1]).reshape(-1, 1)\n",
    "    \n",
    "    def get_weights_FFD(self, d: float, thres: float = 1e-5) -> np.ndarray:\n",
    "        \"\"\"è¨ˆç®— FFD æ¬Šé‡ï¼ˆå›ºå®šçª—å£ï¼‰\"\"\"\n",
    "        w = [1.0]\n",
    "        k = 1\n",
    "        while True:\n",
    "            w_ = -w[-1] / k * (d - k + 1)\n",
    "            if abs(w_) < thres:\n",
    "                break\n",
    "            w.append(float(w_))\n",
    "            k += 1\n",
    "            if k > 10000:\n",
    "                break\n",
    "        return np.array(w[::-1]).reshape(-1, 1)\n",
    "    \n",
    "    def frac_diff_FFD(self, series: pd.Series, d: float, thres: float = 1e-5) -> pd.Series:\n",
    "        \"\"\"\n",
    "        åˆ†æ•¸éšå·®åˆ†ï¼ˆå›ºå®šçª—å£æ–¹æ³•ï¼ŒFFDï¼‰\n",
    "        \n",
    "        é‡è¦ï¼šä¿ç•™æ‰€æœ‰åŸå§‹ indexï¼Œåªåœ¨å‰ width å€‹é»è¨­ç‚º NaN\n",
    "        \"\"\"\n",
    "        # 1. è¨ˆç®—æ¬Šé‡ï¼ˆå›ºå®šçª—å£ï¼‰\n",
    "        w = self.get_weights_FFD(d, thres)\n",
    "        width = len(w) - 1\n",
    "        \n",
    "        if width < 1:\n",
    "            return pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        # 2. æ‡‰ç”¨æ¬Šé‡åˆ°æ•¸å€¼\n",
    "        # é‡è¦ï¼šå…ˆå¡«å…… NaNï¼Œä½†ä¿ç•™åŸå§‹ index\n",
    "        series_filled = series.fillna(method='ffill')\n",
    "        \n",
    "        # å»ºç«‹çµæœ Seriesï¼Œä½¿ç”¨åŸå§‹ series çš„å®Œæ•´ index\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        # 3. å¾ width é–‹å§‹è¨ˆç®—ï¼ˆå‰ width å€‹é»æœƒæ˜¯ NaNï¼‰\n",
    "        valid_count = 0\n",
    "        for iloc1 in range(width, len(series_filled)):\n",
    "            if iloc1 >= len(series.index):\n",
    "                break\n",
    "                \n",
    "            loc0 = series_filled.index[iloc1 - width]\n",
    "            loc1 = series_filled.index[iloc1]\n",
    "            \n",
    "            # æª¢æŸ¥åŸå§‹ series åœ¨ loc1 æ˜¯å¦æœ‰æœ‰æ•ˆå€¼\n",
    "            if loc1 not in series.index or not np.isfinite(series.loc[loc1]):\n",
    "                continue\n",
    "            \n",
    "            # ä½¿ç”¨å›ºå®šçª—å£ï¼š[X_{t-width}, X_{t-width+1}, ..., X_t]\n",
    "            window_data = series_filled.loc[loc0:loc1].values\n",
    "            \n",
    "            if len(window_data) == len(w):\n",
    "                result = np.dot(w.flatten(), window_data)\n",
    "                df_[loc1] = result\n",
    "                valid_count += 1\n",
    "        \n",
    "        return df_\n",
    "    \n",
    "    def frac_diff(self, series: pd.Series, d: float, thres: float = 0.01) -> pd.Series:\n",
    "        \"\"\"åˆ†æ•¸éšå·®åˆ†ï¼ˆæ“´å±•çª—å£ï¼‰\"\"\"\n",
    "        w = self.get_weights(d, series.shape[0])\n",
    "        w_ = np.cumsum(np.abs(w))\n",
    "        w_ /= w_[-1]\n",
    "        skip = w_[w_ > thres].shape[0]\n",
    "        \n",
    "        series_filled = series.fillna(method='ffill')\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        for iloc in range(skip, series_filled.shape[0]):\n",
    "            loc = series_filled.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc]):\n",
    "                continue\n",
    "            \n",
    "            window_data = series_filled.loc[:loc].values\n",
    "            w_subset = w[-(iloc + 1):, :].flatten()\n",
    "            \n",
    "            if len(window_data) == len(w_subset):\n",
    "                df_[loc] = np.dot(w_subset, window_data)\n",
    "        \n",
    "        return df_\n",
    "    \n",
    "    def find_optimal_d(self, series: pd.Series, \n",
    "                      d_range: Tuple[float, float] = (0, 1),\n",
    "                      d_step: float = 0.05,\n",
    "                      method: str = 'FFD',\n",
    "                      thres: float = 1e-5,\n",
    "                      target_pvalue: float = 0.05,\n",
    "                      min_corr: float = 0.5) -> dict:\n",
    "        \"\"\"æ‰¾å‡ºæœ€é©åˆçš„ d å€¼\"\"\"\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        from scipy.stats import jarque_bera\n",
    "        \n",
    "        results = []\n",
    "        d_values = np.arange(d_range[0], d_range[1] + d_step, d_step)\n",
    "        \n",
    "        for d in d_values:\n",
    "            try:\n",
    "                if method == 'FFD':\n",
    "                    fracdiff_series = self.frac_diff_FFD(series, d, thres).dropna()\n",
    "                else:\n",
    "                    fracdiff_series = self.frac_diff(series, d, thres).dropna()\n",
    "                \n",
    "                if len(fracdiff_series) < 10:\n",
    "                    continue\n",
    "                \n",
    "                adf_result = adfuller(fracdiff_series, maxlag=1, regression='c', autolag=None)\n",
    "                adf_stat = adf_result[0]\n",
    "                p_value = adf_result[1]\n",
    "                \n",
    "                common_idx = series.index.intersection(fracdiff_series.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    corr = np.corrcoef(\n",
    "                        series.loc[common_idx].values,\n",
    "                        fracdiff_series.loc[common_idx].values\n",
    "                    )[0, 1]\n",
    "                else:\n",
    "                    corr = 0\n",
    "                \n",
    "                jb_stat, jb_pvalue = jarque_bera(fracdiff_series)[:2]\n",
    "                \n",
    "                results.append({\n",
    "                    'd': d,\n",
    "                    'adf_stat': adf_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'correlation': corr,\n",
    "                    'jb_stat': jb_stat,\n",
    "                    'jb_pvalue': jb_pvalue,\n",
    "                    'is_stationary': p_value < target_pvalue,\n",
    "                    'meets_corr': corr >= min_corr,\n",
    "                    'sample_size': len(fracdiff_series)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            return None\n",
    "        \n",
    "        valid_results = results_df[\n",
    "            (results_df['is_stationary']) & \n",
    "            (results_df['meets_corr'])\n",
    "        ]\n",
    "        \n",
    "        if len(valid_results) > 0:\n",
    "            optimal = valid_results.loc[valid_results['p_value'].idxmin()]\n",
    "            self.optimal_d = optimal['d']\n",
    "        else:\n",
    "            optimal = results_df.loc[results_df['p_value'].idxmin()]\n",
    "            self.optimal_d = optimal['d']\n",
    "        \n",
    "        return {\n",
    "            'optimal_d': self.optimal_d,\n",
    "            'results': results_df,\n",
    "            'optimal_stats': optimal.to_dict()\n",
    "        }\n",
    "\n",
    "    def find_min_d_adf(self, series: pd.Series,\n",
    "                    d_range: Tuple[float, float] = (0, 1),\n",
    "                    n_points: int = 11,\n",
    "                    method: str = 'FFD',\n",
    "                    thres: float = 0.01,\n",
    "                    target_pvalue: float = 0.1) -> dict:\n",
    "        \"\"\"\n",
    "        å°‹æ‰¾æœ€å° d å€¼ï¼ˆåŸºæ–¼ Snippet 5.4ï¼‰\n",
    "        \n",
    "        é€™å€‹æ–¹æ³•èˆ‡ find_optimal_d é¡ä¼¼ï¼Œä½†å°ˆé–€ç”¨æ–¼å°‹æ‰¾æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„æœ€å° d å€¼ã€‚\n",
    "        é€™æ˜¯ AFML æ›¸ä¸­ Snippet 5.4 çš„å¯¦ç¾ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        series : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        d_range : tuple\n",
    "            d å€¼æœå°‹ç¯„åœ (min, max)\n",
    "        n_points : int\n",
    "            æ¸¬è©¦çš„ d å€¼æ•¸é‡ï¼ˆå°æ‡‰ np.linspace(d_range[0], d_range[1], n_points)ï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼ï¼ˆFFD æ–¹æ³•ï¼‰æˆ–ç´¯ç©æ¬Šé‡é–¾å€¼ï¼ˆexpanding æ–¹æ³•ï¼‰\n",
    "        target_pvalue : float\n",
    "            ç›®æ¨™ p-valueï¼ˆADF æª¢é©—ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        result : dict\n",
    "            åŒ…å«ä»¥ä¸‹éµå€¼ï¼š\n",
    "            - 'min_d': æœ€å° d å€¼ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰\n",
    "            - 'results': DataFrameï¼ŒåŒ…å«æ‰€æœ‰æ¸¬è©¦çµæœ\n",
    "            - 'min_d_stats': æœ€å° d å€¼çš„çµ±è¨ˆè³‡è¨Š\n",
    "        \"\"\"\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        \n",
    "        results = []\n",
    "        d_values = np.linspace(d_range[0], d_range[1], n_points)\n",
    "        \n",
    "        print(f\"ğŸ” æœå°‹æœ€å° d å€¼ (ç¯„åœ: {d_range}, æ¸¬è©¦é»æ•¸: {n_points})\")\n",
    "        print(f\"   æ–¹æ³•: {method}, é–¾å€¼: {thres}, ç›®æ¨™ p-value: {target_pvalue}\")\n",
    "        \n",
    "        for i, d in enumerate(d_values):\n",
    "            try:\n",
    "                # è¨ˆç®—åˆ†æ•¸éšå·®åˆ†\n",
    "                if method == 'FFD':\n",
    "                    fracdiff_series = self.frac_diff_FFD(series, d, thres).dropna()\n",
    "                else:\n",
    "                    fracdiff_series = self.frac_diff(series, d, thres).dropna()\n",
    "                \n",
    "                if len(fracdiff_series) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # ADF æª¢é©—\n",
    "                adf_result = adfuller(fracdiff_series, maxlag=1, regression='c', autolag=None)\n",
    "                adf_stat = adf_result[0]\n",
    "                p_value = adf_result[1]\n",
    "                \n",
    "                # è¨ˆç®—èˆ‡åŸå§‹åºåˆ—çš„ç›¸é—œæ€§\n",
    "                common_idx = series.index.intersection(fracdiff_series.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    corr = np.corrcoef(\n",
    "                        series.loc[common_idx].values,\n",
    "                        fracdiff_series.loc[common_idx].values\n",
    "                    )[0, 1]\n",
    "                else:\n",
    "                    corr = 0\n",
    "                \n",
    "                is_stationary = p_value < target_pvalue\n",
    "                \n",
    "                results.append({\n",
    "                    'd': d,\n",
    "                    'adf_stat': adf_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'correlation': corr,\n",
    "                    'is_stationary': is_stationary,\n",
    "                    'sample_size': len(fracdiff_series)\n",
    "                })\n",
    "                \n",
    "                if (i + 1) % 5 == 0 or i == len(d_values) - 1:\n",
    "                    print(f\"   é€²åº¦: {i+1}/{len(d_values)} (d={d:.3f}, p={p_value:.4f}, stationary={is_stationary})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçµæœ\")\n",
    "            return None\n",
    "        \n",
    "        # å°‹æ‰¾æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„æœ€å° d å€¼\n",
    "        stationary_results = results_df[results_df['is_stationary'] == True]\n",
    "        \n",
    "        if len(stationary_results) > 0:\n",
    "            min_d_row = stationary_results.loc[stationary_results['d'].idxmin()]\n",
    "            min_d = min_d_row['d']\n",
    "            self.optimal_d = min_d\n",
    "            \n",
    "            print(f\"\\nâœ… æ‰¾åˆ°æœ€å° d = {min_d:.4f}\")\n",
    "            print(f\"   ADF p-value: {min_d_row['p_value']:.4f}\")\n",
    "            print(f\"   ç›¸é—œæ€§: {min_d_row['correlation']:.4f}\")\n",
    "        else:\n",
    "            # å¦‚æœæ²’æœ‰æ»¿è¶³æ¢ä»¶çš„ï¼Œé¸æ“‡ p-value æœ€å°çš„\n",
    "            min_d_row = results_df.loc[results_df['p_value'].idxmin()]\n",
    "            min_d = min_d_row['d']\n",
    "            self.optimal_d = min_d\n",
    "            \n",
    "            print(f\"\\nâš ï¸ æœªæ‰¾åˆ°æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„ d å€¼\")\n",
    "            print(f\"   ä½¿ç”¨ p-value æœ€å°çš„ d = {min_d:.4f}\")\n",
    "            print(f\"   ADF p-value: {min_d_row['p_value']:.4f} (ç›®æ¨™: < {target_pvalue})\")\n",
    "        \n",
    "        return {\n",
    "            'min_d': min_d,\n",
    "            'results': results_df,\n",
    "            'min_d_stats': min_d_row.to_dict()\n",
    "        }\n",
    "\n",
    "    def _plot_d_search_results(self, result: dict):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ d å€¼æœå°‹çµæœ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        result : dict\n",
    "            find_min_d_adf æˆ– find_optimal_d çš„è¿”å›çµæœ\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if result is None or 'results' not in result:\n",
    "            print(\"âš ï¸ ç„¡æ•ˆçš„çµæœ\")\n",
    "            return\n",
    "        \n",
    "        results_df = result['results']\n",
    "        min_d = result.get('min_d') or result.get('optimal_d')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. ADF p-value vs d\n",
    "        axes[0, 0].plot(results_df['d'], results_df['p_value'], 'o-', linewidth=2, markersize=6)\n",
    "        if min_d is not None:\n",
    "            min_d_pvalue = results_df[results_df['d'] == min_d]['p_value'].values[0]\n",
    "            axes[0, 0].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[0, 0].scatter([min_d], [min_d_pvalue], color='red', s=100, zorder=5)\n",
    "        axes[0, 0].axhline(0.05, color='gray', linestyle=':', alpha=0.5, label='p=0.05')\n",
    "        axes[0, 0].axhline(0.1, color='gray', linestyle=':', alpha=0.5, label='p=0.1')\n",
    "        axes[0, 0].set_xlabel('d', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('ADF p-value', fontsize=12)\n",
    "        axes[0, 0].set_title('ADF Test p-value vs d', fontsize=14)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. ADF Statistic vs d\n",
    "        axes[0, 1].plot(results_df['d'], results_df['adf_stat'], 'o-', linewidth=2, markersize=6, color='green')\n",
    "        if min_d is not None:\n",
    "            min_d_stat = results_df[results_df['d'] == min_d]['adf_stat'].values[0]\n",
    "            axes[0, 1].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[0, 1].scatter([min_d], [min_d_stat], color='red', s=100, zorder=5)\n",
    "        axes[0, 1].axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "        axes[0, 1].set_xlabel('d', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('ADF Statistic', fontsize=12)\n",
    "        axes[0, 1].set_title('ADF Test Statistic vs d', fontsize=14)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Correlation vs d\n",
    "        axes[1, 0].plot(results_df['d'], results_df['correlation'], 'o-', linewidth=2, markersize=6, color='orange')\n",
    "        if min_d is not None:\n",
    "            min_d_corr = results_df[results_df['d'] == min_d]['correlation'].values[0]\n",
    "            axes[1, 0].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[1, 0].scatter([min_d], [min_d_corr], color='red', s=100, zorder=5)\n",
    "        axes[1, 0].set_xlabel('d', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Correlation with Original', fontsize=12)\n",
    "        axes[1, 0].set_title('Correlation vs d', fontsize=14)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Stationarity Status\n",
    "        if 'is_stationary' in results_df.columns:\n",
    "            colors = ['red' if not stat else 'green' for stat in results_df['is_stationary']]\n",
    "            axes[1, 1].scatter(results_df['d'], results_df['is_stationary'].astype(int), \n",
    "                            c=colors, s=100, alpha=0.6)\n",
    "            if min_d is not None:\n",
    "                axes[1, 1].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[1, 1].set_xlabel('d', fontsize=12)\n",
    "            axes[1, 1].set_ylabel('Is Stationary (1=Yes, 0=No)', fontsize=12)\n",
    "            axes[1, 1].set_title('Stationarity Status vs d', fontsize=14)\n",
    "            axes[1, 1].set_ylim(-0.1, 1.1)\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No stationarity data', \n",
    "                            ha='center', va='center', fontsize=12)\n",
    "            axes[1, 1].set_title('Stationarity Status vs d', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_min_d_results(self, result: dict, d: float):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ä½¿ç”¨æœ€å° d å€¼å¾Œçš„çµæœ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        result : dict\n",
    "            find_min_d_adf çš„è¿”å›çµæœ\n",
    "        d : float\n",
    "            ä½¿ç”¨çš„ d å€¼\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"âš ï¸ ç„¡æ•ˆçš„çµæœ\")\n",
    "            return\n",
    "        \n",
    "        # é€™å€‹æ–¹æ³•å¯ä»¥ç¹ªè£½ä½¿ç”¨ d å€¼å¾Œçš„æ™‚é–“åºåˆ—å°æ¯”\n",
    "        # ä½†éœ€è¦å¯¦éš›çš„åˆ†æ•¸éšå·®åˆ†åºåˆ—ï¼Œæ‰€ä»¥é€™è£¡åªé¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "        min_d_stats = result.get('min_d_stats', {})\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æœ€å° d å€¼çµ±è¨ˆè³‡è¨Š\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ä½¿ç”¨çš„ d å€¼: {d:.4f}\")\n",
    "        if min_d_stats:\n",
    "            print(f\"ADF Statistic: {min_d_stats.get('adf_stat', 'N/A'):.4f}\")\n",
    "            print(f\"ADF p-value: {min_d_stats.get('p_value', 'N/A'):.4f}\")\n",
    "            print(f\"èˆ‡åŸå§‹åºåˆ—ç›¸é—œæ€§: {min_d_stats.get('correlation', 'N/A'):.4f}\")\n",
    "            print(f\"æ¨£æœ¬æ•¸: {min_d_stats.get('sample_size', 'N/A'):,}\")\n",
    "            print(f\"æ˜¯å¦å¹³ç©©: {min_d_stats.get('is_stationary', 'N/A')}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    def generate_features(self, df: pd.DataFrame, event_indices: pd.DatetimeIndex,\n",
    "                      price_col: str = 'Close', d: Optional[float] = None,\n",
    "                      method: str = 'FFD', thres: float = 1e-5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç‚ºäº‹ä»¶é»ç”Ÿæˆåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            åŒ…å«åƒ¹æ ¼è³‡æ–™çš„ DataFrame\n",
    "        event_indices : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»\n",
    "        price_col : str\n",
    "            åƒ¹æ ¼æ¬„ä½åç¨±\n",
    "        d : float, optional\n",
    "            åˆ†æ•¸éšå·®åˆ†éšæ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ self.optimal_dï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        features : pd.DataFrame\n",
    "            åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µï¼ˆindex ç‚º event_indicesï¼‰\n",
    "        \"\"\"\n",
    "        if price_col not in df.columns:\n",
    "            raise ValueError(f\"æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½: {price_col}\")\n",
    "        \n",
    "        # ä½¿ç”¨æŒ‡å®šçš„ d å€¼æˆ–æœ€å„ª d å€¼\n",
    "        if d is None:\n",
    "            if self.optimal_d is None:\n",
    "                raise ValueError(\"è«‹å…ˆåŸ·è¡Œ find_optimal_d() æˆ– find_min_d_adf()ï¼Œæˆ–æŒ‡å®š d åƒæ•¸\")\n",
    "            d = self.optimal_d\n",
    "        \n",
    "        print(f\"ğŸ“Š ç”Ÿæˆåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ (d={d:.4f}, method={method})\")\n",
    "        \n",
    "        # è¨ˆç®—æ•´å€‹åºåˆ—çš„åˆ†æ•¸éšå·®åˆ†\n",
    "        price_series = df[price_col]\n",
    "        \n",
    "        if method == 'FFD':\n",
    "            fracdiff_series = self.frac_diff_FFD(price_series, d, thres)\n",
    "        else:\n",
    "            fracdiff_series = self.frac_diff(price_series, d, thres)\n",
    "        \n",
    "        # æå–äº‹ä»¶é»çš„ç‰¹å¾µå€¼\n",
    "        features = pd.DataFrame(index=event_indices)\n",
    "        features['fracdiff'] = fracdiff_series.loc[event_indices].values\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_count = features['fracdiff'].notna().sum()\n",
    "        print(f\"âœ… ç‰¹å¾µç”Ÿæˆå®Œæˆ\")\n",
    "        print(f\"   äº‹ä»¶æ•¸: {len(event_indices):,}\")\n",
    "        print(f\"   æœ‰æ•ˆç‰¹å¾µæ•¸: {valid_count:,}\")\n",
    "        print(f\"   NaN æ•¸: {len(event_indices) - valid_count:,}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def plot_price_vs_fracdiff(self, price_series: pd.Series, \n",
    "                            d: Optional[float] = None,\n",
    "                            method: str = 'FFD',\n",
    "                            thres: float = 1e-5,\n",
    "                            n_points: Optional[int] = None,\n",
    "                            normalize: bool = False):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½åŸå§‹åƒ¹æ ¼èˆ‡åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µçš„å°æ¯”åœ–\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        price_series : pd.Series\n",
    "            åŸå§‹åƒ¹æ ¼åºåˆ—\n",
    "        d : float, optional\n",
    "            åˆ†æ•¸éšå·®åˆ†éšæ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ self.optimal_d æˆ–é è¨­å€¼ 0.5ï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼\n",
    "        n_points : int, optional\n",
    "            å¦‚æœæŒ‡å®šï¼Œåªç¹ªè£½æœ€è¿‘ n_points å€‹é»ï¼ˆç”¨æ–¼å¤§é‡æ•¸æ“šæ™‚ï¼‰\n",
    "        normalize : bool\n",
    "            æ˜¯å¦æ¨™æº–åŒ–å¾Œç¹ªè£½åœ¨åŒä¸€å¼µåœ–ä¸Šï¼ˆé è¨­ Falseï¼Œåˆ†é–‹å…©å€‹å­åœ–ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fracdiff_series : pd.Series\n",
    "            è¨ˆç®—å‡ºçš„åˆ†æ•¸éšå·®åˆ†åºåˆ—\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # ç¢ºå®šä½¿ç”¨çš„ d å€¼\n",
    "        if d is None:\n",
    "            if self.optimal_d is not None:\n",
    "                d_value = self.optimal_d\n",
    "            else:\n",
    "                d_value = 0.5\n",
    "                print(f\"âš ï¸ æœªæŒ‡å®š d å€¼ï¼Œä½¿ç”¨é è¨­å€¼ {d_value}\")\n",
    "        else:\n",
    "            d_value = d\n",
    "        \n",
    "        # è¨ˆç®—æ•´å€‹åºåˆ—çš„åˆ†æ•¸éšå·®åˆ†\n",
    "        print(f\"ğŸ“Š è¨ˆç®—åˆ†æ•¸éšå·®åˆ† (d={d_value:.4f}, method={method})...\")\n",
    "        if method == 'FFD':\n",
    "            fracdiff_series = self.frac_diff_FFD(price_series, d_value, thres)\n",
    "        else:\n",
    "            fracdiff_series = self.frac_diff(price_series, d_value, thres)\n",
    "        \n",
    "        # é¸æ“‡è¦ç¹ªè£½çš„æ•¸æ“šç¯„åœ\n",
    "        if n_points is not None and len(price_series) > n_points:\n",
    "            plot_idx = price_series.index[-n_points:]\n",
    "            price_plot = price_series.loc[plot_idx]\n",
    "            fracdiff_plot = fracdiff_series.loc[plot_idx]\n",
    "            title_suffix = f\" (Last {n_points:,} Points)\"\n",
    "        else:\n",
    "            plot_idx = price_series.index\n",
    "            price_plot = price_series\n",
    "            fracdiff_plot = fracdiff_series\n",
    "            title_suffix = \"\"\n",
    "        \n",
    "        # ç¹ªè£½åœ–è¡¨\n",
    "        if normalize:\n",
    "            # æ–¹æ³• 1: æ¨™æº–åŒ–å¾Œç¹ªè£½åœ¨åŒä¸€å¼µåœ–ä¸Š\n",
    "            common_idx = plot_idx.intersection(fracdiff_plot.dropna().index)\n",
    "            if len(common_idx) > 0:\n",
    "                price_normalized = (price_plot.loc[common_idx] - price_plot.loc[common_idx].mean()) / price_plot.loc[common_idx].std()\n",
    "                fracdiff_normalized = (fracdiff_plot.loc[common_idx] - fracdiff_plot.loc[common_idx].mean()) / fracdiff_plot.loc[common_idx].std()\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(15, 6))\n",
    "                \n",
    "                ax.plot(common_idx, price_normalized, \n",
    "                    label='Original Price (Normalized)', linewidth=1.5, color='blue', alpha=0.7)\n",
    "                ax.plot(common_idx, fracdiff_normalized, \n",
    "                    label='Fractional Differentiation (Normalized)', linewidth=1.5, color='red', alpha=0.7)\n",
    "                ax.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "                ax.set_xlabel('Time', fontsize=12)\n",
    "                ax.set_ylabel('Normalized Value', fontsize=12)\n",
    "                ax.set_title(f'Normalized Comparison: Price vs Fractional Differentiation{title_suffix}\\n(d={d_value:.4f}, method={method})', fontsize=14)\n",
    "                ax.legend(loc='upper left')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            # æ–¹æ³• 2: åˆ†é–‹å…©å€‹å­åœ–ï¼ˆé è¨­ï¼‰\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "            \n",
    "            # ä¸Šåœ–ï¼šåŸå§‹åƒ¹æ ¼\n",
    "            axes[0].plot(plot_idx, price_plot.values, \n",
    "                        label='Original Price', linewidth=1.5, color='blue', alpha=0.7)\n",
    "            axes[0].set_ylabel('Price', fontsize=12)\n",
    "            axes[0].set_title(f'Original Price vs Fractional Differentiation{title_suffix}\\n(d={d_value:.4f}, method={method})', fontsize=14)\n",
    "            axes[0].legend(loc='upper left')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # ä¸‹åœ–ï¼šåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "            valid_fracdiff = fracdiff_plot.dropna()\n",
    "            if len(valid_fracdiff) > 0:\n",
    "                axes[1].plot(valid_fracdiff.index, valid_fracdiff.values, \n",
    "                            label='Fractional Differentiation', linewidth=1.5, color='red', alpha=0.7)\n",
    "            axes[1].axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "            axes[1].set_xlabel('Time', fontsize=12)\n",
    "            axes[1].set_ylabel('Fractional Differentiation', fontsize=12)\n",
    "            axes[1].set_title('Fractional Differentiation Feature', fontsize=14)\n",
    "            axes[1].legend(loc='upper left')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_count = fracdiff_series.notna().sum()\n",
    "        print(f\"âœ… ç¹ªåœ–å®Œæˆ\")\n",
    "        print(f\"   åŸå§‹æ•¸æ“šé»æ•¸: {len(price_series):,}\")\n",
    "        print(f\"   åˆ†æ•¸éšå·®åˆ†æœ‰æ•ˆå€¼: {valid_count:,}\")\n",
    "        print(f\"   NaN å€¼: {len(price_series) - valid_count:,}\")\n",
    "        \n",
    "        return fracdiff_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f44f8",
   "metadata": {},
   "source": [
    "å…¶ä»–å•†å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fracdiff_features_all = pd.read_csv('fracdiff_features_all_products.csv')\n",
    "if 'Date' in fracdiff_features_all.columns:\n",
    "    fracdiff_features_all['Date'] = pd.to_datetime(fracdiff_features_all['Date'])\n",
    "    fracdiff_features_all.set_index('Date', inplace=True)\n",
    "fracdiff_features_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7be784",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ6 ï¼š å…¶ä»–ç‰¹å¾µç”¢ç”Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349eb19",
   "metadata": {},
   "source": [
    "æŠ€è¡“æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"15åˆ†é˜è³‡æ–™ç”¨çš„æŠ€è¡“æŒ‡æ¨™ï¼ˆå››çµ„å°ºåº¦ï¼šSS=4Hã€S=9Hã€M=24Hã€L=72Hï¼‰\"\"\"\n",
    "    tech_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    Open = df['Open']\n",
    "    High = df['High']\n",
    "    Low = df['Low']\n",
    "    Close = df['Close']\n",
    "    Volume = df['Volume']\n",
    "\n",
    "    # é€šç”¨å·¥å…·\n",
    "    def wilders_rma(s, period):\n",
    "        return s.ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "    def true_range(high, low, close):\n",
    "        prev_close = close.shift(1)\n",
    "        return pd.concat([\n",
    "            (high - low),\n",
    "            (high - prev_close).abs(),\n",
    "            (low - prev_close).abs()\n",
    "        ], axis=1).max(axis=1)\n",
    "\n",
    "    def mfi(high, low, close, volume, period=14):\n",
    "        tp = (high + low + close) / 3.0\n",
    "        rmf = tp * volume\n",
    "        pos = (tp > tp.shift(1)).astype(int)\n",
    "        neg = (tp < tp.shift(1)).astype(int)\n",
    "        pos_mf = (rmf * pos).rolling(period).sum()\n",
    "        neg_mf = (rmf * neg).rolling(period).sum()\n",
    "        mr = pos_mf / neg_mf\n",
    "        return 100 - (100 / (1 + mr))\n",
    "\n",
    "    def dm_di_dx(high, low, close, period=14):\n",
    "        up_move = high.diff()\n",
    "        down_move = -low.diff()\n",
    "        plus_dm = ((up_move > down_move) & (up_move > 0)) * up_move\n",
    "        minus_dm = ((down_move > up_move) & (down_move > 0)) * down_move\n",
    "        tr = true_range(high, low, close)\n",
    "        atr = wilders_rma(tr, period)\n",
    "        plus_di = 100 * wilders_rma(plus_dm, period) / atr\n",
    "        minus_di = 100 * wilders_rma(minus_dm, period) / atr\n",
    "        dx = 100 * (plus_di - minus_di).abs() / (plus_di + minus_di)\n",
    "        return plus_dm, minus_dm, plus_di, minus_di, dx\n",
    "\n",
    "    # ========= TRANGE/ATR/NATRï¼ˆåŠ å…¥ SSï¼‰=========\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 14H=56, 24H=96, 72H=288\n",
    "    tech_df['TRANGE'] = true_range(High, Low, Close)\n",
    "    for p, tag in [(16,'SS'), (56,'S'), (96,'M'), (288,'L')]:\n",
    "        atr = wilders_rma(tech_df['TRANGE'], p)\n",
    "        tech_df[f'ATR_{tag}'] = atr\n",
    "        tech_df[f'NATR_{tag}'] = atr / Close * 100.0\n",
    "\n",
    "    # ========= Momentum & Oscillators =========\n",
    "    # MOM / ROC / ROCRï¼ˆåŠ å…¥ SSï¼‰\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 10H=40, 24H=96, 72H=288\n",
    "    for p, tag in [(16,'SS'), (40,'S'), (96,'M'), (288,'L')]:\n",
    "        tech_df[f'MOM_{tag}'] = Close - Close.shift(p)\n",
    "        tech_df[f'ROC_{tag}'] = Close.pct_change(p) * 100.0\n",
    "        tech_df[f'ROCR_{tag}'] = Close / Close.shift(p)\n",
    "\n",
    "    # MFIï¼ˆåŠ å…¥ SSï¼‰\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 14H=56, 24H=96, 72H=288\n",
    "    for p, tag in [(16,'SS'), (56,'S'), (96,'M'), (288,'L')]:\n",
    "        tech_df[f'MFI_{tag}'] = mfi(High, Low, Close, Volume, p)\n",
    "\n",
    "    # ADX/DI/DM/DXï¼ˆåªä¿ç•™ MINUS_DMï¼‰\n",
    "    # 15åˆ†é˜è³‡æ–™ï¼š4H=16, 14H=56, 24H=96, 72H=288\n",
    "    for p, tag in [(16,'SS'), (56,'S'), (96,'M'), (288,'L')]:\n",
    "        _, minus_dm, _, _, _ = dm_di_dx(High, Low, Close, p)\n",
    "        tech_df[f'MINUS_DM_{tag}'] = minus_dm\n",
    "\n",
    "    # ========= Volume & Accumulation =========\n",
    "    # Volume_SMA_20: 20å€‹15åˆ†é˜Kç·šï¼ˆç´„5å°æ™‚ï¼‰ï¼Œä¿æŒä¸è®Š\n",
    "    tech_df['Volume_SMA_20'] = Volume.rolling(20).mean()\n",
    "    tech_df['Volume_Ratio_20'] = Volume / tech_df['Volume_SMA_20']\n",
    "\n",
    "    return tech_df\n",
    "\n",
    "# è¨ˆç®—æŠ€è¡“æŒ‡æ¨™\n",
    "tech_indicators = calculate_technical_indicators(nas100_raw)  # ä½¿ç”¨15åˆ†é˜åŸå§‹è³‡æ–™\n",
    "print(f\"æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œæˆï¼Œå…± {len(tech_indicators.columns)} å€‹æŒ‡æ¨™\")\n",
    "\n",
    "# æª¢æŸ¥è³‡æ–™å“è³ª\n",
    "print(f\"\\nè³‡æ–™å“è³ªæª¢æŸ¥:\")\n",
    "print(f\"ç¸½ç­†æ•¸: {len(tech_indicators)}\")\n",
    "print(f\"ç¼ºå¤±å€¼çµ±è¨ˆ:\")\n",
    "missing_stats = tech_indicators.isnull().sum()\n",
    "print(missing_stats[missing_stats > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27180da",
   "metadata": {},
   "source": [
    "åŸºç¤çµ±è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_indicator = pd.read_csv('NAS100_RS_Volatility.csv')\n",
    "if 'Date' in volatility_indicator.columns:\n",
    "    volatility_indicator['Date'] = pd.to_datetime(volatility_indicator['Date'])\n",
    "    volatility_indicator.set_index('Date', inplace=True)\n",
    "volatility_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aee4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_indicator = pd.read_csv('NAS100_Stats_Indicator.csv')\n",
    "if 'Date' in stats_indicator.columns:\n",
    "    stats_indicator['Date'] = pd.to_datetime(stats_indicator['Date'])\n",
    "    stats_indicator.set_index('Date', inplace=True)\n",
    "stats_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86535cb5",
   "metadata": {},
   "source": [
    "é‡åƒ¹è¡Œç‚º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d471658",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_indicator = pd.read_csv('NAS100_Market_Indicator.csv')\n",
    "if 'Date' in market_indicator.columns:\n",
    "    market_indicator['Date'] = pd.to_datetime(market_indicator['Date'])\n",
    "    market_indicator.set_index('Date', inplace=True)\n",
    "market_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47144768",
   "metadata": {},
   "source": [
    "è³‡è¨Šç†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_indicator = pd.read_csv('NAS100_Entropy_Indicator.csv')\n",
    "# if 'Date' in entropy_indicator.columns:\n",
    "#     entropy_indicator['Date'] = pd.to_datetime(entropy_indicator['Date'])\n",
    "#     entropy_indicator.set_index('Date', inplace=True)\n",
    "# entropy_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4593ae",
   "metadata": {},
   "source": [
    "å¸‚å ´å¾®çµæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_indicator = pd.read_csv('NAS100_Micro_Indicator.csv')\n",
    "if 'Date' in micro_indicator.columns:\n",
    "    micro_indicator['Date'] = pd.to_datetime(micro_indicator['Date'])\n",
    "    micro_indicator.set_index('Date', inplace=True)\n",
    "micro_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8b7e2",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ6.5 ï¼š ç‰¹å¾µé›œè¨Šç¯©é™¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad79ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š æ­¥é©Ÿ6.5ï¼šç‰¹å¾µåˆä½µèˆ‡ç¯©é¸ Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬1éƒ¨åˆ†ï¼šç‰¹å¾µåˆä½µ\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”— ç¬¬1éƒ¨åˆ†ï¼šç‰¹å¾µåˆä½µ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç¬¬0æ­¥ï¼šæŒ‡å®šæ•¸æ“š\n",
    "y = bins['bin']\n",
    "t1 = events['t1']\n",
    "sample_weight = final_weights\n",
    "\n",
    "# æº–å‚™ç‰¹å¾µçŸ©é™£ X\n",
    "event_indices = events.index\n",
    "features_list = []\n",
    "\n",
    "print(f\"\\näº‹ä»¶ç´¢å¼•æ•¸é‡: {len(event_indices):,}\")\n",
    "\n",
    "# =====================================================\n",
    "# åˆ—å‡ºæ‰€æœ‰ç‰¹å¾µ DataFrame åŠå…¶æ¬„ä½\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ æª¢æŸ¥æ‰€æœ‰ç‰¹å¾µ DataFrame åŠå…¶æ¬„ä½...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_dataframes = {\n",
    "    '1. fracdiff_features_all': 'fracdiff_features_all',\n",
    "    '2. tech_indicators': 'tech_indicators',\n",
    "    '3. volatility_indicator': 'volatility_indicator',\n",
    "    '4. market_indicator': 'market_indicator',\n",
    "    '5. entropy_indicator': 'entropy_indicator',\n",
    "    '6. micro_indicator': 'micro_indicator',\n",
    "    '7. stats_indicator': 'stats_indicator',\n",
    "}\n",
    "\n",
    "available_features = {}\n",
    "for name, var_name in feature_dataframes.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if var_name in globals():\n",
    "        df_var = globals()[var_name]\n",
    "        if isinstance(df_var, pd.DataFrame):\n",
    "            print(f\"âœ… DataFrame å­˜åœ¨\")\n",
    "            print(f\"   å½¢ç‹€: {df_var.shape}\")\n",
    "            print(f\"   æ¬„ä½æ•¸: {len(df_var.columns)}\")\n",
    "            print(f\"   ç´¢å¼•ç¯„åœ: {df_var.index.min()} è‡³ {df_var.index.max()}\")\n",
    "            \n",
    "            # æ‰“å°æ‰€æœ‰æ¬„ä½åç¨±\n",
    "            print(f\"\\n   æ¬„ä½åˆ—è¡¨:\")\n",
    "            if len(df_var.columns) > 0:\n",
    "                cols_list = list(df_var.columns)\n",
    "                if len(cols_list) <= 20:\n",
    "                    for i, col in enumerate(cols_list, 1):\n",
    "                        print(f\"     {i:3d}. {col}\")\n",
    "                else:\n",
    "                    for i, col in enumerate(cols_list[:10], 1):\n",
    "                        print(f\"     {i:3d}. {col}\")\n",
    "                    print(f\"     ... (çœç•¥ {len(cols_list) - 20} å€‹æ¬„ä½) ...\")\n",
    "                    for i, col in enumerate(cols_list[-10:], len(cols_list) - 9):\n",
    "                        print(f\"     {i:3d}. {col}\")\n",
    "                    print(f\"     ç¸½å…± {len(cols_list)} å€‹æ¬„ä½\")\n",
    "            else:\n",
    "                print(f\"     âš ï¸ æ²’æœ‰æ¬„ä½\")\n",
    "            \n",
    "            # å°é½Šåˆ°äº‹ä»¶ç´¢å¼•\n",
    "            try:\n",
    "                common_idx = df_var.index.intersection(event_indices)\n",
    "                missing_idx = event_indices.difference(df_var.index)\n",
    "                \n",
    "                if len(missing_idx) > 0:\n",
    "                    print(f\"\\n   âš ï¸ è­¦å‘Šï¼šæœ‰ {len(missing_idx)} å€‹äº‹ä»¶ç´¢å¼•åœ¨ DataFrame ä¸­ä¸å­˜åœ¨\")\n",
    "                    print(f\"      å‰5å€‹ç¼ºå¤±ç´¢å¼•: {list(missing_idx[:5])}\")\n",
    "                \n",
    "                if len(common_idx) > 0:\n",
    "                    df_aligned = df_var.reindex(event_indices, method='ffill')\n",
    "                    df_aligned = df_aligned.fillna(method='bfill')\n",
    "                    df_aligned = df_aligned.fillna(0)\n",
    "                    \n",
    "                    if len(df_aligned.columns) > 0:\n",
    "                        available_features[name] = df_aligned\n",
    "                        print(f\"\\n   âœ… å°é½Šå¾Œ: {len(df_aligned.columns)} å€‹ç‰¹å¾µ, {len(df_aligned)} å€‹æ¨£æœ¬\")\n",
    "                        print(f\"      æœ‰æ•ˆå€¼æ¯”ä¾‹: {(df_aligned.notna().sum().sum() / (len(df_aligned) * len(df_aligned.columns)) * 100):.2f}%\")\n",
    "                    else:\n",
    "                        print(f\"\\n   âš ï¸ å°é½Šå¾Œæ²’æœ‰æ¬„ä½\")\n",
    "                else:\n",
    "                    print(f\"\\n   âŒ æ²’æœ‰å…±åŒç´¢å¼•ï¼Œç„¡æ³•å°é½Š\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n   âŒ å°é½Šå¤±æ•—: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ è®Šæ•¸å­˜åœ¨ä½†ä¸æ˜¯ DataFrame (é¡å‹: {type(df_var)})\")\n",
    "    else:\n",
    "        print(f\"âŒ è®Šæ•¸ä¸å­˜åœ¨\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ“Š ç¸½çµ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"æ‰¾åˆ° {len(available_features)} å€‹å¯ç”¨çš„ç‰¹å¾µ DataFrame\")\n",
    "\n",
    "# =====================================================\n",
    "# æ·»åŠ  HL115% å’Œ çªç ´signalåƒ¹ ç‰¹å¾µ\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š æ·»åŠ  HL115% å’Œ çªç ´signalåƒ¹ ç‰¹å¾µ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'strategy' in globals() and hasattr(strategy, 'signals') and strategy.signals is not None:\n",
    "    signal_features = pd.DataFrame(index=strategy.signals.index)\n",
    "    \n",
    "    if 'HL115%' in strategy.signals.columns:\n",
    "        signal_features['HL115%'] = strategy.signals['HL115%']\n",
    "        print(f\"âœ… åŠ å…¥ HL115% ç‰¹å¾µ\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ æ‰¾ä¸åˆ° HL115% æ¬„ä½\")\n",
    "    \n",
    "    if 'çªç ´signalåƒ¹%' in strategy.signals.columns:\n",
    "        signal_features['çªç ´signalåƒ¹%'] = strategy.signals['çªç ´signalåƒ¹%']\n",
    "        print(f\"âœ… åŠ å…¥ çªç ´signalåƒ¹% ç‰¹å¾µ\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ æ‰¾ä¸åˆ° çªç ´signalåƒ¹% æ¬„ä½\")\n",
    "    \n",
    "    if len(signal_features.columns) > 0:\n",
    "        signal_features_aligned = signal_features.reindex(event_indices)\n",
    "        features_list.append(signal_features_aligned)\n",
    "        print(f\"âœ… æˆåŠŸæ·»åŠ  {len(signal_features.columns)} å€‹ä¿¡è™Ÿç‰¹å¾µåˆ°ç‰¹å¾µåˆ—è¡¨\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ æ²’æœ‰æˆåŠŸæå–ä»»ä½•ä¿¡è™Ÿç‰¹å¾µ\")\n",
    "else:\n",
    "    print(f\"âš ï¸ strategy ä¸å­˜åœ¨æˆ–å°šæœªè¨ˆç®—æŒ‡æ¨™ï¼Œè·³éä¿¡è™Ÿç‰¹å¾µ\")\n",
    "\n",
    "# =====================================================\n",
    "# åˆä½µæ‰€æœ‰ç‰¹å¾µ\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”— åˆä½µæ‰€æœ‰ç‰¹å¾µ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, df_feat in available_features.items():\n",
    "    numeric_cols = df_feat.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        features_list.append(df_feat[numeric_cols])\n",
    "        print(f\"âœ… åŠ å…¥ {name}: {len(numeric_cols)} å€‹æ•¸å€¼ç‰¹å¾µ\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {name}: æ²’æœ‰æ•¸å€¼å‹æ¬„ä½ï¼Œè·³é\")\n",
    "\n",
    "if len(features_list) == 0:\n",
    "    raise ValueError(\"âŒ éŒ¯èª¤ï¼šæ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‰¹å¾µï¼\")\n",
    "\n",
    "print(f\"\\næº–å‚™åˆä½µ {len(features_list)} å€‹ç‰¹å¾µ DataFrame...\")\n",
    "\n",
    "try:\n",
    "    X = pd.concat(features_list, axis=1, join='outer', sort=False)\n",
    "    print(f\"âœ… åˆä½µå®Œæˆ\")\n",
    "    print(f\"   åˆä½µå‰ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "    print(f\"   åˆä½µå‰æ¨£æœ¬æ•¸: {len(X):,}\")\n",
    "    \n",
    "    X = X.reindex(event_indices)\n",
    "    print(f\"   å°é½Šåˆ°äº‹ä»¶ç´¢å¼•å¾Œæ¨£æœ¬æ•¸: {len(X):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise ValueError(f\"âŒ åˆä½µå¤±æ•—: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd31ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ç‚º X çš„æ¯å€‹ç‰¹å¾µåˆ†åˆ¥ç¹ªè£½ä¸€å¼µåœ–ï¼Œä¸¦æ¨™ç¤º 95% winsorize çš„ä¸Šä¸‹é™\n",
    "for col in X.columns:\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    \n",
    "    # æå–ç‰¹å¾µæ•¸æ“š\n",
    "    feature_data = X[col].dropna()\n",
    "    \n",
    "    if len(feature_data) == 0:\n",
    "        ax.text(0.5, 0.5, f'{col}\\n(No data)', \n",
    "               ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(col, fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        continue\n",
    "    \n",
    "    # è¨ˆç®— 95% winsorize çš„ä¸Šä¸‹é™ï¼ˆ2.5% å’Œ 97.5% åˆ†ä½æ•¸ï¼‰\n",
    "    lower_bound = np.percentile(feature_data, 2.5)\n",
    "    upper_bound = np.percentile(feature_data, 97.5)\n",
    "    \n",
    "    # ç¹ªè£½ç‰¹å¾µæ›²ç·š\n",
    "    ax.plot(feature_data.index, feature_data.values, \n",
    "           linewidth=1.5, alpha=0.7, color='blue', label='Feature Value')\n",
    "    \n",
    "    # ç¹ªè£½ winsorize ä¸Šä¸‹é™\n",
    "    ax.axhline(y=upper_bound, color='red', linestyle='--', \n",
    "              linewidth=2, label=f'Upper Bound (97.5%: {upper_bound:.4f})')\n",
    "    ax.axhline(y=lower_bound, color='green', linestyle='--', \n",
    "              linewidth=2, label=f'Lower Bound (2.5%: {lower_bound:.4f})')\n",
    "    \n",
    "    # å¡«å…… winsorize ç¯„åœ\n",
    "    ax.fill_between(feature_data.index, lower_bound, upper_bound, \n",
    "                   alpha=0.15, color='blue', label='95% Winsorize Range')\n",
    "    \n",
    "    # è¨­ç½®æ¨™é¡Œå’Œæ¨™ç±¤\n",
    "    ax.set_title(f'{col}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Time', fontsize=12)\n",
    "    ax.set_ylabel('Value', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ—‹è½‰ x è»¸æ¨™ç±¤ï¼ˆå¦‚æœæ˜¯æ™‚é–“ç´¢å¼•ï¼‰\n",
    "    if isinstance(feature_data.index, pd.DatetimeIndex):\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°è©²ç‰¹å¾µçš„çµ±è¨ˆè³‡è¨Š\n",
    "    print(f\"\\n{col} Statistics:\")\n",
    "    print(f\"  Lower Bound (2.5%): {lower_bound:.6f}\")\n",
    "    print(f\"  Upper Bound (97.5%): {upper_bound:.6f}\")\n",
    "    print(f\"  Mean: {feature_data.mean():.6f}\")\n",
    "    print(f\"  Std: {feature_data.std():.6f}\")\n",
    "    print(f\"  Min: {feature_data.min():.6f}\")\n",
    "    print(f\"  Max: {feature_data.max():.6f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Winsorize è™•ç†ï¼šå° X çš„æ‰€æœ‰ç‰¹å¾µé€²è¡Œ winsorize\n",
    "# =====================================================\n",
    "\n",
    "# è¨­å®š winsorize ç™¾åˆ†æ¯”ï¼ˆä¾‹å¦‚ï¼š95 è¡¨ç¤º 95%ï¼Œå³ä¸Šä¸‹å„æˆªæ–· 2.5%ï¼‰\n",
    "winsorize_percent = 95  # å¯ä»¥æ”¹ç‚º 90, 95, 99 ç­‰\n",
    "\n",
    "# è¨ˆç®—å°æ‡‰çš„åˆ†ä½æ•¸\n",
    "lower_percentile = (100 - winsorize_percent) / 2\n",
    "upper_percentile = 100 - lower_percentile\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Applying {winsorize_percent}% Winsorize to All Features\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Lower Percentile: {lower_percentile}%\")\n",
    "print(f\"Upper Percentile: {upper_percentile}%\")\n",
    "print()\n",
    "\n",
    "# å‰µå»º X_winsorized å‰¯æœ¬\n",
    "X_winsorized = X.copy()\n",
    "\n",
    "# å°æ¯å€‹ç‰¹å¾µé€²è¡Œ winsorize\n",
    "for col in X.columns:\n",
    "    feature_data = X[col].dropna()\n",
    "    \n",
    "    if len(feature_data) == 0:\n",
    "        print(f\"âš ï¸ {col}: No data, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # è¨ˆç®—ä¸Šä¸‹é™\n",
    "    lower_bound = np.percentile(feature_data, lower_percentile)\n",
    "    upper_bound = np.percentile(feature_data, upper_percentile)\n",
    "    \n",
    "    # åŸ·è¡Œ winsorizeï¼šå°‡è¶…å‡ºç¯„åœçš„å€¼æˆªæ–·åˆ°ä¸Šä¸‹é™\n",
    "    X_winsorized[col] = X[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    # çµ±è¨ˆè¢«æˆªæ–·çš„æ•¸æ“šé»\n",
    "    n_clipped_lower = (X[col] < lower_bound).sum()\n",
    "    n_clipped_upper = (X[col] > upper_bound).sum()\n",
    "    n_clipped_total = n_clipped_lower + n_clipped_upper\n",
    "    \n",
    "    # print(f\"âœ… {col}:\")\n",
    "    # print(f\"   Lower Bound ({lower_percentile:.1f}%): {lower_bound:.6f}\")\n",
    "    # print(f\"   Upper Bound ({upper_percentile:.1f}%): {upper_bound:.6f}\")\n",
    "    # print(f\"   Clipped: {n_clipped_total} points ({n_clipped_lower} lower, {n_clipped_upper} upper)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… Winsorize completed!\")\n",
    "print(f\"   Original X shape: {X.shape}\")\n",
    "print(f\"   Winsorized X shape: {X_winsorized.shape}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å°‡ winsorized çš„çµæœè³¦å€¼å› Xï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "X = X_winsorized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ•¸æ“šæ¸…ç†\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ§¹ æ•¸æ“šæ¸…ç†...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X = X.fillna(method='ffill').fillna(method='bfill').fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# ç§»é™¤å®Œå…¨ç‚ºå¸¸æ•¸çš„ç‰¹å¾µ\n",
    "constant_cols = []\n",
    "for col in X.columns:\n",
    "    try:\n",
    "        unique_count = int(X[col].nunique())\n",
    "        if unique_count <= 1:\n",
    "            constant_cols.append(col)\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "\n",
    "if len(constant_cols) > 0:\n",
    "    print(f\"ç§»é™¤ {len(constant_cols)} å€‹å¸¸æ•¸ç‰¹å¾µ\")\n",
    "    X = X.drop(columns=constant_cols)\n",
    "\n",
    "# ç§»é™¤å®Œå…¨ç‚º NaN çš„ç‰¹å¾µ\n",
    "nan_cols = [col for col in X.columns if X[col].isna().all()]\n",
    "if len(nan_cols) > 0:\n",
    "    print(f\"ç§»é™¤ {len(nan_cols)} å€‹å…¨ NaN ç‰¹å¾µ\")\n",
    "    X = X.drop(columns=nan_cols)\n",
    "\n",
    "print(f\"\\nâœ… æ¸…ç†å®Œæˆ\")\n",
    "print(f\"   æœ€çµ‚ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "print(f\"   æœ€çµ‚æ¨£æœ¬æ•¸: {len(X):,}\")\n",
    "\n",
    "if len(X.columns) == 0:\n",
    "    raise ValueError(\"âŒ éŒ¯èª¤ï¼šåˆä½µå¾Œç‰¹å¾µæ•¸é‡ç‚º 0ï¼\")\n",
    "\n",
    "# =====================================================\n",
    "# æ•¸æ“šå°é½Š\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”— æ•¸æ“šå°é½Š...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "common_idx = X.index.intersection(y.index).intersection(t1.index)\n",
    "print(f\"å°é½Šå‰: X={len(X)}, y={len(y)}, t1={len(t1)}\")\n",
    "print(f\"å°é½Šå¾Œ: {len(common_idx)}\")\n",
    "\n",
    "X = X.loc[common_idx]\n",
    "y = y.loc[common_idx]\n",
    "t1 = t1.loc[common_idx]\n",
    "sample_weight = sample_weight.loc[common_idx] if sample_weight is not None else None\n",
    "\n",
    "# æ¨™ç±¤æ˜ å°„ï¼šå°‡ -1, 0, 1 æ˜ å°„ç‚º 0, 1, 2\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "reverse_mapping = {0: -1, 1: 0, 2: 1}\n",
    "y_mapped = y.map(label_mapping)\n",
    "\n",
    "print(f\"\\nâœ… å°é½Šå®Œæˆ\")\n",
    "print(f\"   X: {len(X):,} å€‹æ¨£æœ¬, {len(X.columns):,} å€‹ç‰¹å¾µ\")\n",
    "print(f\"   y: {len(y_mapped):,} å€‹æ¨£æœ¬\")\n",
    "print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {y_mapped.value_counts().to_dict()}\")\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬2éƒ¨åˆ†ï¼šç‰¹å¾µç¯©é¸ Pipelineï¼ˆåªä½¿ç”¨ test_start ä¹‹å‰çš„æ•¸æ“šï¼‰\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” ç¬¬2éƒ¨åˆ†ï¼šç‰¹å¾µç¯©é¸ Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âš ï¸ åªä½¿ç”¨ test_start ({test_start}) ä¹‹å‰çš„æ•¸æ“šé€²è¡Œç¯©é¸\")\n",
    "\n",
    "# åˆ‡åˆ†è¨“ç·´é›†ï¼ˆç”¨æ–¼ç¯©é¸ï¼‰\n",
    "train_mask = (X.index >= train_start) & (X.index < test_start)\n",
    "X_train_filter = X.loc[train_mask].copy()\n",
    "y_train_filter = y_mapped.loc[train_mask].copy()\n",
    "\n",
    "print(f\"\\nç¯©é¸ç”¨è¨“ç·´é›†:\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {len(X_train_filter):,}\")\n",
    "print(f\"   ç‰¹å¾µæ•¸: {len(X_train_filter.columns):,}\")\n",
    "print(f\"   æ™‚é–“ç¯„åœ: {X_train_filter.index.min()} è‡³ {X_train_filter.index.max()}\")\n",
    "\n",
    "# =====================================================\n",
    "# ç‰¹å¾µç¯©é¸ Pipeline Class\n",
    "# =====================================================\n",
    "\n",
    "class FeatureFilteringPipeline:\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ç‰¹å¾µç¯©é¸ Pipeline\n",
    "    \n",
    "    åŒ…å«ï¼š\n",
    "    1. Correlation Filteringï¼ˆç›¸é—œæ€§éæ¿¾ï¼‰\n",
    "    2. Variance Thresholdï¼ˆä½è®Šç•°å‰”é™¤ï¼‰\n",
    "    3. IC (Information Coefficient) Ranking\n",
    "    4. ANOVA / t-test / Ï‡Â² çµ±è¨ˆæª¢å®š\n",
    "    5. MDA (Permutation Importance)\n",
    "    6. Stability Selectionï¼ˆå¯é¸ï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 corr_threshold=0.95,\n",
    "                 variance_threshold=0.0,\n",
    "                 ic_threshold=None,\n",
    "                 anova_p_threshold=0.05,\n",
    "                 mda_top_n=None,\n",
    "                 stability_n_iter=50,\n",
    "                 stability_threshold=0.6):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        corr_threshold : float\n",
    "            ç›¸é—œæ€§é–¾å€¼ï¼Œè¶…éæ­¤å€¼è¦–ç‚ºé«˜åº¦ç›¸é—œï¼ˆé è¨­ 0.95ï¼‰\n",
    "        variance_threshold : float\n",
    "            è®Šç•°æ•¸é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼è¦–ç‚ºä½è®Šç•°ï¼ˆé è¨­ 0.0ï¼‰\n",
    "        ic_threshold : float or None\n",
    "            IC é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼è¦–ç‚ºç„¡é æ¸¬åŠ›ï¼ˆNone è¡¨ç¤ºä¿ç•™ top Nï¼‰\n",
    "        anova_p_threshold : float\n",
    "            ANOVA p-value é–¾å€¼ï¼ˆé è¨­ 0.05ï¼‰\n",
    "        mda_top_n : int or None\n",
    "            MDA ä¿ç•™å‰ N å€‹ç‰¹å¾µï¼ˆNone è¡¨ç¤ºä¿ç•™æ‰€æœ‰é€šéæª¢å®šçš„ï¼‰\n",
    "        stability_n_iter : int\n",
    "            Stability Selection è¿­ä»£æ¬¡æ•¸ï¼ˆé è¨­ 50ï¼‰\n",
    "        stability_threshold : float\n",
    "            Stability Selection é¸æ“‡é »ç‡é–¾å€¼ï¼ˆé è¨­ 0.6ï¼‰\n",
    "        \"\"\"\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.ic_threshold = ic_threshold\n",
    "        self.anova_p_threshold = anova_p_threshold\n",
    "        self.mda_top_n = mda_top_n\n",
    "        self.stability_n_iter = stability_n_iter\n",
    "        self.stability_threshold = stability_threshold\n",
    "        \n",
    "        self.selected_features = None\n",
    "        self.filter_results = {}\n",
    "        \n",
    "    def correlation_filter(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        1ï¸âƒ£ Correlation Filteringï¼ˆç›¸é—œæ€§éæ¿¾ï¼‰\n",
    "        \n",
    "        ç§»é™¤é«˜åº¦ç›¸é—œçš„ç‰¹å¾µï¼ˆ|Ï| > thresholdï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"1ï¸âƒ£ Correlation Filtering\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        corr_matrix = X.corr().abs()\n",
    "        \n",
    "        # æ‰¾å‡ºé«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°\n",
    "        upper_triangle = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        \n",
    "        # æ‰¾å‡ºéœ€è¦ç§»é™¤çš„ç‰¹å¾µ\n",
    "        to_remove = set()\n",
    "        for col in upper_triangle.columns:\n",
    "            high_corr = upper_triangle.index[upper_triangle[col] > self.corr_threshold]\n",
    "            if len(high_corr) > 0:\n",
    "                # ä¿ç•™èˆ‡å…¶ä»–ç‰¹å¾µç›¸é—œæ€§è¼ƒé«˜çš„ï¼ˆä¿ç•™æ›´é‡è¦çš„ï¼‰\n",
    "                # ç°¡å–®ç­–ç•¥ï¼šä¿ç•™ç¬¬ä¸€å€‹ï¼Œç§»é™¤å…¶ä»–çš„\n",
    "                to_remove.update(high_corr[1:])\n",
    "        \n",
    "        removed_count = len(to_remove)\n",
    "        remaining_features = [col for col in X.columns if col not in to_remove]\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤é«˜åº¦ç›¸é—œç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(remaining_features):,}\")\n",
    "        print(f\"   ç›¸é—œæ€§é–¾å€¼: {self.corr_threshold}\")\n",
    "        \n",
    "        self.filter_results['correlation'] = {\n",
    "            'removed': list(to_remove),\n",
    "            'remaining': remaining_features,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[remaining_features]\n",
    "    \n",
    "    def variance_filter(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        2ï¸âƒ£ Variance Thresholdï¼ˆä½è®Šç•°å‰”é™¤ï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"2ï¸âƒ£ Variance Threshold\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        selector = VarianceThreshold(threshold=self.variance_threshold)\n",
    "        X_selected = selector.fit_transform(X)\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "        removed_count = len(X.columns) - len(selected_features)\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä½è®Šç•°ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected_features):,}\")\n",
    "        print(f\"   è®Šç•°æ•¸é–¾å€¼: {self.variance_threshold}\")\n",
    "        \n",
    "        self.filter_results['variance'] = {\n",
    "            'removed': [col for col in X.columns if col not in selected_features],\n",
    "            'remaining': list(selected_features),\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(X_selected, index=X.index, columns=selected_features)\n",
    "    \n",
    "    def ic_ranking(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        3ï¸âƒ£ IC (Information Coefficient) Ranking\n",
    "        \n",
    "        è¨ˆç®—æ¯å€‹ç‰¹å¾µèˆ‡æ¨™ç±¤çš„ Rank Correlation\n",
    "        ä½¿ç”¨ |IC| ä¾†åˆ¤æ–·é æ¸¬èƒ½åŠ›ï¼Œæ­£è²  IC éƒ½æœ‰é æ¸¬åƒ¹å€¼\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"3ï¸âƒ£ IC (Information Coefficient) Ranking\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        ic_scores = {}\n",
    "        \n",
    "        for col in X.columns:\n",
    "            try:\n",
    "                # è¨ˆç®— Spearman rank correlation\n",
    "                ic = stats.spearmanr(X[col], y, nan_policy='omit')[0]\n",
    "                if np.isnan(ic):\n",
    "                    ic = 0.0\n",
    "                ic_scores[col] = ic\n",
    "            except:\n",
    "                ic_scores[col] = 0.0\n",
    "        \n",
    "        ic_series = pd.Series(ic_scores)\n",
    "        ic_series = ic_series.sort_values(ascending=False)\n",
    "        \n",
    "        # æ ¹æ“š |IC| é–¾å€¼æˆ–ä¿ç•™ top Nï¼ˆä½¿ç”¨çµ•å°å€¼ï¼‰\n",
    "        if self.ic_threshold is not None:\n",
    "            # ä½¿ç”¨çµ•å°å€¼åˆ¤æ–·é æ¸¬èƒ½åŠ›\n",
    "            selected = ic_series[ic_series.abs() >= self.ic_threshold].index.tolist()\n",
    "        else:\n",
    "            # ä¿ç•™ |IC| å‰ 50% çš„ç‰¹å¾µï¼ˆæŒ‰çµ•å°å€¼æ’åºï¼‰\n",
    "            n_keep = max(1, int(len(ic_series) * 0.5))\n",
    "            selected = ic_series.abs().nlargest(n_keep).index.tolist()\n",
    "        \n",
    "        removed_count = len(X.columns) - len(selected)\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        positive_ic = ic_series[ic_series > 0]\n",
    "        negative_ic = ic_series[ic_series < 0]\n",
    "        abs_ic = ic_series.abs()\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä½ |IC| ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected):,}\")\n",
    "        print(f\"   IC é–¾å€¼: |IC| >= {self.ic_threshold if self.ic_threshold is not None else 'Top 50%'}\")\n",
    "        print(f\"\\n   IC çµ±è¨ˆ:\")\n",
    "        print(f\"     å¹³å‡ IC: {ic_series.mean():.4f}\")\n",
    "        print(f\"     å¹³å‡ |IC|: {abs_ic.mean():.4f}\")\n",
    "        print(f\"     ä¸­ä½æ•¸ |IC|: {abs_ic.median():.4f}\")\n",
    "        print(f\"     æœ€å¤§ IC: {ic_series.max():.4f}\")\n",
    "        print(f\"     æœ€å° IC: {ic_series.min():.4f}\")\n",
    "        print(f\"     æœ€å¤§ |IC|: {abs_ic.max():.4f}\")\n",
    "        print(f\"\\n   IC åˆ†å¸ƒ:\")\n",
    "        print(f\"     æ­£ IC ç‰¹å¾µæ•¸: {len(positive_ic):,} (å¹³å‡: {positive_ic.mean():.4f})\")\n",
    "        print(f\"     è²  IC ç‰¹å¾µæ•¸: {len(negative_ic):,} (å¹³å‡: {negative_ic.mean():.4f})\")\n",
    "        print(f\"     é›¶ IC ç‰¹å¾µæ•¸: {len(ic_series[ic_series == 0]):,}\")\n",
    "        \n",
    "        # é¡¯ç¤ºç¯©é¸å¾Œçš„ç‰¹å¾µï¼ˆæŒ‰ |IC| æ’åºï¼‰\n",
    "        selected_ic = ic_series[selected].abs().sort_values(ascending=False)\n",
    "        print(f\"\\n   Top 10 ç¯©é¸å¾Œç‰¹å¾µ (æŒ‰ |IC| æ’åº):\")\n",
    "        for i, (feat, ic_val) in enumerate(selected_ic.head(10).items(), 1):\n",
    "            original_ic = ic_series[feat]\n",
    "            direction = \"æ­£ç›¸é—œ\" if original_ic > 0 else \"è² ç›¸é—œ\"\n",
    "            print(f\"     {i:2d}. {feat}: |IC|={ic_val:.4f} (IC={original_ic:.4f}, {direction})\")\n",
    "        \n",
    "        self.filter_results['ic'] = {\n",
    "            'scores': ic_series.to_dict(),\n",
    "            'abs_scores': abs_ic.to_dict(),\n",
    "            'selected': selected,\n",
    "            'removed_count': removed_count,\n",
    "            'positive_count': len(positive_ic),\n",
    "            'negative_count': len(negative_ic)\n",
    "        }\n",
    "        \n",
    "        return X[selected]\n",
    "    \n",
    "    def anova_filter(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        4ï¸âƒ£ ANOVA / t-test / Ï‡Â² çµ±è¨ˆæª¢å®š\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"4ï¸âƒ£ ANOVA / t-test / Ï‡Â² Statistical Tests\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        selected_features = []\n",
    "        p_values = {}\n",
    "        \n",
    "        for col in X.columns:\n",
    "            try:\n",
    "                # æ ¹æ“šæ¨™ç±¤åˆ†çµ„\n",
    "                groups = [X[col][y == label].dropna() for label in y.unique()]\n",
    "                groups = [g for g in groups if len(g) > 0]\n",
    "                \n",
    "                if len(groups) < 2:\n",
    "                    p_values[col] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                # ä½¿ç”¨ ANOVA F-test\n",
    "                f_stat, p_val = stats.f_oneway(*groups)\n",
    "                p_values[col] = p_val\n",
    "                \n",
    "                if p_val < self.anova_p_threshold:\n",
    "                    selected_features.append(col)\n",
    "            except:\n",
    "                p_values[col] = 1.0\n",
    "        \n",
    "        removed_count = len(X.columns) - len(selected_features)\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤é«˜ p-value ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected_features):,}\")\n",
    "        print(f\"   p-value é–¾å€¼: {self.anova_p_threshold}\")\n",
    "        \n",
    "        # é¡¯ç¤ºçµ±è¨ˆé¡¯è‘—çš„ç‰¹å¾µ\n",
    "        significant = pd.Series(p_values)\n",
    "        significant = significant[significant < self.anova_p_threshold].sort_values()\n",
    "        print(f\"\\n   Top 10 æœ€é¡¯è‘—ç‰¹å¾µ:\")\n",
    "        for i, (feat, p_val) in enumerate(significant.head(10).items(), 1):\n",
    "            print(f\"     {i:2d}. {feat}: p={p_val:.6f}\")\n",
    "        \n",
    "        self.filter_results['anova'] = {\n",
    "            'p_values': p_values,\n",
    "            'selected': selected_features,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[selected_features]\n",
    "    \n",
    "    def mda_importance(self, X: pd.DataFrame, y: pd.Series, \n",
    "                       sample_weight=None, n_estimators=100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        5ï¸âƒ£ MDA (Permutation Importance)\n",
    "        \n",
    "        ä½¿ç”¨ Random Forest è¨ˆç®— Permutation Importance\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"5ï¸âƒ£ MDA (Permutation Importance)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"   è¨“ç·´ Random Forest æ¨¡å‹...\")\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        rf.fit(X, y, sample_weight=sample_weight)\n",
    "        \n",
    "        # è¨ˆç®— baseline score\n",
    "        baseline_score = rf.score(X, y, sample_weight=sample_weight)\n",
    "        \n",
    "        print(f\"   Baseline Score: {baseline_score:.4f}\")\n",
    "        print(f\"   è¨ˆç®— Permutation Importance...\")\n",
    "        \n",
    "        # è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„ permutation importance\n",
    "        importances = {}\n",
    "        for col in X.columns:\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[col] = np.random.permutation(X_permuted[col].values)\n",
    "            permuted_score = rf.score(X_permuted, y, sample_weight=sample_weight)\n",
    "            importance = baseline_score - permuted_score\n",
    "            importances[col] = importance\n",
    "        \n",
    "        importance_series = pd.Series(importances).sort_values(ascending=False)\n",
    "        \n",
    "        # é¸æ“‡é‡è¦ç‰¹å¾µ\n",
    "        if self.mda_top_n is not None:\n",
    "            selected = importance_series.head(self.mda_top_n).index.tolist()\n",
    "        else:\n",
    "            # ä¿ç•™é‡è¦æ€§ > 0 çš„ç‰¹å¾µ\n",
    "            selected = importance_series[importance_series > 0].index.tolist()\n",
    "        \n",
    "        removed_count = len(X.columns) - len(selected)\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä½é‡è¦æ€§ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected):,}\")\n",
    "        print(f\"\\n   Top 10 æœ€é‡è¦ç‰¹å¾µ:\")\n",
    "        for i, (feat, imp) in enumerate(importance_series.head(10).items(), 1):\n",
    "            print(f\"     {i:2d}. {feat}: {imp:.6f}\")\n",
    "        \n",
    "        self.filter_results['mda'] = {\n",
    "            'importances': importance_series.to_dict(),\n",
    "            'selected': selected,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[selected]\n",
    "    \n",
    "    def stability_selection(self, X: pd.DataFrame, y: pd.Series,\n",
    "                           sample_weight=None, n_estimators=50):\n",
    "        \"\"\"\n",
    "        6ï¸âƒ£ Stability Selection\n",
    "        \n",
    "        é€šéå¤šæ¬¡å­æŠ½æ¨£å’Œæ¨¡å‹è¨“ç·´ï¼Œçµ±è¨ˆç‰¹å¾µè¢«é¸ä¸­çš„é »ç‡\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"6ï¸âƒ£ Stability Selection\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"   è¿­ä»£æ¬¡æ•¸: {self.stability_n_iter}\")\n",
    "        print(f\"   é¸æ“‡é »ç‡é–¾å€¼: {self.stability_threshold}\")\n",
    "        \n",
    "        feature_selection_counts = {col: 0 for col in X.columns}\n",
    "        \n",
    "        for i in range(self.stability_n_iter):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   é€²åº¦: {i+1}/{self.stability_n_iter}\")\n",
    "            \n",
    "            # å­æŠ½æ¨£ï¼ˆ80% æ•¸æ“šï¼‰\n",
    "            n_samples = int(len(X) * 0.8)\n",
    "            sample_idx = np.random.choice(X.index, size=n_samples, replace=False)\n",
    "            X_sub = X.loc[sample_idx]\n",
    "            y_sub = y.loc[sample_idx]\n",
    "            w_sub = sample_weight.loc[sample_idx] if sample_weight is not None else None\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹ä¸¦ç²å–ç‰¹å¾µé‡è¦æ€§\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=10,\n",
    "                min_samples_split=20,\n",
    "                random_state=i,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            \n",
    "            rf.fit(X_sub, y_sub, sample_weight=w_sub)\n",
    "            \n",
    "            # ç²å–ç‰¹å¾µé‡è¦æ€§ï¼ˆä½¿ç”¨ MDIï¼‰\n",
    "            importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "            top_features = importances.nlargest(int(len(X.columns) * 0.3)).index  # ä¿ç•™å‰30%\n",
    "            \n",
    "            for feat in top_features:\n",
    "                feature_selection_counts[feat] += 1\n",
    "        \n",
    "        # è¨ˆç®—é¸æ“‡é »ç‡\n",
    "        selection_freq = pd.Series(feature_selection_counts) / self.stability_n_iter\n",
    "        selection_freq = selection_freq.sort_values(ascending=False)\n",
    "        \n",
    "        # æ ¹æ“šé–¾å€¼é¸æ“‡ç‰¹å¾µ\n",
    "        selected = selection_freq[selection_freq >= self.stability_threshold].index.tolist()\n",
    "        removed_count = len(X.columns) - len(selected)\n",
    "        \n",
    "        print(f\"\\n   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä¸ç©©å®šç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected):,}\")\n",
    "        print(f\"\\n   Top 10 æœ€ç©©å®šç‰¹å¾µ:\")\n",
    "        for i, (feat, freq) in enumerate(selection_freq.head(10).items(), 1):\n",
    "            print(f\"     {i:2d}. {feat}: {freq:.4f}\")\n",
    "        \n",
    "        self.filter_results['stability'] = {\n",
    "            'frequencies': selection_freq.to_dict(),\n",
    "            'selected': selected,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[selected]\n",
    "    \n",
    "    def fit_transform(self, X: pd.DataFrame, y: pd.Series, \n",
    "                     sample_weight=None, use_stability=False):\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œå®Œæ•´çš„ç¯©é¸æµç¨‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "        y : pd.Series\n",
    "            æ¨™ç±¤\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        use_stability : bool\n",
    "            æ˜¯å¦ä½¿ç”¨ Stability Selectionï¼ˆè¼ƒæ…¢ï¼‰\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_selected : pd.DataFrame\n",
    "            ç¯©é¸å¾Œçš„ç‰¹å¾µçŸ©é™£\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ğŸš€ é–‹å§‹åŸ·è¡Œç‰¹å¾µç¯©é¸ Pipeline\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"åŸå§‹æ¨£æœ¬æ•¸: {len(X):,}\")\n",
    "        \n",
    "        X_current = X.copy()\n",
    "        \n",
    "        # 1. Correlation Filter\n",
    "        X_current = self.correlation_filter(X_current)\n",
    "        \n",
    "        # 2. Variance Filter\n",
    "        X_current = self.variance_filter(X_current)\n",
    "        \n",
    "        # 3. IC Ranking\n",
    "        X_current = self.ic_ranking(X_current, y)\n",
    "        \n",
    "        # 4. ANOVA Filter\n",
    "        X_current = self.anova_filter(X_current, y)\n",
    "        \n",
    "        # 5. MDA Importance\n",
    "        X_current = self.mda_importance(X_current, y, sample_weight)\n",
    "        \n",
    "        # 6. Stability Selection (å¯é¸ï¼Œè¼ƒæ…¢)\n",
    "        if use_stability:\n",
    "            X_current = self.stability_selection(X_current, y, sample_weight)\n",
    "        \n",
    "        self.selected_features = list(X_current.columns)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"âœ… ç‰¹å¾µç¯©é¸å®Œæˆ\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"æœ€çµ‚ç‰¹å¾µæ•¸: {len(self.selected_features):,}\")\n",
    "        print(f\"ç‰¹å¾µæ¸›å°‘æ¯”ä¾‹: {(1 - len(self.selected_features) / len(X.columns)) * 100:.2f}%\")\n",
    "        \n",
    "        return X_current\n",
    "\n",
    "# =====================================================\n",
    "# åŸ·è¡Œç‰¹å¾µç¯©é¸\n",
    "# =====================================================\n",
    "\n",
    "# åˆå§‹åŒ– Pipeline\n",
    "filter_pipeline = FeatureFilteringPipeline(\n",
    "    corr_threshold=0.95,          # ç›¸é—œæ€§é–¾å€¼\n",
    "    variance_threshold=0.005,      # è®Šç•°æ•¸é–¾å€¼\n",
    "    ic_threshold=None,            # IC é–¾å€¼ï¼ˆNone è¡¨ç¤ºä¿ç•™ top 50%ï¼‰\n",
    "    anova_p_threshold=0.2,       # ANOVA p-value é–¾å€¼\n",
    "    mda_top_n=None,               # MDA ä¿ç•™æ‰€æœ‰é‡è¦æ€§ > 0 çš„ç‰¹å¾µ\n",
    "    stability_n_iter=5,          # Stability Selection è¿­ä»£æ¬¡æ•¸\n",
    "    stability_threshold=0.1      # Stability Selection é »ç‡é–¾å€¼\n",
    ")\n",
    "\n",
    "# åŸ·è¡Œç¯©é¸ï¼ˆä¸ä½¿ç”¨ Stability Selectionï¼Œå› ç‚ºè¼ƒæ…¢ï¼‰\n",
    "# å¦‚æœéœ€è¦æœ€ç©©å®šçš„ç‰¹å¾µï¼Œå¯ä»¥è¨­ç½® use_stability=True\n",
    "X_train_filtered = filter_pipeline.fit_transform(\n",
    "    X_train_filter, \n",
    "    y_train_filter,\n",
    "    sample_weight=sample_weight.loc[train_mask] if sample_weight is not None else None,\n",
    "    use_stability=False  # è¨­ç‚º True å¯å•Ÿç”¨ Stability Selectionï¼ˆè¼ƒæ…¢ï¼‰\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0b0a1",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ7ï¼šML Modelæ§‹å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_continuous\n",
    "import numpy as np\n",
    "\n",
    "class logUniform_gen(rv_continuous):\n",
    "    \"\"\"\n",
    "    Log-uniform distribution random variable generator\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    éš¨æ©Ÿè®Šæ•¸ x åœ¨ [a, b] å€é–“å…§éµå¾ª log-uniform åˆ†å¸ƒï¼Œ\n",
    "    ç•¶ä¸”åƒ…ç•¶ log[x] ~ U[log[a], log[b]]\n",
    "    \n",
    "    é€™å€‹åˆ†å¸ƒå°æ–¼æ¢ç´¢éç·šæ€§éŸ¿æ‡‰çš„åƒæ•¸ç©ºé–“ç‰¹åˆ¥æœ‰æ•ˆï¼Œ\n",
    "    ä¾‹å¦‚ SVC çš„ C åƒæ•¸å’Œ RBF kernel çš„ gamma åƒæ•¸ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cdf(self, x):\n",
    "        \"\"\"ç´¯ç©åˆ†å¸ƒå‡½æ•¸ (CDF)\"\"\"\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def logUniform(a=1, b=np.exp(1)):\n",
    "    \"\"\"\n",
    "    å‰µå»º log-uniform åˆ†å¸ƒ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    a : float\n",
    "        ä¸‹ç•Œï¼ˆå¿…é ˆ > 0ï¼‰\n",
    "    b : float\n",
    "        ä¸Šç•Œï¼ˆå¿…é ˆ > aï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    logUniform_gen : logUniform_gen\n",
    "        Log-uniform åˆ†å¸ƒç”Ÿæˆå™¨\n",
    "    \"\"\"\n",
    "    return logUniform_gen(a=a, b=b, name='logUniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ7ï¼šML Model æ§‹å»ºèˆ‡è¨“ç·´ï¼ˆæ“´å±•ç‰ˆ - æ”¯æ´å¤šç¨®æ¨¡å‹ï¼‰\n",
    "# =====================================================\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              ExtraTreesClassifier, AdaBoostClassifier,\n",
    "                              HistGradientBoostingClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            roc_auc_score, precision_recall_curve, \n",
    "                            roc_curve, accuracy_score, f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, Any, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    æ“´å±•ç‰ˆæ¨¡å‹è¨“ç·´å™¨ï¼šæ”¯æ´å¤šç¨®æ¨¡å‹\n",
    "    \n",
    "    æ”¯æ´çš„æ¨¡å‹é¡å‹:\n",
    "    -----\n",
    "    - 'xgboost': XGBoost\n",
    "    - 'lightgbm': LightGBM\n",
    "    - 'catboost': CatBoost\n",
    "    - 'rf': Random Forest\n",
    "    - 'et': Extra Trees\n",
    "    - 'gb': Gradient Boosting\n",
    "    - 'histgbm': HistGradientBoosting\n",
    "    - 'dt': Decision Tree\n",
    "    - 'lr': Logistic Regression\n",
    "    - 'svm': Support Vector Machine\n",
    "    - 'mlp': Multi-Layer Perceptron\n",
    "    - 'lstm': LSTM (éœ€è¦é¡å¤–è™•ç†)\n",
    "    - 'ada': AdaBoost\n",
    "    - 'knn': K-Nearest Neighbors\n",
    "    - 'nb': Naive Bayes\n",
    "    - 'qda': Quadratic Discriminant Analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'xgboost'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ¨¡å‹è¨“ç·´å™¨\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            æ¨¡å‹é¡å‹\n",
    "        \"\"\"\n",
    "        self.model_type = model_type.lower()\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.cv_scores = None\n",
    "        \n",
    "    def _create_model(self, **kwargs):\n",
    "        \"\"\"\n",
    "        å‰µå»ºæ¨¡å‹å¯¦ä¾‹\n",
    "        \"\"\"\n",
    "        if self.model_type == 'xgboost':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'eval_metric': 'logloss',\n",
    "                'use_label_encoder': False,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return XGBClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'lightgbm':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'verbosity': -1,\n",
    "                'force_col_wise': True\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return LGBMClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'catboost':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'verbose': False,\n",
    "                'thread_count': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return CatBoostClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return RandomForestClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'et':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return ExtraTreesClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'gb':\n",
    "            default_params = {\n",
    "                'random_state': 42\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return GradientBoostingClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'histgbm':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_iter': 100,\n",
    "                'early_stopping': True\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return HistGradientBoostingClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'dt':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_depth': 10\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return DecisionTreeClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'lr':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_iter': 1000,\n",
    "                'solver': 'lbfgs'\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return LogisticRegression(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'svm':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'probability': True,\n",
    "                'kernel': 'rbf'\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return SVC(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'mlp':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_iter': 500,\n",
    "                'early_stopping': True,\n",
    "                'validation_fraction': 0.1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return MLPClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'ada':\n",
    "            default_params = {\n",
    "                'random_state': 42\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return AdaBoostClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'knn':\n",
    "            default_params = {\n",
    "                'n_neighbors': 5,\n",
    "                'weights': 'distance',\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return KNeighborsClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'nb':\n",
    "            default_params = {}\n",
    "            default_params.update(kwargs)\n",
    "            return GaussianNB(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'qda':\n",
    "            default_params = {}\n",
    "            default_params.update(kwargs)\n",
    "            return QuadraticDiscriminantAnalysis(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'lstm':\n",
    "            # LSTM éœ€è¦ç‰¹æ®Šè™•ç†ï¼Œé€™è£¡å…ˆæ‹‹å‡ºæç¤º\n",
    "            raise NotImplementedError(\n",
    "                \"LSTM éœ€è¦åºåˆ—æ•¸æ“šå’Œç‰¹æ®Šè™•ç†ï¼Œè«‹ä½¿ç”¨å°ˆé–€çš„æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼ˆå¦‚ TensorFlow/Kerasï¼‰\"\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}\")\n",
    "    \n",
    "    def _get_param_grid(self, use_randomized: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å–å¾—åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        print(f\"ğŸš€ å–å¾— {self.model_type} æ¨¡å‹åƒæ•¸ç¶²æ ¼\")\n",
    "        if self.model_type == 'xgboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lightgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10, -1],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__num_leaves': [15, 31, 50, 100],\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_samples': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__num_leaves': [31, 50],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'catboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__l2_leaf_reg': [1, 3, 5, 7],\n",
    "                    f'{self.model_type}__border_count': [32, 64, 128],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__l2_leaf_reg': [3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500, 1000],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': logUniform(1e-3, 1e2),\n",
    "                    f'{self.model_type}__min_samples_leaf': logUniform(1e-3, 1e1),\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                    f'{self.model_type}__bootstrap': [True, False]\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None]\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'et':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'gb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200],\n",
    "                    f'{self.model_type}__max_depth': [3, 5],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'histgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200, 300],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'dt':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "            }\n",
    "            return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lr':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-3, 1e3),\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.01, 0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'svm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-2, 1e2),\n",
    "                    f'{self.model_type}__gamma': logUniform(1e-4, 1e-1),\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'mlp':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                    f'{self.model_type}__alpha': logUniform(1e-5, 1e-1),\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "                    f'{self.model_type}__alpha': [0.0001, 0.001, 0.01],\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'ada':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_estimators': [50, 100, 200],\n",
    "                f'{self.model_type}__learning_rate': [0.01, 0.1, 1.0],\n",
    "            }\n",
    "            return param_grid   \n",
    "        \n",
    "        elif self.model_type == 'knn':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_neighbors': [3, 5, 7, 10, 15],\n",
    "                f'{self.model_type}__weights': ['uniform', 'distance'],\n",
    "                f'{self.model_type}__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "            }\n",
    "            return param_grid\n",
    "        elif self.model_type == 'nb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': logUniform(1e-9, 1e-3),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6],\n",
    "                }\n",
    "                return param_grid\n",
    "        elif self.model_type == 'qda':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': logUniform(1e-3, 1.0),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': [0.0, 0.1, 0.5, 1.0],\n",
    "                }   \n",
    "                return param_grid\n",
    "        else:\n",
    "            # å¦‚æœæ¨¡å‹é¡å‹æ²’æœ‰å°æ‡‰çš„åƒæ•¸ç¶²æ ¼ï¼Œè¿”å›ç©ºå­—å…¸\n",
    "            param_grid = {}\n",
    "            print(f\"âš ï¸ è­¦å‘Š: æ¨¡å‹é¡å‹ '{self.model_type}' æ²’æœ‰å®šç¾©åƒæ•¸ç¶²æ ¼ï¼Œä½¿ç”¨ç©ºå­—å…¸\")\n",
    "        \n",
    "        return param_grid\n",
    "    \n",
    "    def train(self, \n",
    "              X: pd.DataFrame,\n",
    "              y: pd.Series,\n",
    "              t1: pd.Series,\n",
    "              sample_weight: Optional[pd.Series] = None,\n",
    "              cv: int = 5,\n",
    "              pctEmbargo: float = 0.01,\n",
    "              use_hyperopt: bool = True,\n",
    "              use_randomized: bool = False,\n",
    "              rndSearchIter: int = 50,\n",
    "              n_jobs: int = -1,\n",
    "              **model_kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸš€ é–‹å§‹è¨“ç·´ {self.model_type.upper()} æ¨¡å‹\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if sample_weight is None:\n",
    "            sample_weight = pd.Series(1.0, index=X.index)\n",
    "        \n",
    "        # åˆ¤æ–·æ˜¯å¦éœ€è¦æ¨™æº–åŒ–ï¼ˆç·šæ€§æ¨¡å‹å’Œ SVMã€MLP éœ€è¦ï¼‰\n",
    "        needs_scaling = self.model_type in ['lr', 'svm', 'mlp', 'knn', 'nb', 'qda']\n",
    "        \n",
    "        if use_hyperopt:\n",
    "            print(f\"\\nğŸ“Š ä½¿ç”¨ {'RandomizedSearchCV' if use_randomized else 'GridSearchCV'} é€²è¡Œè¶…åƒæ•¸èª¿å„ª...\")\n",
    "            \n",
    "            # å‰µå»º pipeline\n",
    "            steps = []\n",
    "            if needs_scaling:\n",
    "                steps.append(('scaler', StandardScaler()))\n",
    "            steps.append((self.model_type, self._create_model(**model_kwargs)))\n",
    "            \n",
    "            pipe_clf = MyPipeline(steps)\n",
    "            \n",
    "            param_grid = self._get_param_grid(use_randomized=use_randomized)\n",
    "            \n",
    "            # æº–å‚™ fit_paramsï¼ˆæŸäº›æ¨¡å‹ä¸æ”¯æ´ sample_weightï¼‰\n",
    "            fit_params = {}\n",
    "            if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda']:\n",
    "                fit_params[f'{self.model_type}__sample_weight'] = sample_weight\n",
    "            \n",
    "            best_pipeline = clfHyperFit(\n",
    "                feat=X,\n",
    "                lbl=y,\n",
    "                t1=t1,\n",
    "                pipe_clf=pipe_clf,\n",
    "                param_grid=param_grid,\n",
    "                cv=cv,\n",
    "                bagging=[0, None, 1.],\n",
    "                rndSearchIter=rndSearchIter if use_randomized else 0,\n",
    "                n_jobs=n_jobs,\n",
    "                pctEmbargo=pctEmbargo,\n",
    "                **fit_params\n",
    "            )\n",
    "            \n",
    "            self.model = best_pipeline.named_steps[self.model_type]\n",
    "            \n",
    "            if hasattr(best_pipeline, 'best_params_'):\n",
    "                self.best_params = best_pipeline.best_params_\n",
    "            \n",
    "            print(f\"\\nâœ… è¶…åƒæ•¸èª¿å„ªå®Œæˆ\")\n",
    "            if self.best_params:\n",
    "                print(f\"æœ€ä½³åƒæ•¸:\")\n",
    "                for key, value in list(self.best_params.items())[:5]:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                if len(self.best_params) > 5:\n",
    "                    print(f\"  ... (å…± {len(self.best_params)} å€‹åƒæ•¸)\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ“Š ç›´æ¥è¨“ç·´æ¨¡å‹ï¼ˆä¸ä½¿ç”¨è¶…åƒæ•¸èª¿å„ªï¼‰...\")\n",
    "            \n",
    "            inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "            \n",
    "            # å°æ–¼éœ€è¦æ¨™æº–åŒ–çš„æ¨¡å‹ï¼Œä½¿ç”¨ Pipeline\n",
    "            if needs_scaling:\n",
    "                from sklearn.pipeline import Pipeline\n",
    "                clf = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('clf', self._create_model(**model_kwargs))\n",
    "                ])\n",
    "                fit_params = {}\n",
    "                if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda']:\n",
    "                    fit_params['clf__sample_weight'] = sample_weight\n",
    "            else:\n",
    "                clf = self._create_model(**model_kwargs)\n",
    "                fit_params = {}\n",
    "                if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda']:\n",
    "                    fit_params['sample_weight'] = sample_weight\n",
    "            \n",
    "            self.cv_scores = cvScore(\n",
    "                clf=clf,\n",
    "                X=X,\n",
    "                y=y,\n",
    "                sample_weight=sample_weight if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda'] else None,\n",
    "                scoring='f1',\n",
    "                cvGen=inner_cv\n",
    "            )\n",
    "            \n",
    "            print(f\"\\näº¤å‰é©—è­‰åˆ†æ•¸: {self.cv_scores}\")\n",
    "            print(f\"å¹³å‡åˆ†æ•¸: {self.cv_scores.mean():.4f} Â± {self.cv_scores.std():.4f}\")\n",
    "            \n",
    "            if needs_scaling:\n",
    "                from sklearn.pipeline import Pipeline\n",
    "                self.model = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('clf', self._create_model(**model_kwargs))\n",
    "                ])\n",
    "                self.model.fit(X, y, **fit_params)\n",
    "            else:\n",
    "                self.model = clf.fit(X, y, **fit_params)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"é€²è¡Œé æ¸¬\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        predictions = self.model.predict(X)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def evaluate(self, \n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight: Optional[pd.Series] = None) -> Dict[str, float]:\n",
    "        \"\"\"è©•ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        y_pred, y_proba = self.predict(X)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred, sample_weight=sample_weight),\n",
    "            'f1': f1_score(y, y_pred, sample_weight=sample_weight, average='weighted'),\n",
    "        }\n",
    "        \n",
    "        if len(np.unique(y)) == 2:\n",
    "            metrics['roc_auc'] = roc_auc_score(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            metrics['f1_binary'] = f1_score(y, y_pred, sample_weight=sample_weight)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self, \n",
    "                    X: pd.DataFrame,\n",
    "                    y: pd.Series,\n",
    "                    sample_weight: Optional[pd.Series] = None):\n",
    "        \"\"\"ç¹ªè£½æ¨¡å‹è©•ä¼°çµæœï¼ˆèˆ‡åŸç‰ˆç›¸åŒï¼‰\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        y_pred, y_proba = self.predict(X)\n",
    "        metrics = self.evaluate(X, y, sample_weight)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. æ··æ·†çŸ©é™£\n",
    "        cm = confusion_matrix(y, y_pred, sample_weight=sample_weight)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "        axes[0, 0].set_ylabel('True Label', fontsize=12)\n",
    "        axes[0, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "        \n",
    "        # 2. ROC æ›²ç·š\n",
    "        if len(np.unique(y)) == 2:\n",
    "            fpr, tpr, _ = roc_curve(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
    "            axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n",
    "            axes[0, 1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "            axes[0, 1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "            axes[0, 1].set_title('ROC Curve', fontsize=14)\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'ROC Curve\\n(Only for binary classification)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 1].set_title('ROC Curve', fontsize=14)\n",
    "        \n",
    "        # 3. Precision-Recall æ›²ç·š\n",
    "        if len(np.unique(y)) == 2:\n",
    "            precision, recall, _ = precision_recall_curve(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            axes[1, 0].plot(recall, precision, linewidth=2)\n",
    "            axes[1, 0].set_xlabel('Recall', fontsize=12)\n",
    "            axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
    "            axes[1, 0].set_title('Precision-Recall Curve', fontsize=14)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Precision-Recall Curve\\n(Only for binary classification)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 0].set_title('Precision-Recall Curve', fontsize=14)\n",
    "        \n",
    "        # 4. è©•ä¼°æŒ‡æ¨™æ‘˜è¦\n",
    "        axes[1, 1].axis('off')\n",
    "        metrics_text = f\"\"\"\n",
    "        Model: {self.model_type.upper()}\n",
    "        \n",
    "        Evaluation Metrics:\n",
    "        --------------------\n",
    "        Accuracy: {metrics['accuracy']:.4f}\n",
    "        F1 Score: {metrics['f1']:.4f}\n",
    "        \"\"\"\n",
    "        if 'roc_auc' in metrics:\n",
    "            metrics_text += f\"ROC AUC: {metrics['roc_auc']:.4f}\\n\"\n",
    "        if 'f1_binary' in metrics:\n",
    "            metrics_text += f\"F1 (Binary): {metrics['f1_binary']:.4f}\\n\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, \n",
    "                        verticalalignment='center', family='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š è©³ç´°åˆ†é¡å ±å‘Š\")\n",
    "        print(\"=\" * 60)\n",
    "        print(classification_report(y, y_pred, sample_weight=sample_weight))\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc72f07",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ8ï¼šEmbargo Purged K-Fold Classäº¤å‰é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Purged K-Fold Cross-Validation (Snippet 7.3 & 7.4)\n",
    "# =====================================================\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Generator, Tuple\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "class PurgedKFold(_BaseKFold):\n",
    "    \"\"\"\n",
    "    Extend KFold class to work with labels that span intervals\n",
    "    \n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    é€™å€‹é¡åˆ¥æ“´å±•äº† scikit-learn çš„ KFoldï¼Œç”¨æ–¼è™•ç†æ¨™ç±¤é‡ç–Šçš„æƒ…æ³ã€‚\n",
    "    ç•¶æ¨™ç±¤è·¨è¶Šæ™‚é–“å€é–“æ™‚ï¼ˆä¾‹å¦‚ Triple Barrier Methodï¼‰ï¼Œæ¸¬è©¦é›†çš„æ¨™ç±¤\n",
    "    å¯èƒ½æœƒèˆ‡è¨“ç·´é›†çš„æ¨™ç±¤é‡ç–Šï¼Œå°è‡´æ•¸æ“šæ´©æ¼ã€‚\n",
    "    \n",
    "    è§£æ±ºæ–¹æ³•:\n",
    "    ---------\n",
    "    1. Purgingï¼ˆæ¸…é™¤ï¼‰: å¾è¨“ç·´é›†ä¸­ç§»é™¤èˆ‡æ¸¬è©¦é›†æ¨™ç±¤å€é–“é‡ç–Šçš„è§€æ¸¬å€¼\n",
    "    2. Embargoï¼ˆç¦åˆ¶ï¼‰: åœ¨æ¸¬è©¦é›†çµæŸå¾Œï¼Œé¡å¤–ç§»é™¤ä¸€æ®µæ™‚é–“çš„è¨“ç·´æ¨£æœ¬\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    n_splits : int\n",
    "        K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "    t1 : pd.Series\n",
    "        æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰ï¼Œindex å¿…é ˆèˆ‡ X çš„ index ä¸€è‡´\n",
    "    pctEmbargo : float\n",
    "        ç¦åˆ¶æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œä¾‹å¦‚ 0.01 è¡¨ç¤ºç¦åˆ¶ 1% çš„æ•¸æ“š\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 3, t1: Optional[pd.Series] = None, pctEmbargo: float = 0.0):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– PurgedKFold\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        n_splits : int\n",
    "            K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "        t1 : pd.Series, optional\n",
    "            æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰ï¼Œindex å¿…é ˆèˆ‡ X çš„ index ä¸€è‡´\n",
    "        pctEmbargo : float\n",
    "            ç¦åˆ¶æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œä¾‹å¦‚ 0.01 è¡¨ç¤ºç¦åˆ¶ 1% çš„æ•¸æ“š\n",
    "        \"\"\"\n",
    "        if t1 is not None and not isinstance(t1, pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pd.Series')\n",
    "        \n",
    "        super(PurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.t1 = t1\n",
    "        self.pctEmbargo = pctEmbargo\n",
    "    \n",
    "    def split(self, X: pd.DataFrame, y: Optional[pd.Series] = None, \n",
    "              groups: Optional[np.ndarray] = None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆè¨“ç·´/æ¸¬è©¦é›†çš„ç´¢å¼•\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µæ•¸æ“šï¼ˆindex å¿…é ˆèˆ‡ t1 çš„ index ä¸€è‡´ï¼‰\n",
    "        y : pd.Series, optional\n",
    "            æ¨™ç±¤ï¼ˆæœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ç¬¦åˆ scikit-learn æ¥å£ï¼‰\n",
    "        groups : np.ndarray, optional\n",
    "            åˆ†çµ„ï¼ˆæœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ç¬¦åˆ scikit-learn æ¥å£ï¼‰\n",
    "            \n",
    "        Yields:\n",
    "        -------\n",
    "        train_indices : np.ndarray\n",
    "            è¨“ç·´é›†ç´¢å¼•\n",
    "        test_indices : np.ndarray\n",
    "            æ¸¬è©¦é›†ç´¢å¼•\n",
    "        \"\"\"\n",
    "        if self.t1 is None:\n",
    "            raise ValueError('t1 (ThruDate) must be provided')\n",
    "        \n",
    "        # é©—è­‰ X å’Œ t1 çš„ index ä¸€è‡´\n",
    "        if (X.index == self.t1.index).sum() != len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "        \n",
    "        indices = np.arange(X.shape[0])\n",
    "        mbrg = int(X.shape[0] * self.pctEmbargo)  # embargo çš„æ¨£æœ¬æ•¸\n",
    "        \n",
    "        # å°‡æ•¸æ“šåˆ†æˆ n_splits å€‹é€£çºŒçš„æ¸¬è©¦é›†\n",
    "        test_starts = [(i[0], i[-1] + 1) for i in \n",
    "                      np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "        \n",
    "        for i, j in test_starts:\n",
    "            # æ¸¬è©¦é›†çš„èµ·å§‹æ™‚é–“\n",
    "            t0 = self.t1.index[i]  # start of test set\n",
    "            test_indices = indices[i:j]\n",
    "            \n",
    "            # æ‰¾åˆ°æ¸¬è©¦é›†ä¸­æœ€æ™šçš„çµæŸæ™‚é–“\n",
    "            maxT1Idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
    "            \n",
    "            # å·¦å´è¨“ç·´é›†ï¼šçµæŸæ™‚é–“ <= t0 çš„æ‰€æœ‰æ¨£æœ¬\n",
    "            train_indices = self.t1.index.searchsorted(\n",
    "                self.t1[self.t1 <= t0].index\n",
    "            )\n",
    "            \n",
    "            # å³å´è¨“ç·´é›†ï¼šåœ¨æ¸¬è©¦é›†çµæŸå¾Œï¼ŒåŠ ä¸Š embargo æœŸé–“\n",
    "            if maxT1Idx < X.shape[0]:\n",
    "                # right train (with embargo)\n",
    "                train_indices = np.concatenate((\n",
    "                    train_indices, \n",
    "                    indices[maxT1Idx + mbrg:]\n",
    "                ))\n",
    "            \n",
    "            yield train_indices, test_indices\n",
    "\n",
    "\n",
    "def macro_accuracy_score(y_true, y_pred, sample_weight=None, zero_division=0):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Macro Accuracyï¼ˆæ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡ç›´æ¥å¹³å‡ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        çœŸå¯¦æ¨™ç±¤\n",
    "    y_pred : array-like\n",
    "        é æ¸¬æ¨™ç±¤\n",
    "    sample_weight : array-like, optional\n",
    "        æ¨£æœ¬æ¬Šé‡\n",
    "    zero_division : float\n",
    "        ç•¶åˆ†æ¯ç‚º 0 æ™‚è¿”å›çš„å€¼\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    macro_accuracy : float\n",
    "        Macro Accuracyï¼ˆæ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡ç›´æ¥å¹³å‡ï¼‰\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.utils.multiclass import unique_labels\n",
    "    import numpy as np\n",
    "    \n",
    "    # ç²å–æ‰€æœ‰é¡åˆ¥\n",
    "    labels = unique_labels(y_true, y_pred)\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡\n",
    "    per_class_accuracies = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # æ‰¾å‡ºè©²é¡åˆ¥çš„æ‰€æœ‰æ¨£æœ¬\n",
    "        mask = (y_true == label)\n",
    "        \n",
    "        if mask.sum() == 0:\n",
    "            # å¦‚æœè©²é¡åˆ¥åœ¨çœŸå¯¦æ¨™ç±¤ä¸­ä¸å­˜åœ¨ï¼Œè·³é\n",
    "            continue\n",
    "            \n",
    "        # è©²é¡åˆ¥çš„æ¨£æœ¬\n",
    "        y_true_class = y_true[mask]\n",
    "        y_pred_class = y_pred[mask]\n",
    "        \n",
    "        # å¦‚æœæœ‰æ¨£æœ¬æ¬Šé‡\n",
    "        if sample_weight is not None:\n",
    "            sample_weight_class = sample_weight[mask]\n",
    "        else:\n",
    "            sample_weight_class = None\n",
    "        \n",
    "        # è¨ˆç®—è©²é¡åˆ¥çš„æº–ç¢ºç‡\n",
    "        class_accuracy = accuracy_score(\n",
    "            y_true_class, \n",
    "            y_pred_class, \n",
    "            sample_weight=sample_weight_class\n",
    "        )\n",
    "        per_class_accuracies.append(class_accuracy)\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰ä»»ä½•é¡åˆ¥ï¼Œè¿”å› zero_division\n",
    "    if len(per_class_accuracies) == 0:\n",
    "        return zero_division\n",
    "    \n",
    "    # è¿”å›å¹³å‡æº–ç¢ºç‡ï¼ˆMacro Accuracyï¼‰\n",
    "    return np.mean(per_class_accuracies)\n",
    "\n",
    "def cvScore(\n",
    "    clf,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    sample_weight: Optional[pd.Series] = None,\n",
    "    scoring: str = 'f1',\n",
    "    t1: Optional[pd.Series] = None,\n",
    "    cv: Optional[int] = None,\n",
    "    cvGen: Optional[PurgedKFold] = None,\n",
    "    pctEmbargo: Optional[float] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ PurgedKFold é€²è¡Œäº¤å‰é©—è­‰è©•åˆ†ï¼Œç¢ºä¿æ¯æŠ˜æ¨™ç±¤é‡æ–°ç·¨ç¢¼ç‚ºé€£çºŒæ•´æ•¸ã€‚\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import (\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        accuracy_score,\n",
    "        log_loss,\n",
    "    )\n",
    "\n",
    "    if cvGen is None:\n",
    "        if cv is None:\n",
    "            raise ValueError(\"è«‹æä¾› cv æˆ– cvGen å…¶ä¸­ä¹‹ä¸€\")\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = pd.Series(1.0, index=X.index)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cvGen.split(X=X)):\n",
    "        X_train_fold = X.iloc[train_idx, :]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx, :]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "\n",
    "        fold_label_encoder = LabelEncoder()\n",
    "        y_train_fold_encoded = pd.Series(\n",
    "            fold_label_encoder.fit_transform(y_train_fold),\n",
    "            index=y_train_fold.index,\n",
    "        )\n",
    "        y_test_fold_encoded = pd.Series(\n",
    "            fold_label_encoder.transform(y_test_fold),\n",
    "            index=y_test_fold.index,\n",
    "        )\n",
    "\n",
    "        sw_train = sample_weight.iloc[train_idx].values\n",
    "        sw_test = sample_weight.iloc[test_idx].values\n",
    "\n",
    "        fit = clf.fit(\n",
    "            X=X_train_fold,\n",
    "            y=y_train_fold_encoded,\n",
    "            sample_weight=sw_train,\n",
    "        )\n",
    "\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = fit.predict_proba(X_test_fold)\n",
    "            score_ = -log_loss(\n",
    "                y_test_fold_encoded,\n",
    "                prob,\n",
    "                sample_weight=sw_test,\n",
    "            )\n",
    "        elif scoring in ('f1', 'f1_macro'):\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = f1_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'f1_weighted':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = f1_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='weighted',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'precision_macro':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = precision_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'recall_macro':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = recall_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'accuracy':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = accuracy_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                sample_weight=sw_test,\n",
    "            )\n",
    "        else:\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = f1_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "\n",
    "        scores.append(score_)\n",
    "\n",
    "    return np.array(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23efcb",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ9 ï¼š è¶…åƒæ•¸èª¿æ•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Hyperparameter Tuning with Purged K-Fold CV\n",
    "# Snippet 9.1, 9.2, 9.3, 9.4\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from scipy.stats import rv_continuous, kstest\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.2: Enhanced Pipeline Class\n",
    "# =====================================================\n",
    "\n",
    "class MyPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Enhanced Pipeline class that handles sample_weight argument\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    sklearn çš„ Pipeline çš„ fit æ–¹æ³•ä¸æ¥å— sample_weight åƒæ•¸ï¼Œ\n",
    "    è€Œæ˜¯æœŸæœ› fit_params é—œéµå­—åƒæ•¸ã€‚é€™æ˜¯ä¸€å€‹å·²çŸ¥çš„ bugã€‚\n",
    "    é€™å€‹é¡åˆ¥æ“´å±•äº† Pipelineï¼Œé‡å¯« fit æ–¹æ³•ä»¥è™•ç† sample_weightã€‚\n",
    "    \n",
    "    åƒè€ƒ:\n",
    "    -----\n",
    "    - GitHub issue: sklearn Pipeline sample_weight bug\n",
    "    - Stackoverflow: Understanding Python super with __init__ methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \"\"\"\n",
    "        é‡å¯« fit æ–¹æ³•ä»¥è™•ç† sample_weight\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            ç‰¹å¾µæ•¸æ“š\n",
    "        y : array-like\n",
    "            æ¨™ç±¤\n",
    "        sample_weight : array-like, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        **fit_params : dict\n",
    "            å…¶ä»– fit åƒæ•¸\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : MyPipeline\n",
    "            æ“¬åˆå¾Œçš„ pipeline\n",
    "        \"\"\"\n",
    "        if sample_weight is not None:\n",
    "            # å°‡ sample_weight å‚³éçµ¦æœ€å¾Œä¸€å€‹æ­¥é©Ÿï¼ˆé€šå¸¸æ˜¯åˆ†é¡å™¨ï¼‰\n",
    "            last_step_name = self.steps[-1][0]\n",
    "            fit_params[f'{last_step_name}__sample_weight'] = sample_weight\n",
    "        \n",
    "        return super(MyPipeline, self).fit(X, y, **fit_params)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.4: Log-Uniform Distribution\n",
    "# =====================================================\n",
    "\n",
    "class logUniform_gen(rv_continuous):\n",
    "    \"\"\"\n",
    "    Log-uniform distribution random variable generator\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    éš¨æ©Ÿè®Šæ•¸ x åœ¨ [a, b] å€é–“å…§éµå¾ª log-uniform åˆ†å¸ƒï¼Œ\n",
    "    ç•¶ä¸”åƒ…ç•¶ log[x] ~ U[log[a], log[b]]\n",
    "    \n",
    "    é€™å€‹åˆ†å¸ƒå°æ–¼æ¢ç´¢éç·šæ€§éŸ¿æ‡‰çš„åƒæ•¸ç©ºé–“ç‰¹åˆ¥æœ‰æ•ˆï¼Œ\n",
    "    ä¾‹å¦‚ SVC çš„ C åƒæ•¸å’Œ RBF kernel çš„ gamma åƒæ•¸ã€‚\n",
    "    \n",
    "    æ•¸å­¸å®šç¾©:\n",
    "    ---------\n",
    "    CDF: F[x] = log[x] - log[a] / log[b] - log[a]  for a â‰¤ x â‰¤ b\n",
    "    PDF: f[x] = 1 / (x * log[b/a])                 for a â‰¤ x â‰¤ b\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cdf(self, x):\n",
    "        \"\"\"ç´¯ç©åˆ†å¸ƒå‡½æ•¸ (CDF)\"\"\"\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def logUniform(a=1, b=np.exp(1)):\n",
    "    \"\"\"\n",
    "    å‰µå»º log-uniform åˆ†å¸ƒ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    a : float\n",
    "        ä¸‹ç•Œï¼ˆå¿…é ˆ > 0ï¼‰\n",
    "    b : float\n",
    "        ä¸Šç•Œï¼ˆå¿…é ˆ > aï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    logUniform_gen : logUniform_gen\n",
    "        Log-uniform åˆ†å¸ƒç”Ÿæˆå™¨\n",
    "    \"\"\"\n",
    "    return logUniform_gen(a=a, b=b, name='logUniform')\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.1 & 9.3: Hyperparameter Fitting Function\n",
    "# =====================================================\n",
    "\n",
    "def clfHyperFit(feat: pd.DataFrame, \n",
    "                lbl: pd.Series,\n",
    "                t1: pd.Series,\n",
    "                pipe_clf: Pipeline,\n",
    "                param_grid: Dict[str, List[Any]],\n",
    "                cv: int = 3,\n",
    "                bagging: List[Any] = [0, None, 1.],\n",
    "                rndSearchIter: int = 0,\n",
    "                n_jobs: int = -1,\n",
    "                pctEmbargo: float = 0.0,\n",
    "                scoring: str = 'f1_macro',  # â­ æ–°å¢åƒæ•¸\n",
    "                **fit_params) -> Pipeline:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Purged K-Fold äº¤å‰é©—è­‰é€²è¡Œè¶…åƒæ•¸èª¿å„ª\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    scoring : str\n",
    "        è©•åˆ†æ–¹æ³•ï¼ˆç”¨æ–¼ GridSearchCV/RandomizedSearchCVï¼‰\n",
    "        - 'f1_macro': Macro-F1\n",
    "        - 'f1_weighted': Weighted-F1\n",
    "        - 'accuracy_macro': Macro Accuracy\n",
    "        - 'precision_macro': Macro Precision\n",
    "        - 'recall_macro': Macro Recall\n",
    "        - 'accuracy': æº–ç¢ºç‡\n",
    "        - 'neg_log_loss': è² å°æ•¸æå¤±\n",
    "    **fit_params : dict\n",
    "        å…¶ä»– fit åƒæ•¸ï¼ˆä¾‹å¦‚ sample_weightï¼‰ï¼Œä½†ä¸åŒ…æ‹¬ scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    # â­ é‡è¦ï¼šå¾ fit_params ä¸­ç§»é™¤ scoringï¼ˆå¦‚æœå­˜åœ¨çš„è©±ï¼‰\n",
    "    # å› ç‚º scoring æ˜¯ GridSearchCV çš„åƒæ•¸ï¼Œä¸æ˜¯ Pipeline.fit() çš„åƒæ•¸\n",
    "    fit_params_clean = {k: v for k, v in fit_params.items() if k != 'scoring'}\n",
    "    \n",
    "    # 1) è¶…åƒæ•¸æœå°‹ï¼Œåœ¨è¨“ç·´æ•¸æ“šä¸Š\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "    \n",
    "    # âš ï¸ æ³¨æ„ï¼šsklearn çš„ GridSearchCV/RandomizedSearchCV ä¸ç›´æ¥æ”¯æ´è‡ªå®šç¾©çš„ scoring å­—ç¬¦ä¸²\n",
    "    # éœ€è¦å‰µå»ºä¸€å€‹è‡ªå®šç¾©çš„ scorer æˆ–ä½¿ç”¨ make_scorer\n",
    "    # ä½†ç”±æ–¼æˆ‘å€‘ä½¿ç”¨çš„æ˜¯ PurgedKFoldï¼Œå¯èƒ½éœ€è¦è‡ªå®šç¾©è©•åˆ†é‚è¼¯\n",
    "    \n",
    "    # æš«æ™‚ä½¿ç”¨ scoring åƒæ•¸ï¼ˆå¦‚æœ sklearn æ”¯æ´çš„è©±ï¼‰\n",
    "    # å¦‚æœä¸æ”¯æ´ï¼Œéœ€è¦å‰µå»ºè‡ªå®šç¾© scorer\n",
    "    from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score\n",
    "    \n",
    "    # å‰µå»ºè‡ªå®šç¾© scorerï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "    if scoring == 'f1_macro':\n",
    "        scorer = make_scorer(f1_score, average='macro', zero_division=0)\n",
    "    elif scoring == 'f1_weighted':\n",
    "        scorer = make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "    elif scoring == 'precision_macro':\n",
    "        scorer = make_scorer(precision_score, average='macro', zero_division=0)\n",
    "    elif scoring == 'recall_macro':\n",
    "        scorer = make_scorer(recall_score, average='macro', zero_division=0)\n",
    "    elif scoring == 'accuracy_macro':\n",
    "        # ä½¿ç”¨è‡ªå®šç¾©çš„ macro_accuracy_score\n",
    "        scorer = make_scorer(macro_accuracy_score, zero_division=0)\n",
    "    elif scoring == 'accuracy':\n",
    "        scorer = make_scorer(accuracy_score)\n",
    "    elif scoring == 'neg_log_loss':\n",
    "        from sklearn.metrics import log_loss\n",
    "        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "    else:\n",
    "        # é è¨­ä½¿ç”¨ f1_macro\n",
    "        scorer = make_scorer(f1_score, average='macro', zero_division=0)\n",
    "    \n",
    "    if rndSearchIter == 0:\n",
    "        gs = GridSearchCV(\n",
    "            estimator=pipe_clf,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scorer,  # â­ ä½¿ç”¨è‡ªå®šç¾© scorer\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "    else:\n",
    "        gs = RandomizedSearchCV(\n",
    "            estimator=pipe_clf,\n",
    "            param_distributions=param_grid,\n",
    "            scoring=scorer,  # â­ ä½¿ç”¨è‡ªå®šç¾© scorer\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs,\n",
    "            n_iter=rndSearchIter\n",
    "        )\n",
    "    \n",
    "    # â­ ä½¿ç”¨æ¸…ç†å¾Œçš„ fit_paramsï¼ˆä¸åŒ…å« scoringï¼‰\n",
    "    gs = gs.fit(feat, lbl, **fit_params_clean).best_estimator_\n",
    "    \n",
    "    # 2) åœ¨å…¨éƒ¨æ•¸æ“šä¸Šæ“¬åˆé©—è­‰å¾Œçš„æ¨¡å‹\n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(\n",
    "            base_estimator=MyPipeline(gs.steps),\n",
    "            n_estimators=int(bagging[0]),\n",
    "            max_samples=float(bagging[1]),\n",
    "            max_features=float(bagging[2]),\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        gs = gs.fit(feat, lbl, **fit_params_clean)\n",
    "    \n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3b637",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ10 ï¼š åŸ·è¡Œé æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a336f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ10ï¼šPredictionPipeline - æ•´åˆæ­¥é©Ÿ7ã€8ã€9\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any, Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PredictionPipeline:\n",
    "    \"\"\"\n",
    "    é æ¸¬ Pipelineï¼šæ•´åˆæ¨¡å‹è¨“ç·´ã€äº¤å‰é©—è­‰ã€è¶…åƒæ•¸èª¿æ•´\n",
    "    \n",
    "    åŠŸèƒ½:\n",
    "    -----\n",
    "    1. æ¨¡å‹è¨“ç·´ï¼ˆæ­¥é©Ÿ7ï¼‰\n",
    "    2. PurgedKFold äº¤å‰é©—è­‰ï¼ˆæ­¥é©Ÿ8ï¼‰\n",
    "    3. è¶…åƒæ•¸èª¿æ•´ï¼ˆæ­¥é©Ÿ9ï¼‰\n",
    "    4. æ¨¡å‹é æ¸¬\n",
    "    5. çµæœè©•ä¼°å’Œå¯è¦–åŒ–\n",
    "    \n",
    "    ä½¿ç”¨æ–¹å¼:\n",
    "    --------\n",
    "    pipeline = PredictionPipeline(\n",
    "        model_type='xgboost',  # æˆ– 'rf'\n",
    "        use_hyperopt=True,     # æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´\n",
    "        use_cv=True            # æ˜¯å¦ä½¿ç”¨äº¤å‰é©—è­‰\n",
    "    )\n",
    "    \n",
    "    # å‚³å…¥æº–å‚™å¥½çš„æ•¸æ“š\n",
    "    pipeline.fit(\n",
    "        X=X,                    # ç‰¹å¾µçŸ©é™£\n",
    "        y=y,                    # æ¨™ç±¤\n",
    "        t1=t1,                  # æ¨™ç±¤çµæŸæ™‚é–“\n",
    "        sample_weight=weights   # æ¨£æœ¬æ¬Šé‡ï¼ˆå¯é¸ï¼‰\n",
    "    )\n",
    "    \n",
    "    # é æ¸¬\n",
    "    predictions, probabilities = pipeline.predict(X_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_type: str = 'xgboost',\n",
    "                 cv_scoring: str = 'f1_macro',  # â­ æ–°å¢é€™å€‹åƒæ•¸\n",
    "                 use_hyperopt: bool = True,\n",
    "                 use_cv: bool = True,\n",
    "                 cv_splits: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 hyperopt_method: str = 'randomized',  # 'randomized' æˆ– 'grid'\n",
    "                 n_iter: int = 30,\n",
    "                 bagging: Optional[List[Any]] = None,\n",
    "                 n_jobs: int = -1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–é æ¸¬ Pipeline\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            æ¨¡å‹é¡å‹ï¼š'xgboost' æˆ– 'rf'\n",
    "        use_hyperopt : bool\n",
    "            æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_cv : bool\n",
    "            æ˜¯å¦ä½¿ç”¨äº¤å‰é©—è­‰ï¼ˆé è¨­ Trueï¼‰\n",
    "        cv_splits : int\n",
    "            äº¤å‰é©—è­‰æŠ˜æ•¸ï¼ˆé è¨­ 5ï¼‰\n",
    "        pct_embargo : float\n",
    "            ç¦åˆ¶æ¯”ä¾‹ï¼ˆé è¨­ 0.01ï¼‰\n",
    "        hyperopt_method : str\n",
    "            è¶…åƒæ•¸èª¿æ•´æ–¹æ³•ï¼š'randomized' æˆ– 'grid'ï¼ˆé è¨­ 'randomized'ï¼‰\n",
    "        n_iter : int\n",
    "            éš¨æ©Ÿæœå°‹è¿­ä»£æ¬¡æ•¸ï¼ˆé è¨­ 30ï¼‰\n",
    "        bagging : list, optional\n",
    "            Bagging åƒæ•¸ [n_estimators, max_samples, max_features]\n",
    "        n_jobs : int\n",
    "            ä¸¦è¡Œä½œæ¥­æ•¸ï¼ˆé è¨­ -1ï¼‰\n",
    "        \"\"\"\n",
    "        self.model_type = model_type.lower()\n",
    "        self.cv_scoring = cv_scoring  # â­ æ–°å¢é€™è¡Œ\n",
    "        self.use_hyperopt = use_hyperopt\n",
    "        self.use_cv = use_cv\n",
    "        self.cv_splits = cv_splits\n",
    "        self.pct_embargo = pct_embargo\n",
    "        self.hyperopt_method = hyperopt_method\n",
    "        self.n_iter = n_iter\n",
    "        self.bagging = bagging if bagging is not None else [0, None, 1.]\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        # å…§éƒ¨è®Šé‡\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.best_params = None\n",
    "        self.cv_scores = None\n",
    "        self.best_cv_scores = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.t1_train = None\n",
    "        self.sample_weight_train = None\n",
    "        self.feature_columns = None\n",
    "        \n",
    "    # åœ¨ PredictionPipeline é¡ä¸­ï¼Œä¿®æ”¹ _create_pipeline æ–¹æ³•\n",
    "    def _create_pipeline(self, n_classes: Optional[int] = None, **model_kwargs) -> MyPipeline:\n",
    "        \"\"\"\n",
    "        å»ºç«‹æ¨¡å‹ Pipelineï¼Œæœƒä¾ç…§å¯¦éš›é¡åˆ¥æ•¸èª¿æ•´æ¨¡å‹ç›®æ¨™å‡½å¼ã€‚\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            from xgboost import XGBClassifier\n",
    "            if n_classes is None or n_classes == 2:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'eval_metric': 'logloss',\n",
    "                    'use_label_encoder': False,\n",
    "                    'objective': 'binary:logistic',\n",
    "                }\n",
    "            else:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'eval_metric': 'mlogloss',\n",
    "                    'use_label_encoder': False,\n",
    "                    'objective': 'multi:softprob',\n",
    "                    'num_class': n_classes,\n",
    "                }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = XGBClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type in ('lightgbm', 'lgb'):\n",
    "            from lightgbm import LGBMClassifier\n",
    "            if n_classes is None or n_classes == 2:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': -1,\n",
    "                    'force_col_wise': True,\n",
    "                    'objective': 'binary',\n",
    "                }\n",
    "            else:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': -1,\n",
    "                    'force_col_wise': True,\n",
    "                    'objective': 'multiclass',\n",
    "                    'num_class': n_classes,\n",
    "                }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = LGBMClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'catboost':\n",
    "            from catboost import CatBoostClassifier\n",
    "            if n_classes is None or n_classes == 2:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbose': False,\n",
    "                    'thread_count': -1,\n",
    "                    'loss_function': 'Logloss',\n",
    "                }\n",
    "            else:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbose': False,\n",
    "                    'thread_count': -1,\n",
    "                    'loss_function': 'MultiClass',\n",
    "                }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = CatBoostClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'rf':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            default_params = {'random_state': 42, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = RandomForestClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'et':\n",
    "            from sklearn.ensemble import ExtraTreesClassifier\n",
    "            default_params = {'random_state': 42, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = ExtraTreesClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'gb':\n",
    "            from sklearn.ensemble import GradientBoostingClassifier\n",
    "            default_params = {'random_state': 42}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = GradientBoostingClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'histgbm':\n",
    "            from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "            default_params = {'random_state': 42}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = HistGradientBoostingClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'dt':\n",
    "            from sklearn.tree import DecisionTreeClassifier\n",
    "            default_params = {'random_state': 42, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = DecisionTreeClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'lr':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            default_params = {'random_state': 42, 'max_iter': 1000, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = LogisticRegression(**default_params)\n",
    "\n",
    "        elif self.model_type == 'svm':\n",
    "            from sklearn.svm import SVC\n",
    "            default_params = {'random_state': 42, 'probability': True, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = SVC(**default_params)\n",
    "\n",
    "        elif self.model_type == 'mlp':\n",
    "            from sklearn.neural_network import MLPClassifier\n",
    "            default_params = {'random_state': 42, 'max_iter': 1000}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = MLPClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'ada':\n",
    "            from sklearn.ensemble import AdaBoostClassifier\n",
    "            default_params = {'random_state': 42}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = AdaBoostClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'nb':\n",
    "            from sklearn.naive_bayes import GaussianNB\n",
    "            clf = GaussianNB()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}\")\n",
    "\n",
    "        needs_scaling = self.model_type in ['lr', 'svm', 'mlp', 'knn', 'nb']\n",
    "        if needs_scaling:\n",
    "            pipe = MyPipeline([('scaler', StandardScaler()), (self.model_type, clf)])\n",
    "        else:\n",
    "            pipe = MyPipeline([(self.model_type, clf)])\n",
    "\n",
    "        return pipe\n",
    "    \n",
    "    def _get_param_grid(self, use_randomized: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å–å¾—åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        print(f\"ğŸš€ å–å¾— {self.model_type} æ¨¡å‹åƒæ•¸ç¶²æ ¼\")\n",
    "        if self.model_type == 'xgboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lightgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10, -1],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__num_leaves': [15, 31, 50, 100],\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_samples': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__num_leaves': [31, 50],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'catboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__l2_leaf_reg': [1, 3, 5, 7],\n",
    "                    f'{self.model_type}__border_count': [32, 64, 128],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__l2_leaf_reg': [3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500, 1000],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': logUniform(1e-3, 1e2),\n",
    "                    f'{self.model_type}__min_samples_leaf': logUniform(1e-3, 1e1),\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                    f'{self.model_type}__bootstrap': [True, False]\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None]\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'et':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'gb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200],\n",
    "                    f'{self.model_type}__max_depth': [3, 5],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'histgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200, 300],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'dt':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "            }\n",
    "            return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lr':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-3, 1e3),\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.01, 0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'svm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-2, 1e2),\n",
    "                    f'{self.model_type}__gamma': logUniform(1e-4, 1e-1),\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'mlp':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                    f'{self.model_type}__alpha': logUniform(1e-5, 1e-1),\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "                    f'{self.model_type}__alpha': [0.0001, 0.001, 0.01],\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'ada':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_estimators': [50, 100, 200],\n",
    "                f'{self.model_type}__learning_rate': [0.01, 0.1, 1.0],\n",
    "            }\n",
    "            return param_grid   \n",
    "        \n",
    "        elif self.model_type == 'knn':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_neighbors': [3, 5, 7, 10, 15],\n",
    "                f'{self.model_type}__weights': ['uniform', 'distance'],\n",
    "                f'{self.model_type}__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "            }\n",
    "            return param_grid\n",
    "        elif self.model_type == 'nb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': logUniform(1e-9, 1e-3),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6],\n",
    "                }\n",
    "                return param_grid\n",
    "        elif self.model_type == 'qda':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': logUniform(1e-3, 1.0),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': [0.0, 0.1, 0.5, 1.0],\n",
    "                }   \n",
    "                return param_grid\n",
    "        else:\n",
    "            # å¦‚æœæ¨¡å‹é¡å‹æ²’æœ‰å°æ‡‰çš„åƒæ•¸ç¶²æ ¼ï¼Œè¿”å›ç©ºå­—å…¸\n",
    "            param_grid = {}\n",
    "            print(f\"âš ï¸ è­¦å‘Š: æ¨¡å‹é¡å‹ '{self.model_type}' æ²’æœ‰å®šç¾©åƒæ•¸ç¶²æ ¼ï¼Œä½¿ç”¨ç©ºå­—å…¸\")\n",
    "        \n",
    "        return param_grid\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        t1: Optional[pd.Series],\n",
    "        sample_weight: Optional[pd.Series] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> \"PredictionPipeline\":\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¨¡å‹ï¼ˆæ•´åˆ PurgedKFoldã€è¶…åƒæ•¸æœå°‹ï¼‰\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ ({self.model_type.upper()})\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # æ­¥é©Ÿ0ï¼šæ¨™ç±¤é‡æ–°ç·¨ç¢¼\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\")\n",
    "        print(\"=\" * 60)\n",
    "        original_unique = sorted(y.unique())\n",
    "        print(f\"åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: {original_unique}\")\n",
    "        print(f\"åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {y.value_counts().to_dict()}\")\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = pd.Series(label_encoder.fit_transform(y), index=y.index)\n",
    "        self.label_encoder_ = label_encoder\n",
    "\n",
    "        print(f\"ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: {sorted(y_encoded.unique())}\")\n",
    "        print(f\"ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {y_encoded.value_counts().to_dict()}\")\n",
    "        print(f\"ç·¨ç¢¼æ˜ å°„: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "        n_classes = len(label_encoder.classes_)\n",
    "        print(f\"å¯¦éš›é¡åˆ¥æ•¸: {n_classes}\")\n",
    "\n",
    "        y = y_encoded\n",
    "\n",
    "        # æ­¥é©Ÿ1ï¼šç´¢å¼•å°é½Š\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        common_idx = X.index.intersection(y.index)\n",
    "        if t1 is not None:\n",
    "            common_idx = common_idx.intersection(t1.index)\n",
    "        if sample_weight is not None:\n",
    "            common_idx = common_idx.intersection(sample_weight.index)\n",
    "\n",
    "        X = X.loc[common_idx]\n",
    "        y = y.loc[common_idx]\n",
    "        if t1 is not None:\n",
    "            t1 = t1.loc[common_idx]\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = sample_weight.loc[common_idx]\n",
    "\n",
    "        print(f\"å°é½Šå¾Œæ¨£æœ¬æ•¸: {len(X):,}\")\n",
    "        print(f\"å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {y.value_counts().to_dict()}\")\n",
    "\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.t1_train = t1\n",
    "        self.sample_weight_train = sample_weight\n",
    "        self.feature_columns = X.columns.tolist()\n",
    "\n",
    "        # æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\")\n",
    "        print(\"=\" * 60)\n",
    "        baseline_pipe = self._create_pipeline(n_classes=n_classes, **model_kwargs)\n",
    "\n",
    "        # æ­¥é©Ÿ3ï¼šPurgedKFold äº¤å‰é©—è­‰\n",
    "        if self.use_cv:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            cv = PurgedKFold(\n",
    "                n_splits=self.cv_splits,\n",
    "                t1=self.t1_train,\n",
    "                pctEmbargo=self.pct_embargo,\n",
    "            )\n",
    "            scoring = self.cv_scoring\n",
    "            print(\"äº¤å‰é©—è­‰é…ç½®:\")\n",
    "            print(f\"  K æŠ˜æ•¸: {self.cv_splits}\")\n",
    "            print(f\"  ç¦åˆ¶æ¯”ä¾‹: {self.pct_embargo:.2%}\")\n",
    "            print(f\"  è©•åˆ†æ–¹æ³•: {scoring}\")\n",
    "\n",
    "            self.cv_scores = cvScore(\n",
    "                clf=baseline_pipe,\n",
    "                X=self.X_train,\n",
    "                y=self.y_train,\n",
    "                sample_weight=self.sample_weight_train,\n",
    "                scoring=scoring,\n",
    "                cvGen=cv,\n",
    "            )\n",
    "            print(\"\\nâœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\")\n",
    "            print(f\"   åˆ†æ•¸: {self.cv_scores}\")\n",
    "\n",
    "        # æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
    "        if self.use_hyperopt:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            param_grid = self._get_param_grid(\n",
    "                use_randomized=(self.hyperopt_method == 'randomized')\n",
    "            )\n",
    "            print(f\"è¶…åƒæ•¸æœå°‹æ–¹å¼: {self.hyperopt_method}\")\n",
    "            print(f\"åƒæ•¸ç©ºé–“å¤§å°: {len(param_grid)}\")\n",
    "\n",
    "            pipe_for_search = self._create_pipeline(n_classes=n_classes, **model_kwargs)\n",
    "            self.model = clfHyperFit(\n",
    "                feat=self.X_train,\n",
    "                lbl=self.y_train,\n",
    "                t1=self.t1_train,\n",
    "                pipe_clf=pipe_for_search,\n",
    "                param_grid=param_grid,\n",
    "                cv=self.cv_splits,\n",
    "                bagging=self.bagging,\n",
    "                rndSearchIter=self.n_iter if self.hyperopt_method == 'randomized' else 0,\n",
    "                n_jobs=self.n_jobs,\n",
    "                pctEmbargo=self.pct_embargo,\n",
    "                scoring=self.cv_scoring,\n",
    "                sample_weight=(\n",
    "                    self.sample_weight_train.values\n",
    "                    if self.sample_weight_train is not None\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "            print(\"\\nâœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ4ï¼šä½¿ç”¨é è¨­è¶…åƒæ•¸è¨“ç·´\")\n",
    "            print(\"=\" * 60)\n",
    "            self.model = baseline_pipe\n",
    "            self.model.fit(\n",
    "                self.X_train,\n",
    "                self.y_train,\n",
    "                **(\n",
    "                    {}\n",
    "                    if self.sample_weight_train is None\n",
    "                    else {f\"{self.model_type}__sample_weight\": self.sample_weight_train.values}\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # æ­¥é©Ÿ5ï¼šå»ºç«‹æ¨™æº–åŒ–å™¨ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰\n",
    "        needs_scaling = self.model_type in ['lr', 'svm', 'mlp', 'knn', 'nb']\n",
    "        if needs_scaling:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.X_train)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "\n",
    "        print(\"\\nâœ… æ¨¡å‹è¨“ç·´å®Œæˆ\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        é æ¸¬\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : np.ndarray\n",
    "            é æ¸¬æ¨™ç±¤ï¼ˆç·¨ç¢¼å¾Œçš„ï¼‰\n",
    "        probabilities : np.ndarray\n",
    "            é æ¸¬æ¦‚ç‡\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆåŸ·è¡Œ fit() è¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        # é æ¸¬\n",
    "        predictions_encoded = self.model.predict(X)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        # å¦‚æœéœ€è¦ï¼Œå¯ä»¥å°‡ç·¨ç¢¼å¾Œçš„æ¨™ç±¤è½‰å›åŸå§‹æ¨™ç±¤\n",
    "        # ä½†é€šå¸¸æˆ‘å€‘ä¿æŒç·¨ç¢¼å¾Œçš„æ¨™ç±¤ï¼Œè®“èª¿ç”¨è€…è‡ªå·±è™•ç†æ˜ å°„\n",
    "        \n",
    "        return predictions_encoded, probabilities\n",
    "    \n",
    "    def evaluate(self,\n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight: Optional[pd.Series] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        è©•ä¼°æ¨¡å‹æ€§èƒ½\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "        y : pd.Series\n",
    "            çœŸå¯¦æ¨™ç±¤\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            è©•ä¼°æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import (accuracy_score, f1_score, \n",
    "                                   precision_score, recall_score,\n",
    "                                   roc_auc_score)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆåŸ·è¡Œ fit() è¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # é æ¸¬\n",
    "        predictions, probabilities = self.predict(X)\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ¨™\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, predictions, sample_weight=sample_weight),\n",
    "            'precision': precision_score(y, predictions, average='weighted', \n",
    "                                        sample_weight=sample_weight, zero_division=0),\n",
    "            'recall': recall_score(y, predictions, average='weighted', \n",
    "                                  sample_weight=sample_weight, zero_division=0),\n",
    "            'f1': f1_score(y, predictions, average='weighted', \n",
    "                          sample_weight=sample_weight, zero_division=0),\n",
    "        }\n",
    "        \n",
    "        # å¦‚æœæ˜¯äºŒåˆ†é¡ï¼Œè¨ˆç®—æ›´å¤šæŒ‡æ¨™\n",
    "        if len(np.unique(y)) == 2:\n",
    "            metrics['f1_binary'] = f1_score(y, predictions, sample_weight=sample_weight, zero_division=0)\n",
    "            metrics['roc_auc'] = roc_auc_score(y, probabilities[:, 1], sample_weight=sample_weight)\n",
    "        \n",
    "        # æ‰“å°çµæœ\n",
    "        print(f\"\\nè©•ä¼°æŒ‡æ¨™:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # æ··æ·†çŸ©é™£\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y, predictions, sample_weight=sample_weight)\n",
    "        print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self,\n",
    "                    X: pd.DataFrame = None,\n",
    "                    y: pd.Series = None,\n",
    "                    sample_weight: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½çµæœï¼ˆäº¤å‰é©—è­‰åˆ†æ•¸ã€é æ¸¬çµæœç­‰ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame, optional\n",
    "            ç‰¹å¾µçŸ©é™£ï¼ˆç”¨æ–¼é æ¸¬å’Œè©•ä¼°ï¼‰\n",
    "        y : pd.Series, optional\n",
    "            çœŸå¯¦æ¨™ç±¤ï¼ˆç”¨æ–¼è©•ä¼°ï¼‰\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. äº¤å‰é©—è­‰åˆ†æ•¸å°æ¯”\n",
    "        if self.cv_scores is not None and self.best_cv_scores is not None:\n",
    "            axes[0, 0].bar(range(len(self.cv_scores)), self.cv_scores, alpha=0.6, \n",
    "                          label='Baseline Model', color='blue', width=0.4)\n",
    "            axes[0, 0].bar([i + 0.4 for i in range(len(self.best_cv_scores))], \n",
    "                          self.best_cv_scores, alpha=0.6, \n",
    "                          label='Best Model (After Tuning)', color='red', width=0.4)\n",
    "            axes[0, 0].axhline(self.cv_scores.mean(), color='blue', linestyle='--', \n",
    "                             label=f'Baseline Mean: {self.cv_scores.mean():.4f}', linewidth=1.5)\n",
    "            axes[0, 0].axhline(self.best_cv_scores.mean(), color='red', linestyle='--', \n",
    "                             label=f'Best Mean: {self.best_cv_scores.mean():.4f}', linewidth=1.5)\n",
    "            axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "            axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores: Baseline vs Best Model', fontsize=14)\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_xticks(range(len(self.cv_scores)))\n",
    "            axes[0, 0].set_xticklabels([f'Fold {i+1}' for i in range(len(self.cv_scores))])\n",
    "        elif self.cv_scores is not None:\n",
    "            axes[0, 0].bar(range(len(self.cv_scores)), self.cv_scores, alpha=0.7, color='blue')\n",
    "            axes[0, 0].axhline(self.cv_scores.mean(), color='red', linestyle='--', \n",
    "                             label=f'Mean: {self.cv_scores.mean():.4f}', linewidth=2)\n",
    "            axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "            axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores', fontsize=14)\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, 'No CV scores available', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores', fontsize=14)\n",
    "        \n",
    "        # 2. åˆ†æ•¸åˆ†å¸ƒå°æ¯”\n",
    "        if self.cv_scores is not None and self.best_cv_scores is not None:\n",
    "            axes[0, 1].boxplot([self.cv_scores, self.best_cv_scores], \n",
    "                             labels=['Baseline', 'Best Model'])\n",
    "            axes[0, 1].scatter([1] * len(self.cv_scores), self.cv_scores, \n",
    "                             alpha=0.3, s=30, color='blue')\n",
    "            axes[0, 1].scatter([2] * len(self.best_cv_scores), self.best_cv_scores, \n",
    "                             alpha=0.3, s=30, color='red')\n",
    "            axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution Comparison', fontsize=14)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        elif self.cv_scores is not None:\n",
    "            axes[0, 1].boxplot([self.cv_scores], labels=['Baseline'])\n",
    "            axes[0, 1].scatter([1] * len(self.cv_scores), self.cv_scores, \n",
    "                             alpha=0.3, s=30, color='blue')\n",
    "            axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution', fontsize=14)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No CV scores available', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution', fontsize=14)\n",
    "        \n",
    "        # 3. é æ¸¬çµæœï¼ˆå¦‚æœæœ‰ X å’Œ yï¼‰\n",
    "        if X is not None and y is not None:\n",
    "            predictions, probabilities = self.predict(X)\n",
    "            \n",
    "            # æ··æ·†çŸ©é™£\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(y, predictions, sample_weight=sample_weight)\n",
    "            sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 0])  # ä½¿ç”¨æµ®é»æ•¸æ ¼å¼\n",
    "            axes[1, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "            axes[1, 0].set_ylabel('True Label', fontsize=12)\n",
    "            axes[1, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "            \n",
    "            # é æ¸¬åˆ†å¸ƒ\n",
    "            pred_counts = pd.Series(np.ravel(predictions)).value_counts().sort_index()\n",
    "            axes[1, 1].bar(pred_counts.index, pred_counts.values, alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('Predicted Label', fontsize=12)\n",
    "            axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "            axes[1, 1].set_title('Prediction Distribution', fontsize=14)\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No prediction data\\n(provide X and y)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "            axes[1, 1].text(0.5, 0.5, 'No prediction data\\n(provide X and y)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 1].set_title('Prediction Distribution', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e91e7",
   "metadata": {},
   "source": [
    "æ•´ç†ç‰¹å¾µç›®æ¨™èˆ‡åˆ‡å‰²æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = final_weights_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================\n",
    "# æ­¥é©Ÿ10ï¼šæ•¸æ“šæº–å‚™ï¼ˆä¿®æ­£å°é½Šå•é¡Œï¼‰\n",
    "# =====================================================\n",
    "\n",
    "# =====================================================\n",
    "# å°‡ç¯©é¸çµæœæ‡‰ç”¨åˆ°å®Œæ•´æ•¸æ“šé›†\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š æ‡‰ç”¨ç¯©é¸çµæœåˆ°å®Œæ•´æ•¸æ“šé›†\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åªä¿ç•™ç¯©é¸å¾Œçš„ç‰¹å¾µ\n",
    "X_filtered = X[filter_pipeline.selected_features].copy()\n",
    "\n",
    "print(f\"ç¯©é¸å‰: {len(X.columns):,} å€‹ç‰¹å¾µ\")\n",
    "print(f\"ç¯©é¸å¾Œ: {len(X_filtered.columns):,} å€‹ç‰¹å¾µ\")\n",
    "print(f\"ç‰¹å¾µæ¸›å°‘: {len(X.columns) - len(X_filtered.columns):,} å€‹ ({(1 - len(X_filtered.columns) / len(X.columns)) * 100:.2f}%)\")\n",
    "\n",
    "# æ›´æ–° X ç‚ºç¯©é¸å¾Œçš„ç‰¹å¾µ\n",
    "X = X_filtered\n",
    "\n",
    "# =====================================================\n",
    "# æ•¸æ“šåˆ†å‰²ï¼šè¨“ç·´é›†å’Œæ¸¬è©¦é›†\n",
    "# =====================================================\n",
    "\n",
    "# æ–¹æ³• 1ï¼šå¦‚æœ sample_weight æ˜¯ SampleWeight å¯¦ä¾‹\n",
    "if hasattr(sample_weight, 'sample_weights'):\n",
    "    # sample_weight æ˜¯ SampleWeight å¯¦ä¾‹\n",
    "    sample_weight_series = sample_weight.sample_weights\n",
    "else:\n",
    "    # sample_weight å·²ç¶“æ˜¯ pd.Series\n",
    "    sample_weight_series = sample_weight\n",
    "\n",
    "# åˆ†å‰²æ•¸æ“š\n",
    "X_train = X.loc[(X.index >= train_start) & (X.index < test_start)].copy()\n",
    "X_test = X.loc[X.index >= test_start].copy()\n",
    "y_train = y_mapped.loc[(y_mapped.index >= train_start) & (y_mapped.index < test_start)].copy()\n",
    "y_test = y_mapped.loc[y_mapped.index >= test_start].copy()\n",
    "t1_train = t1.loc[(t1.index >= train_start) & (t1.index < test_start)].copy()\n",
    "t1_test = t1.loc[t1.index >= test_start].copy()\n",
    "\n",
    "# åˆ†å‰²æ¨£æœ¬æ¬Šé‡\n",
    "if sample_weight_series is not None:\n",
    "    sample_weight_train = sample_weight_series.loc[\n",
    "        (sample_weight_series.index >= train_start) & \n",
    "        (sample_weight_series.index < test_start)\n",
    "    ].copy()\n",
    "    sample_weight_test = sample_weight_series.loc[\n",
    "        sample_weight_series.index >= test_start\n",
    "    ].copy()\n",
    "else:\n",
    "    sample_weight_train = None\n",
    "    sample_weight_test = None\n",
    "\n",
    "print(f\"è¨“ç·´é›†: {len(X_train):,} å€‹æ¨£æœ¬, {len(X_train.columns):,} å€‹ç‰¹å¾µ\")\n",
    "print(f\"æ¸¬è©¦é›†: {len(X_test):,} å€‹æ¨£æœ¬, {len(X_test.columns):,} å€‹ç‰¹å¾µ\")\n",
    "print(f\"è¨“ç·´é›†æ™‚é–“ç¯„åœ: {X_train.index.min()} è‡³ {X_train.index.max()}\")\n",
    "print(f\"æ¸¬è©¦é›†æ™‚é–“ç¯„åœ: {X_test.index.min()} è‡³ {X_test.index.max()}\")\n",
    "\n",
    "print(f\"\\nâœ… æ­¥é©Ÿ6.5 å®Œæˆï¼\")\n",
    "print(f\"   ç‰¹å¾µå·²åˆä½µä¸¦ç¯©é¸\")\n",
    "print(f\"   è¨“ç·´/æ¸¬è©¦é›†å·²åˆ‡åˆ†\")\n",
    "print(f\"   å¯ä»¥é€²è¡Œä¸‹ä¸€æ­¥æ¨¡å‹è¨“ç·´\")\n",
    "\n",
    "# =====================================================\n",
    "# æœ€çµ‚é©—è­‰\n",
    "# =====================================================\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… æ•¸æ“šæº–å‚™å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"è¨“ç·´é›†:\")\n",
    "print(f\"  ç‰¹å¾µæ•¸: {len(X_train.columns):,}\")\n",
    "print(f\"  æ¨£æœ¬æ•¸: {len(X_train):,}\")\n",
    "print(f\"  æ¨™ç±¤åˆ†å¸ƒ: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"\\næ¸¬è©¦é›†:\")\n",
    "print(f\"  ç‰¹å¾µæ•¸: {len(X_test.columns):,}\")\n",
    "print(f\"  æ¨£æœ¬æ•¸: {len(X_test):,}\")\n",
    "print(f\"  æ¨™ç±¤åˆ†å¸ƒ: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# é¡¯ç¤ºç‰¹å¾µä¾†æºçµ±è¨ˆ\n",
    "print(f\"\\nğŸ“Š ç‰¹å¾µä¾†æºçµ±è¨ˆ:\")\n",
    "feature_source_count = {}\n",
    "for name, df_feat in available_features.items():\n",
    "    numeric_cols = df_feat.select_dtypes(include=[np.number]).columns\n",
    "    final_cols = [col for col in numeric_cols if col in X.columns]\n",
    "    if len(final_cols) > 0:\n",
    "        feature_source_count[name] = len(final_cols)\n",
    "        print(f\"  {name}: {len(final_cols)} å€‹ç‰¹å¾µ\")\n",
    "\n",
    "if len(X_train.columns) == 0:\n",
    "    raise ValueError(\"âŒ éŒ¯èª¤ï¼šè¨“ç·´é›†ç‰¹å¾µæ•¸é‡ç‚º 0ï¼\")\n",
    "\n",
    "if len(X_train) == 0:\n",
    "    raise ValueError(\"âŒ éŒ¯èª¤ï¼šè¨“ç·´é›†æ¨£æœ¬æ•¸é‡ç‚º 0ï¼\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e13fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c7c1e",
   "metadata": {},
   "source": [
    "åŸ·è¡Œæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ11ï¼šæ¨¡å‹è¨“ç·´èˆ‡é æ¸¬\n",
    "# =====================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, \n",
    "                             recall_score, confusion_matrix, roc_auc_score)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ æ­¥é©Ÿ11ï¼šæ¨¡å‹è¨“ç·´èˆ‡é æ¸¬\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬1æ­¥ï¼šåˆå§‹åŒ– Pipeline\n",
    "# =====================================================\n",
    "print(f\"\\n1ï¸âƒ£ åˆå§‹åŒ– PredictionPipeline...\")\n",
    "\n",
    "    # æ”¯æ´çš„æ¨¡å‹é¡å‹:\n",
    "    # -----\n",
    "    # - 'xgboost': XGBoost\n",
    "    # - 'lightgbm': LightGBM\n",
    "    # - 'catboost': CatBoost\n",
    "    # - 'rf': Random Forest\n",
    "    # - 'et': Extra Trees\n",
    "    # - 'gb': Gradient Boosting\n",
    "    # - 'histgbm': HistGradientBoosting\n",
    "    # - 'dt': Decision Tree\n",
    "    # - 'lr': Logistic Regression\n",
    "    # - 'svm': Support Vector Machine\n",
    "    # - 'mlp': Multi-Layer Perceptron\n",
    "    # - 'ada': AdaBoost\n",
    "    # - 'nb': Naive Bayes\n",
    "\n",
    "\n",
    "pipeline = PredictionPipeline(\n",
    "    model_type='lightgbm',  \n",
    "    cv_scoring='f1_weighted',  # ['neg_log_loss', 'accuracy', 'f1', 'f1_weighted', 'precision', 'recall', 'roc_auc'].\n",
    "    use_hyperopt=True,\n",
    "    use_cv=True,\n",
    "    cv_splits=5,\n",
    "    pct_embargo=0.01,\n",
    "    hyperopt_method='grid',  # 'grid' æˆ– 'random'\n",
    "    n_iter=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"âœ… Pipeline åˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"   æ¨¡å‹é¡å‹: {pipeline.model_type}\")\n",
    "print(f\"   ä½¿ç”¨è¶…åƒæ•¸å„ªåŒ–: {pipeline.use_hyperopt}\")\n",
    "print(f\"   ä½¿ç”¨äº¤å‰é©—è­‰: {pipeline.use_cv}\")\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬2æ­¥ï¼šè¨“ç·´æ¨¡å‹\n",
    "# =====================================================\n",
    "print(f\"\\n2ï¸âƒ£ é–‹å§‹è¨“ç·´æ¨¡å‹...\")\n",
    "print(f\"   è¨“ç·´é›†ç‰¹å¾µæ•¸: {len(X_train.columns):,}\")\n",
    "print(f\"   è¨“ç·´é›†æ¨£æœ¬æ•¸: {len(X_train):,}\")\n",
    "print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "pipeline.fit(X_train, y_train, t1_train, sample_weight_train)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬3æ­¥ï¼šå¯è¦–åŒ–è¨“ç·´çµæœ\n",
    "# =====================================================\n",
    "print(f\"\\n3ï¸âƒ£ å¯è¦–åŒ–è¨“ç·´çµæœ...\")\n",
    "\n",
    "pipeline.plot_results(X=X_test, y=y_test, sample_weight=sample_weight_test)\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬4æ­¥ï¼šé æ¸¬\n",
    "# =====================================================\n",
    "print(f\"\\n4ï¸âƒ£ é–‹å§‹é æ¸¬...\")\n",
    "print(f\"   æ¸¬è©¦é›†ç‰¹å¾µæ•¸: {len(X_test.columns):,}\")\n",
    "print(f\"   æ¸¬è©¦é›†æ¨£æœ¬æ•¸: {len(X_test):,}\")\n",
    "\n",
    "predictions_test_mapped, probabilities_test = pipeline.predict(X_test)\n",
    "\n",
    "# å°‡é æ¸¬çµæœæ˜ å°„å›åŸå§‹æ¨™ç±¤ (-1, 0, 1)\n",
    "predictions_test = pd.Series(np.ravel(predictions_test_mapped)).map(reverse_mapping).values\n",
    "\n",
    "print(f\"âœ… é æ¸¬å®Œæˆ\")\n",
    "print(f\"   é æ¸¬æ¨£æœ¬æ•¸: {len(predictions_test):,}\")\n",
    "\n",
    "# æª¢æŸ¥å¯¦éš›çš„é¡åˆ¥æ•¸é‡\n",
    "n_classes = probabilities_test.shape[1]\n",
    "print(f\"   å¯¦éš›é¡åˆ¥æ•¸: {n_classes}\")\n",
    "print(f\"   æ¦‚ç‡çŸ©é™£å½¢ç‹€: {probabilities_test.shape}\")\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬5æ­¥ï¼šå¯è¦–åŒ–æ¦‚ç‡åˆ†å¸ƒ\n",
    "# =====================================================\n",
    "print(f\"\\n5ï¸âƒ£ å¯è¦–åŒ–æ¦‚ç‡åˆ†å¸ƒ...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# æ ¹æ“šå¯¦éš›é¡åˆ¥æ•¸é‡å‹•æ…‹è¨­ç½®é¡è‰²å’Œæ¨™ç±¤\n",
    "if n_classes == 2:\n",
    "    colors = ['red', 'blue']\n",
    "    class_labels = ['Class 0 (è·Œ)', 'Class 1 (ä¸è·Œ)']\n",
    "    class_labels_short = ['Class 0\\n(è·Œ)', 'Class 1\\n(ä¸è·Œ)']\n",
    "elif n_classes == 3:\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    class_labels = ['Class 0 (åŸ-1)', 'Class 1 (åŸ0)', 'Class 2 (åŸ1)']\n",
    "    class_labels_short = ['Class 0\\n(åŸ-1)', 'Class 1\\n(åŸ0)', 'Class 2\\n(åŸ1)']\n",
    "else:\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple'][:n_classes]\n",
    "    class_labels = [f'Class {i}' for i in range(n_classes)]\n",
    "    class_labels_short = [f'Class {i}' for i in range(n_classes)]\n",
    "\n",
    "# æ¦‚ç‡åˆ†å¸ƒç›´æ–¹åœ–\n",
    "for i in range(n_classes):\n",
    "    axes[0, 0].hist(probabilities_test[:, i], bins=50, alpha=0.7, \n",
    "                    label=class_labels[i], color=colors[i])\n",
    "axes[0, 0].set_xlabel('Probability', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Probability Distribution by Class', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# å¹³å‡æ¦‚ç‡\n",
    "mean_probs = probabilities_test.mean(axis=0)\n",
    "axes[0, 1].bar(range(n_classes), mean_probs, alpha=0.7, color=colors)\n",
    "axes[0, 1].set_xticks(range(n_classes))\n",
    "axes[0, 1].set_xticklabels(class_labels_short)\n",
    "axes[0, 1].set_ylabel('Mean Probability', fontsize=12)\n",
    "axes[0, 1].set_title('Mean Probability by Class', fontsize=14)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# é æ¸¬é¡åˆ¥åˆ†å¸ƒ\n",
    "pred_counts = pd.Series(predictions_test_mapped).value_counts().sort_index()\n",
    "axes[1, 0].bar(pred_counts.index, pred_counts.values, alpha=0.7, \n",
    "               color=[colors[i] if i < len(colors) else 'gray' for i in pred_counts.index])\n",
    "axes[1, 0].set_xlabel('Predicted Class', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Count', fontsize=12)\n",
    "axes[1, 0].set_title('Predicted Class Distribution', fontsize=14)\n",
    "axes[1, 0].set_xticks(range(n_classes))\n",
    "axes[1, 0].set_xticklabels(class_labels_short)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# çœŸå¯¦é¡åˆ¥åˆ†å¸ƒ\n",
    "true_counts = y_test.value_counts().sort_index()\n",
    "axes[1, 1].bar(true_counts.index, true_counts.values, alpha=0.7, \n",
    "               color=[colors[i] if i < len(colors) else 'gray' for i in true_counts.index])\n",
    "axes[1, 1].set_xlabel('True Class', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[1, 1].set_title('True Class Distribution (Test Set)', fontsize=14)\n",
    "axes[1, 1].set_xticks(range(n_classes))\n",
    "axes[1, 1].set_xticklabels(class_labels_short)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ‰“å°æ¦‚ç‡çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š é æ¸¬æ¦‚ç‡çµ±è¨ˆ\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(n_classes):\n",
    "    print(f\"{class_labels[i]} æ¦‚ç‡: å¹³å‡={probabilities_test[:, i].mean():.4f}, ä¸­ä½æ•¸={np.median(probabilities_test[:, i]):.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬6æ­¥ï¼šè©•ä¼°æ¨¡å‹æ€§èƒ½\n",
    "# =====================================================\n",
    "print(f\"\\n6ï¸âƒ£ è©•ä¼°æ¨¡å‹æ€§èƒ½...\")\n",
    "\n",
    "# ç²å–åŸå§‹æ¨™ç±¤ï¼ˆæœªæ˜ å°„çš„ï¼‰\n",
    "y_test_original = y.loc[y_test.index]\n",
    "\n",
    "# è¨ˆç®—è©•ä¼°æŒ‡æ¨™\n",
    "metrics_test = {\n",
    "    'accuracy': accuracy_score(y_test_original, predictions_test, sample_weight=sample_weight_test),\n",
    "    'precision': precision_score(y_test_original, predictions_test, average='weighted', \n",
    "                                 sample_weight=sample_weight_test, zero_division=0),\n",
    "    'recall': recall_score(y_test_original, predictions_test, average='weighted', \n",
    "                          sample_weight=sample_weight_test, zero_division=0),\n",
    "    'f1': f1_score(y_test_original, predictions_test, average='weighted', \n",
    "                  sample_weight=sample_weight_test, zero_division=0),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š æ¸¬è©¦é›†è©•ä¼°çµæœ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"è©•ä¼°æ¨£æœ¬æ•¸: {len(y_test_original):,}\")\n",
    "for key, value in metrics_test.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# æ··æ·†çŸ©é™£\n",
    "cm = confusion_matrix(y_test_original, predictions_test, sample_weight=sample_weight_test)\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm)\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™\n",
    "print(f\"\\nå„é¡åˆ¥è©³ç´°æŒ‡æ¨™:\")\n",
    "unique_labels = sorted(y_test_original.unique())\n",
    "for i, label in enumerate(unique_labels):\n",
    "    if i < len(cm):\n",
    "        true_pos = cm[i, i] if i < cm.shape[0] and i < cm.shape[1] else 0\n",
    "        false_pos = cm[:, i].sum() - true_pos if i < cm.shape[1] else 0\n",
    "        false_neg = cm[i, :].sum() - true_pos if i < cm.shape[0] else 0\n",
    "        \n",
    "        precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "        recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"  é¡åˆ¥ {label}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =====================================================\n",
    "# ç¬¬7æ­¥ï¼šå‰µå»ºé æ¸¬çµæœ DataFrame\n",
    "# =====================================================\n",
    "print(f\"\\n7ï¸âƒ£ å‰µå»ºé æ¸¬çµæœ DataFrame...\")\n",
    "\n",
    "pred_df = pd.DataFrame(index=X_test.index)\n",
    "pred_df['t1'] = t1_test\n",
    "pred_df['side'] = events.loc[X_test.index, 'side']\n",
    "pred_df['true_label'] = y_test_original\n",
    "pred_df['predicted_label'] = predictions_test\n",
    "\n",
    "# æ ¹æ“šå¯¦éš›é¡åˆ¥æ•¸é‡å‹•æ…‹æ·»åŠ æ¦‚ç‡åˆ—\n",
    "for i in range(n_classes):\n",
    "    if n_classes == 2:\n",
    "        prob_label = f'prob_class_{i}' + ('_è·Œ' if i == 0 else '_ä¸è·Œ')\n",
    "    else:\n",
    "        prob_label = f'prob_class_{i}'\n",
    "    pred_df[prob_label] = probabilities_test[:, i]\n",
    "\n",
    "pred_df['prediction_confidence'] = np.max(probabilities_test, axis=1)\n",
    "pred_df['is_correct'] = (pred_df['predicted_label'] == pred_df['true_label'])\n",
    "\n",
    "# äº¤æ˜“ä¿¡è™Ÿï¼šæ ¹æ“šé æ¸¬æ¨™ç±¤å’Œ side ç”Ÿæˆ\n",
    "pred_df['trade_signal'] = np.where(\n",
    "    (pred_df['predicted_label'] == 1) & (pred_df['side'] == 1), 1,\n",
    "    np.where((pred_df['predicted_label'] == 1) & (pred_df['side'] == -1), -1, 0)\n",
    ")\n",
    "\n",
    "print(f\"âœ… é æ¸¬çµæœ DataFrame å‰µå»ºå®Œæˆ\")\n",
    "print(f\"   å½¢ç‹€: {pred_df.shape}\")\n",
    "print(f\"   æ­£ç¢ºé æ¸¬ç‡: {pred_df['is_correct'].mean():.4f}\")\n",
    "\n",
    "# é¡¯ç¤ºå‰å¹¾ç­†çµæœ\n",
    "print(f\"\\nå‰ 10 ç­†é æ¸¬çµæœ:\")\n",
    "print(pred_df.head(10))\n",
    "\n",
    "# é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "print(f\"\\né æ¸¬çµæœçµ±è¨ˆ:\")\n",
    "print(f\"  ç¸½æ¨£æœ¬æ•¸: {len(pred_df):,}\")\n",
    "print(f\"  æ­£ç¢ºé æ¸¬æ•¸: {pred_df['is_correct'].sum():,}\")\n",
    "print(f\"  éŒ¯èª¤é æ¸¬æ•¸: {(~pred_df['is_correct']).sum():,}\")\n",
    "print(f\"  å¹³å‡ç½®ä¿¡åº¦: {pred_df['prediction_confidence'].mean():.4f}\")\n",
    "print(f\"  é æ¸¬æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "unique_pred_labels = sorted(pred_df['predicted_label'].unique())\n",
    "for label in unique_pred_labels:\n",
    "    count = (pred_df['predicted_label'] == label).sum()\n",
    "    print(f\"    {label}: {count:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… é æ¸¬æµç¨‹å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# é¡¯ç¤ºæœ€çµ‚çš„é æ¸¬çµæœ DataFrame\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df.to_csv('Pred_lgb_23.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49544186",
   "metadata": {},
   "source": [
    "æ‰€æœ‰æ¨¡å‹è¼ªæµåŸ·è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =====================================================\n",
    "# # è‡ªå‹•éæ­·æ‰€æœ‰æ¨¡å‹ä¸¦é¡¯ç¤ºæ··æ·†çŸ©é™£\n",
    "# # =====================================================\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import (accuracy_score, f1_score, precision_score, \n",
    "#                              recall_score, confusion_matrix)\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"ğŸš€ è‡ªå‹•éæ­·æ‰€æœ‰æ¨¡å‹ - å®Œæ•´è©•ä¼°\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹é¡å‹\n",
    "# model_types = [\n",
    "#     'xgboost',\n",
    "#     'lightgbm',\n",
    "#     'catboost',\n",
    "#     'rf',\n",
    "#     'et',\n",
    "#     'gb',\n",
    "#     'histgbm',\n",
    "#     'dt',\n",
    "#     'lr',\n",
    "#     'svm',\n",
    "#     'mlp',\n",
    "#     'ada',\n",
    "#     'nb'\n",
    "# ]\n",
    "\n",
    "# # =====================================================\n",
    "# # æ­¥é©Ÿ1ï¼šæª¢æŸ¥ä¸¦æ¸…ç†æ¨™ç±¤\n",
    "# # =====================================================\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"ğŸ“Š æ­¥é©Ÿ1ï¼šæª¢æŸ¥ä¸¦æ¸…ç†æ¨™ç±¤\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# # ç²å–åŸå§‹æ¨™ç±¤ï¼ˆæœªæ˜ å°„çš„ï¼‰\n",
    "# y_test_original = y.loc[y_test.index]\n",
    "\n",
    "# # æª¢æŸ¥æ¨™ç±¤åˆ†å¸ƒ\n",
    "# print(f\"\\nåŸå§‹æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "# print(f\"   y_test_original å”¯ä¸€å€¼: {sorted(y_test_original.unique())}\")\n",
    "# print(f\"   y_train å”¯ä¸€å€¼: {sorted(y_train.unique())}\")\n",
    "\n",
    "# # æª¢æŸ¥æ˜¯å¦æœ‰æœªæ˜ å°„çš„å€¼\n",
    "# valid_labels = set(label_mapping.keys())\n",
    "# y_test_unique = set(y_test_original.unique())\n",
    "# y_train_unique = set(y.loc[y_train.index].unique())\n",
    "\n",
    "# unmapped_test = y_test_unique - valid_labels\n",
    "# unmapped_train = y_train_unique - valid_labels\n",
    "\n",
    "# if unmapped_test or unmapped_train:\n",
    "#     print(f\"\\nâš ï¸  ç™¼ç¾æœªæ˜ å°„çš„æ¨™ç±¤å€¼:\")\n",
    "#     if unmapped_test:\n",
    "#         print(f\"   æ¸¬è©¦é›†ä¸­: {sorted(unmapped_test)}\")\n",
    "#     if unmapped_train:\n",
    "#         print(f\"   è¨“ç·´é›†ä¸­: {sorted(unmapped_train)}\")\n",
    "#     print(f\"   æœ‰æ•ˆæ¨™ç±¤: {sorted(valid_labels)}\")\n",
    "#     print(f\"   å°‡éæ¿¾æ‰æœªæ˜ å°„çš„æ¨™ç±¤...\")\n",
    "\n",
    "# # éæ¿¾æ‰æœªæ˜ å°„çš„æ¨™ç±¤ï¼ˆåªä¿ç•™ -1, 0, 1ï¼‰\n",
    "# y_test_original_cleaned = y_test_original[y_test_original.isin(valid_labels)].copy()\n",
    "# y_train_original_cleaned = y.loc[y_train.index][y.loc[y_train.index].isin(valid_labels)].copy()\n",
    "\n",
    "# print(f\"\\néæ¿¾å¾Œ:\")\n",
    "# print(f\"   æ¸¬è©¦é›†æ¨£æœ¬æ•¸: {len(y_test_original_cleaned):,} (åŸå§‹: {len(y_test_original):,})\")\n",
    "# print(f\"   è¨“ç·´é›†æ¨£æœ¬æ•¸: {len(y_train_original_cleaned):,} (åŸå§‹: {len(y_train):,})\")\n",
    "\n",
    "# # =====================================================\n",
    "# # æ­¥é©Ÿ2ï¼šå°é½Šæ•¸æ“šç´¢å¼•\n",
    "# # =====================================================\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"ğŸ“Š æ­¥é©Ÿ2ï¼šå°é½Šæ•¸æ“šç´¢å¼•\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# # ç¢ºä¿è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„ç´¢å¼•å°é½Šï¼ˆåªä½¿ç”¨æ¸…ç†å¾Œçš„æ¨™ç±¤ï¼‰\n",
    "# X_train_cleaned = X_train.loc[y_train_original_cleaned.index]\n",
    "# y_train_mapped_cleaned = y_train.loc[y_train_original_cleaned.index]\n",
    "# t1_train_cleaned = t1_train.loc[y_train_original_cleaned.index] if t1_train is not None else None\n",
    "# sample_weight_train_cleaned = sample_weight_train.loc[y_train_original_cleaned.index] if sample_weight_train is not None else None\n",
    "\n",
    "# X_test_cleaned = X_test.loc[y_test_original_cleaned.index]\n",
    "# y_test_mapped_cleaned = y_test.loc[y_test_original_cleaned.index]\n",
    "# t1_test_cleaned = t1_test.loc[y_test_original_cleaned.index] if t1_test is not None else None\n",
    "# sample_weight_test_cleaned = sample_weight_test.loc[y_test_original_cleaned.index] if sample_weight_test is not None else None\n",
    "\n",
    "# # æª¢æŸ¥æ˜ å°„å¾Œçš„æ¨™ç±¤åˆ†å¸ƒ\n",
    "# print(f\"\\næ˜ å°„å¾Œçš„æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "# print(f\"   y_train_mapped_cleaned å”¯ä¸€å€¼: {sorted(y_train_mapped_cleaned.unique())}\")\n",
    "# print(f\"   y_test_mapped_cleaned å”¯ä¸€å€¼: {sorted(y_test_mapped_cleaned.unique())}\")\n",
    "\n",
    "# # =====================================================\n",
    "# # æ­¥é©Ÿ3ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤ï¼ˆç¢ºä¿å¾0é–‹å§‹ä¸”é€£çºŒï¼‰\n",
    "# # =====================================================\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"ğŸ“Š æ­¥é©Ÿ3ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# # ä½¿ç”¨ LabelEncoder é‡æ–°ç·¨ç¢¼ï¼Œç¢ºä¿æ¨™ç±¤å¾ 0 é–‹å§‹ä¸”é€£çºŒ\n",
    "# label_encoder = LabelEncoder()\n",
    "# # åˆä½µè¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„æ‰€æœ‰å”¯ä¸€æ¨™ç±¤ä¾†æ“¬åˆ encoder\n",
    "# all_unique_labels = sorted(set(list(y_train_mapped_cleaned.unique()) + list(y_test_mapped_cleaned.unique())))\n",
    "# label_encoder.fit(all_unique_labels)\n",
    "\n",
    "# print(f\"   åŸå§‹å”¯ä¸€æ¨™ç±¤: {all_unique_labels}\")\n",
    "# print(f\"   ç·¨ç¢¼å¾Œæ¨™ç±¤: {label_encoder.classes_.tolist()}\")\n",
    "# print(f\"   ç·¨ç¢¼æ˜ å°„: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# # ç·¨ç¢¼è¨“ç·´é›†å’Œæ¸¬è©¦é›†æ¨™ç±¤\n",
    "# y_train_encoded = pd.Series(\n",
    "#     label_encoder.transform(y_train_mapped_cleaned),\n",
    "#     index=y_train_mapped_cleaned.index\n",
    "# )\n",
    "# y_test_encoded = pd.Series(\n",
    "#     label_encoder.transform(y_test_mapped_cleaned),\n",
    "#     index=y_test_mapped_cleaned.index\n",
    "# )\n",
    "\n",
    "# print(f\"   ç·¨ç¢¼å¾Œè¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒ: {y_train_encoded.value_counts().to_dict()}\")\n",
    "# print(f\"   ç·¨ç¢¼å¾Œæ¸¬è©¦é›†æ¨™ç±¤åˆ†å¸ƒ: {y_test_encoded.value_counts().to_dict()}\")\n",
    "\n",
    "# # ç²å–å¯¦éš›çš„é¡åˆ¥æ•¸\n",
    "# n_classes_actual = len(label_encoder.classes_)\n",
    "# print(f\"\\nå¯¦éš›é¡åˆ¥æ•¸: {n_classes_actual}\")\n",
    "\n",
    "# # ç²å–æ‰€æœ‰å”¯ä¸€çš„åŸå§‹æ¨™ç±¤å€¼ï¼ˆç”¨æ–¼å‹•æ…‹è™•ç†ï¼‰\n",
    "# unique_labels_original = sorted(y_test_original_cleaned.unique())\n",
    "# n_classes_original = len(unique_labels_original)\n",
    "\n",
    "# print(f\"åŸå§‹æ¨™ç±¤é¡åˆ¥: {unique_labels_original}\")\n",
    "# print(f\"åŸå§‹é¡åˆ¥æ•¸: {n_classes_original}\")\n",
    "\n",
    "# # =====================================================\n",
    "# # æ­¥é©Ÿ4ï¼šç¢ºä¿æ‰€æœ‰æ•¸æ“šç´¢å¼•å®Œå…¨å°é½Š\n",
    "# # =====================================================\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"ğŸ“Š æ­¥é©Ÿ4ï¼šæœ€çµ‚æ•¸æ“šå°é½Šæª¢æŸ¥\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# # è¨“ç·´é›†ç´¢å¼•å°é½Š\n",
    "# common_idx_train = X_train_cleaned.index.intersection(y_train_encoded.index)\n",
    "# if t1_train_cleaned is not None:\n",
    "#     common_idx_train = common_idx_train.intersection(t1_train_cleaned.index)\n",
    "# if sample_weight_train_cleaned is not None:\n",
    "#     common_idx_train = common_idx_train.intersection(sample_weight_train_cleaned.index)\n",
    "\n",
    "# X_train_final = X_train_cleaned.loc[common_idx_train]\n",
    "# y_train_final = y_train_encoded.loc[common_idx_train]\n",
    "# t1_train_final = t1_train_cleaned.loc[common_idx_train] if t1_train_cleaned is not None else None\n",
    "# sample_weight_train_final = sample_weight_train_cleaned.loc[common_idx_train] if sample_weight_train_cleaned is not None else None\n",
    "\n",
    "# # æ¸¬è©¦é›†ç´¢å¼•å°é½Š\n",
    "# common_idx_test = X_test_cleaned.index.intersection(y_test_encoded.index)\n",
    "# if t1_test_cleaned is not None:\n",
    "#     common_idx_test = common_idx_test.intersection(t1_test_cleaned.index)\n",
    "# if sample_weight_test_cleaned is not None:\n",
    "#     common_idx_test = common_idx_test.intersection(sample_weight_test_cleaned.index)\n",
    "\n",
    "# X_test_final = X_test_cleaned.loc[common_idx_test]\n",
    "# y_test_final = y_test_encoded.loc[common_idx_test]\n",
    "# t1_test_final = t1_test_cleaned.loc[common_idx_test] if t1_test_cleaned is not None else None\n",
    "# sample_weight_test_final = sample_weight_test_cleaned.loc[common_idx_test] if sample_weight_test_cleaned is not None else None\n",
    "\n",
    "# # ç²å–å°æ‡‰çš„åŸå§‹æ¨™ç±¤\n",
    "# y_test_original_final = y_test_original_cleaned.loc[common_idx_test]\n",
    "\n",
    "# print(f\"   å°é½Šå¾Œè¨“ç·´é›†æ¨£æœ¬æ•¸: {len(X_train_final):,}\")\n",
    "# print(f\"   å°é½Šå¾Œæ¸¬è©¦é›†æ¨£æœ¬æ•¸: {len(X_test_final):,}\")\n",
    "# print(f\"   è¨“ç·´é›†ç´¢å¼•å°é½Š: {X_train_final.index.equals(y_train_final.index)}\")\n",
    "# print(f\"   æ¸¬è©¦é›†ç´¢å¼•å°é½Š: {X_test_final.index.equals(y_test_final.index)}\")\n",
    "\n",
    "# # =====================================================\n",
    "# # æ­¥é©Ÿ5ï¼šéæ­·æ‰€æœ‰æ¨¡å‹\n",
    "# # =====================================================\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"ğŸ“Š æ­¥é©Ÿ5ï¼šéæ­·æ‰€æœ‰æ¨¡å‹é€²è¡Œè¨“ç·´å’Œè©•ä¼°\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# # å­˜å„²æ‰€æœ‰æ¨¡å‹çš„çµæœ\n",
    "# results_summary = []\n",
    "\n",
    "# for i, model_type in enumerate(model_types, 1):\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"ğŸ“Š [{i}/{len(model_types)}] æ¨¡å‹: {model_type.upper()}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     try:\n",
    "#         # åˆå§‹åŒ– Pipeline\n",
    "#         pipeline = PredictionPipeline(\n",
    "#             model_type=model_type,\n",
    "#             cv_scoring='f1',  # ä½¿ç”¨ f1 ä½œç‚ºè©•åˆ†æ¨™æº–\n",
    "#             use_hyperopt=True,\n",
    "#             use_cv=True,\n",
    "#             cv_splits=5,\n",
    "#             pct_embargo=0.01,\n",
    "#             hyperopt_method='grid',  # 'grid' æˆ– 'random'\n",
    "#             n_iter=20,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "        \n",
    "#         # è¨“ç·´æ¨¡å‹\n",
    "#         print(f\"\\nè¨“ç·´ä¸­...\")\n",
    "#         print(f\"   è¨“ç·´é›†æ¨£æœ¬æ•¸: {len(X_train_final):,}\")\n",
    "#         print(f\"   è¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒ: {y_train_final.value_counts().to_dict()}\")\n",
    "#         print(f\"   é¡åˆ¥æ•¸: {n_classes_actual}\")\n",
    "        \n",
    "#         pipeline.fit(X_train_final, y_train_final, t1_train_final, sample_weight_train_final)\n",
    "        \n",
    "#         # é æ¸¬\n",
    "#         print(f\"\\né æ¸¬ä¸­...\")\n",
    "#         print(f\"   æ¸¬è©¦é›†æ¨£æœ¬æ•¸: {len(X_test_final):,}\")\n",
    "        \n",
    "#         predictions_test_encoded, probabilities_test = pipeline.predict(X_test_final)\n",
    "        \n",
    "#         # æª¢æŸ¥å¯¦éš›çš„é¡åˆ¥æ•¸é‡\n",
    "#         n_classes_predicted = probabilities_test.shape[1] if probabilities_test is not None else len(np.unique(predictions_test_encoded))\n",
    "#         print(f\"   æ¨¡å‹é æ¸¬é¡åˆ¥æ•¸: {n_classes_predicted}\")\n",
    "        \n",
    "#         # å°‡ç·¨ç¢¼å¾Œçš„é æ¸¬çµæœè½‰å›æ˜ å°„å¾Œçš„æ¨™ç±¤\n",
    "#         predictions_test_mapped = label_encoder.inverse_transform(predictions_test_encoded)\n",
    "        \n",
    "#         # å°‡æ˜ å°„å¾Œçš„æ¨™ç±¤è½‰å›åŸå§‹æ¨™ç±¤ (-1, 0, 1)\n",
    "#         predictions_test = pd.Series(predictions_test_mapped).map(reverse_mapping).values\n",
    "        \n",
    "#         # è¨ˆç®—è©•ä¼°æŒ‡æ¨™\n",
    "#         accuracy = accuracy_score(y_test_original_final, predictions_test, \n",
    "#                                 sample_weight=sample_weight_test_final)\n",
    "#         precision = precision_score(y_test_original_final, predictions_test, average='weighted', \n",
    "#                                    sample_weight=sample_weight_test_final, zero_division=0)\n",
    "#         recall = recall_score(y_test_original_final, predictions_test, average='weighted', \n",
    "#                             sample_weight=sample_weight_test_final, zero_division=0)\n",
    "#         f1 = f1_score(y_test_original_final, predictions_test, average='weighted', \n",
    "#                     sample_weight=sample_weight_test_final, zero_division=0)\n",
    "        \n",
    "#         # è¨ˆç®—æ··æ·†çŸ©é™£\n",
    "#         cm = confusion_matrix(y_test_original_final, predictions_test, \n",
    "#                             labels=unique_labels_original,\n",
    "#                             sample_weight=sample_weight_test_final)\n",
    "        \n",
    "#         # ç²å–å¯¦éš›å‡ºç¾åœ¨é æ¸¬å’ŒçœŸå¯¦æ¨™ç±¤ä¸­çš„æ‰€æœ‰å”¯ä¸€å€¼\n",
    "#         all_unique_labels = sorted(set(list(y_test_original_final.unique()) + list(predictions_test)))\n",
    "#         actual_n_classes = len(all_unique_labels)\n",
    "        \n",
    "#         print(f\"\\næ··æ·†çŸ©é™£å½¢ç‹€: {cm.shape}\")\n",
    "#         print(f\"å¯¦éš›æ··æ·†çŸ©é™£é¡åˆ¥æ•¸: {actual_n_classes}\")\n",
    "        \n",
    "#         # å‹•æ…‹ç”Ÿæˆæ¨™ç±¤åç¨±\n",
    "#         if actual_n_classes == 2:\n",
    "#             if 0 in all_unique_labels and 1 in all_unique_labels:\n",
    "#                 pred_labels = ['Pred 0 (è·Œ)', 'Pred 1 (ä¸è·Œ)']\n",
    "#                 true_labels = ['True 0 (è·Œ)', 'True 1 (ä¸è·Œ)']\n",
    "#                 header_labels = ['é æ¸¬0(è·Œ)', 'é æ¸¬1(ä¸è·Œ)']\n",
    "#             elif -1 in all_unique_labels and 1 in all_unique_labels:\n",
    "#                 pred_labels = ['Pred -1 (è·Œ)', 'Pred 1 (ä¸è·Œ)']\n",
    "#                 true_labels = ['True -1 (è·Œ)', 'True 1 (ä¸è·Œ)']\n",
    "#                 header_labels = ['é æ¸¬-1(è·Œ)', 'é æ¸¬1(ä¸è·Œ)']\n",
    "#             else:\n",
    "#                 pred_labels = [f'Pred {label}' for label in all_unique_labels]\n",
    "#                 true_labels = [f'True {label}' for label in all_unique_labels]\n",
    "#                 header_labels = [f'é æ¸¬{label}' for label in all_unique_labels]\n",
    "#         elif actual_n_classes == 3:\n",
    "#             pred_labels = ['Pred -1', 'Pred 0', 'Pred 1']\n",
    "#             true_labels = ['True -1', 'True 0', 'True 1']\n",
    "#             header_labels = ['é æ¸¬-1', 'é æ¸¬0', 'é æ¸¬1']\n",
    "#         else:\n",
    "#             pred_labels = [f'Pred {label}' for label in all_unique_labels]\n",
    "#             true_labels = [f'True {label}' for label in all_unique_labels]\n",
    "#             header_labels = [f'é æ¸¬{label}' for label in all_unique_labels]\n",
    "        \n",
    "#         # é¡¯ç¤ºæ··æ·†çŸ©é™£ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰\n",
    "#         print(f\"\\næ··æ·†çŸ©é™£ (Confusion Matrix):\")\n",
    "#         header_str = f\"{'':>10}\"\n",
    "#         for label in header_labels:\n",
    "#             header_str += f\" {label:>12}\"\n",
    "#         print(header_str)\n",
    "        \n",
    "#         for i, true_label in enumerate(true_labels):\n",
    "#             if i < cm.shape[0]:\n",
    "#                 row_str = f\"{true_label:>10}\"\n",
    "#                 for j in range(cm.shape[1]):\n",
    "#                     row_str += f\" {cm[i, j]:>12.0f}\"\n",
    "#                 print(row_str)\n",
    "        \n",
    "#         # é¡¯ç¤ºè©•ä¼°æŒ‡æ¨™\n",
    "#         print(f\"\\nè©•ä¼°æŒ‡æ¨™:\")\n",
    "#         print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "#         print(f\"  Precision: {precision:.4f}\")\n",
    "#         print(f\"  Recall: {recall:.4f}\")\n",
    "#         print(f\"  F1-Score: {f1:.4f}\")\n",
    "        \n",
    "#         # å¯è¦–åŒ–æ··æ·†çŸ©é™£\n",
    "#         plt.figure(figsize=(max(8, actual_n_classes * 2), max(6, actual_n_classes * 1.5)))\n",
    "#         sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues', \n",
    "#                     xticklabels=pred_labels,\n",
    "#                     yticklabels=true_labels,\n",
    "#                     cbar_kws={'label': 'Count'})\n",
    "#         plt.title(f'Confusion Matrix - {model_type.upper()}', fontsize=14, fontweight='bold')\n",
    "#         plt.ylabel('True Label', fontsize=12)\n",
    "#         plt.xlabel('Predicted Label', fontsize=12)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         # å­˜å„²çµæœ\n",
    "#         results_summary.append({\n",
    "#             'model': model_type.upper(),\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1': f1,\n",
    "#             'n_classes': actual_n_classes\n",
    "#         })\n",
    "        \n",
    "#         print(f\"âœ… {model_type.upper()} å®Œæˆ\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ {model_type.upper()} å¤±æ•—: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "        \n",
    "#         # å­˜å„²å¤±æ•—çµæœ\n",
    "#         results_summary.append({\n",
    "#             'model': model_type.upper(),\n",
    "#             'accuracy': np.nan,\n",
    "#             'precision': np.nan,\n",
    "#             'recall': np.nan,\n",
    "#             'f1': np.nan,\n",
    "#             'n_classes': np.nan,\n",
    "#             'error': str(e)\n",
    "#         })\n",
    "#         continue\n",
    "\n",
    "# # =====================================================\n",
    "# # æ­¥é©Ÿ6ï¼šé¡¯ç¤ºæ‰€æœ‰æ¨¡å‹çµæœæ‘˜è¦\n",
    "# # =====================================================\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"ğŸ“Š æ­¥é©Ÿ6ï¼šæ‰€æœ‰æ¨¡å‹çµæœæ‘˜è¦\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# results_df = pd.DataFrame(results_summary)\n",
    "# results_df = results_df.sort_values('f1', ascending=False, na_last=True)\n",
    "\n",
    "# print(\"\\næ¨¡å‹æ€§èƒ½æ’å (æŒ‰ F1-Score æ’åº):\")\n",
    "# print(\"=\" * 80)\n",
    "# print(results_df.to_string(index=False))\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # å¯è¦–åŒ–çµæœæ¯”è¼ƒ\n",
    "# if len(results_df) > 0:\n",
    "#     successful_models = results_df.dropna(subset=['f1'])\n",
    "    \n",
    "#     if len(successful_models) > 0:\n",
    "#         fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "#         # F1-Score æ¯”è¼ƒ\n",
    "#         axes[0, 0].barh(successful_models['model'], successful_models['f1'], color='steelblue')\n",
    "#         axes[0, 0].set_xlabel('F1-Score', fontsize=12)\n",
    "#         axes[0, 0].set_title('F1-Score by Model', fontsize=14, fontweight='bold')\n",
    "#         axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "#         # Accuracy æ¯”è¼ƒ\n",
    "#         axes[0, 1].barh(successful_models['model'], successful_models['accuracy'], color='green')\n",
    "#         axes[0, 1].set_xlabel('Accuracy', fontsize=12)\n",
    "#         axes[0, 1].set_title('Accuracy by Model', fontsize=14, fontweight='bold')\n",
    "#         axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "#         # Precision æ¯”è¼ƒ\n",
    "#         axes[1, 0].barh(successful_models['model'], successful_models['precision'], color='orange')\n",
    "#         axes[1, 0].set_xlabel('Precision', fontsize=12)\n",
    "#         axes[1, 0].set_title('Precision by Model', fontsize=14, fontweight='bold')\n",
    "#         axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "#         # Recall æ¯”è¼ƒ\n",
    "#         axes[1, 1].barh(successful_models['model'], successful_models['recall'], color='red')\n",
    "#         axes[1, 1].set_xlabel('Recall', fontsize=12)\n",
    "#         axes[1, 1].set_title('Recall by Model', fontsize=14, fontweight='bold')\n",
    "#         axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         # é¡¯ç¤ºæœ€ä½³æ¨¡å‹\n",
    "#         best_model = successful_models.iloc[0]\n",
    "#         print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['model']}\")\n",
    "#         print(f\"   F1-Score: {best_model['f1']:.4f}\")\n",
    "#         print(f\"   Accuracy: {best_model['accuracy']:.4f}\")\n",
    "#         print(f\"   Precision: {best_model['precision']:.4f}\")\n",
    "#         print(f\"   Recall: {best_model['recall']:.4f}\")\n",
    "\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"âœ… æ‰€æœ‰æ¨¡å‹éæ­·å®Œæˆ\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# # é¡¯ç¤ºæœ€çµ‚çµæœæ‘˜è¦\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139adde",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ11 ï¼š ç‰¹å¾µé‡è¦æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ee390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ11ï¼šç‰¹å¾µé‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹ï¼‰\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from scipy.stats import weightedtau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š æ­¥é©Ÿ11ï¼šç‰¹å¾µé‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰å·²è¨“ç·´çš„æ¨¡å‹\n",
    "if 'pipeline' not in globals() or pipeline.model is None:\n",
    "    raise ValueError(\"âŒ è«‹å…ˆåŸ·è¡Œ pipeline.fit() è¨“ç·´æ¨¡å‹\")\n",
    "\n",
    "print(f\"âœ… ä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹: {pipeline.model_type}\")\n",
    "\n",
    "# =====================================================\n",
    "# 1ï¸âƒ£ MDI (Mean Decrease Impurity) - åƒ…é©ç”¨æ–¼æ¨¹æ¨¡å‹\n",
    "# =====================================================\n",
    "\n",
    "def get_mdi_from_model(model, X_train):\n",
    "    \"\"\"\n",
    "    å¾å·²è¨“ç·´çš„æ¨¹æ¨¡å‹ç²å– MDI é‡è¦æ€§\n",
    "    \n",
    "    é©ç”¨ï¼šXGBoost, LightGBM, Random Forest, Extra Trees, Decision Tree, Gradient Boosting\n",
    "    \"\"\"\n",
    "    print(f\"\\n1ï¸âƒ£ è¨ˆç®— MDI (Mean Decrease Impurity)...\")\n",
    "    \n",
    "    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ feature_importances_ å±¬æ€§\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # å‰µå»º Series\n",
    "        mdi_importance = pd.Series(importances, index=X_train.columns)\n",
    "        mdi_importance = mdi_importance.sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"   âœ… MDI è¨ˆç®—å®Œæˆ\")\n",
    "        print(f\"   å‰10å€‹é‡è¦ç‰¹å¾µ:\")\n",
    "        for i, (feat, imp) in enumerate(mdi_importance.head(10).items(), 1):\n",
    "            print(f\"     {i:2d}. {feat}: {imp:.6f}\")\n",
    "        \n",
    "        return mdi_importance\n",
    "    else:\n",
    "        print(f\"   âš ï¸ æ¨¡å‹ {type(model).__name__} ä¸æ”¯æ´ MDIï¼ˆéæ¨¹æ¨¡å‹ï¼‰\")\n",
    "        return None\n",
    "\n",
    "# =====================================================\n",
    "# 2ï¸âƒ£ MDA (Mean Decrease Accuracy) - ä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹\n",
    "# =====================================================\n",
    "\n",
    "def calculate_mda_from_model(model, X_test, y_test, sample_weight_test=None,\n",
    "                            scoring='neg_log_loss', n_permutations=1, random_state=42):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹è¨ˆç®— MDA (Mean Decrease Accuracy)\n",
    "    \n",
    "    é©ç”¨ï¼šä»»æ„æ¨¡å‹\n",
    "    \"\"\"\n",
    "    print(f\"\\n2ï¸âƒ£ è¨ˆç®— MDA (Mean Decrease Accuracy)...\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # è¨ˆç®—åŸºæº–åˆ†æ•¸\n",
    "    if scoring == 'neg_log_loss':\n",
    "        baseline_pred = model.predict_proba(X_test)\n",
    "        baseline_score = -log_loss(y_test, baseline_pred, \n",
    "                                   sample_weight=sample_weight_test.values if sample_weight_test is not None else None)\n",
    "    elif scoring == 'accuracy':\n",
    "        baseline_pred = model.predict(X_test)\n",
    "        baseline_score = accuracy_score(y_test, baseline_pred,\n",
    "                                       sample_weight=sample_weight_test.values if sample_weight_test is not None else None)\n",
    "    else:\n",
    "        raise ValueError(f\"ä¸æ”¯æ´çš„è©•åˆ†æ–¹æ³•: {scoring}\")\n",
    "    \n",
    "    print(f\"   åŸºæº–åˆ†æ•¸ ({scoring}): {baseline_score:.6f}\")\n",
    "    \n",
    "    # å°æ¯å€‹ç‰¹å¾µé€²è¡Œç½®æ›æ¸¬è©¦\n",
    "    mda_scores = {}\n",
    "    \n",
    "    for i, feature in enumerate(X_test.columns, 1):\n",
    "        perm_scores = []\n",
    "        \n",
    "        for perm in range(n_permutations):\n",
    "            # è¤‡è£½æ¸¬è©¦é›†\n",
    "            X_perm = X_test.copy()\n",
    "            \n",
    "            # æ‰“äº‚è©²ç‰¹å¾µçš„å€¼\n",
    "            perm_values = X_perm[feature].values.copy()\n",
    "            np.random.shuffle(perm_values)\n",
    "            X_perm[feature] = perm_values\n",
    "            \n",
    "            # è¨ˆç®—ç½®æ›å¾Œçš„åˆ†æ•¸\n",
    "            if scoring == 'neg_log_loss':\n",
    "                perm_pred = model.predict_proba(X_perm)\n",
    "                perm_score = -log_loss(y_test, perm_pred,\n",
    "                                     sample_weight=sample_weight_test.values if sample_weight_test is not None else None)\n",
    "            else:  # accuracy\n",
    "                perm_pred = model.predict(X_perm)\n",
    "                perm_score = accuracy_score(y_test, perm_pred,\n",
    "                                          sample_weight=sample_weight_test.values if sample_weight_test is not None else None)\n",
    "            \n",
    "            perm_scores.append(perm_score)\n",
    "        \n",
    "        # è¨ˆç®—é‡è¦æ€§ï¼ˆåŸºæº–åˆ†æ•¸ - ç½®æ›å¾Œåˆ†æ•¸ï¼‰\n",
    "        avg_perm_score = np.mean(perm_scores)\n",
    "        importance = baseline_score - avg_perm_score\n",
    "        mda_scores[feature] = importance\n",
    "        \n",
    "        if (i - 1) % 50 == 0 or i == len(X_test.columns):\n",
    "            print(f\"   é€²åº¦: {i}/{len(X_test.columns)} ({i/len(X_test.columns)*100:.1f}%)\")\n",
    "    \n",
    "    # å‰µå»º Series\n",
    "    mda_importance = pd.Series(mda_scores)\n",
    "    mda_importance = mda_importance.sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"   âœ… MDA è¨ˆç®—å®Œæˆ\")\n",
    "    print(f\"   å‰10å€‹é‡è¦ç‰¹å¾µ:\")\n",
    "    for i, (feat, imp) in enumerate(mda_importance.head(10).items(), 1):\n",
    "        print(f\"     {i:2d}. {feat}: {imp:.6f}\")\n",
    "    \n",
    "    return mda_importance\n",
    "\n",
    "# =====================================================\n",
    "# 3ï¸âƒ£ SFI (Single Feature Importance) - ä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹é¡å‹\n",
    "# =====================================================\n",
    "\n",
    "def calculate_sfi_from_model_type(model_type, X_train, y_train, t1_train, \n",
    "                                  sample_weight_train=None, cv_splits=5, \n",
    "                                  pct_embargo=0.01, scoring='neg_log_loss', \n",
    "                                  random_state=42):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é¡å‹è¨ˆç®— SFI (Single Feature Importance)\n",
    "    \n",
    "    é©ç”¨ï¼šä»»æ„æ¨¡å‹\n",
    "    \"\"\"\n",
    "    print(f\"\\n3ï¸âƒ£ è¨ˆç®— SFI (Single Feature Importance)...\")\n",
    "    \n",
    "    # è¼”åŠ©å‡½æ•¸ï¼šPurged Time Series Split\n",
    "    def purged_time_series_split(X, y, t1, n_splits=5, pct_embargo=0.01):\n",
    "        indices = np.arange(len(X))\n",
    "        mbrg = int(len(X) * pct_embargo)\n",
    "        sorted_indices = X.index.argsort()\n",
    "        test_size = len(X) // (n_splits + 1)\n",
    "        \n",
    "        for i in range(n_splits):\n",
    "            test_start = (i + 1) * test_size\n",
    "            test_end = min((i + 2) * test_size, len(X))\n",
    "            test_idx = sorted_indices[test_start:test_end]\n",
    "            train_end = test_start - mbrg\n",
    "            train_idx = sorted_indices[:train_end]\n",
    "            yield train_idx, test_idx\n",
    "    \n",
    "    # è¼”åŠ©å‡½æ•¸ï¼šäº¤å‰é©—è­‰è©•åˆ†\n",
    "    def cv_score_single_feature(clf, X, y, sample_weight, scoring, cv):\n",
    "        scores = []\n",
    "        for train_idx, test_idx in cv:\n",
    "            X_train_fold = X.iloc[train_idx]\n",
    "            X_test_fold = X.iloc[test_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            y_test_fold = y.iloc[test_idx]\n",
    "            sw_train = sample_weight.iloc[train_idx].values if sample_weight is not None else None\n",
    "            sw_test = sample_weight.iloc[test_idx].values if sample_weight is not None else None\n",
    "            \n",
    "            clf.fit(X_train_fold, y_train_fold, sample_weight=sw_train)\n",
    "            \n",
    "            if scoring == 'neg_log_loss':\n",
    "                y_pred_proba = clf.predict_proba(X_test_fold)\n",
    "                score = -log_loss(y_test_fold, y_pred_proba, sample_weight=sw_test)\n",
    "            elif scoring == 'accuracy':\n",
    "                y_pred = clf.predict(X_test_fold)\n",
    "                score = accuracy_score(y_test_fold, y_pred, sample_weight=sw_test)\n",
    "            scores.append(score)\n",
    "        return np.array(scores)\n",
    "    \n",
    "    # æ ¹æ“šæ¨¡å‹é¡å‹å‰µå»ºåˆ†é¡å™¨\n",
    "    if model_type == 'xgb' or model_type == 'xgboost':\n",
    "        from xgboost import XGBClassifier\n",
    "        clf = XGBClassifier(n_estimators=100, random_state=random_state, \n",
    "                           eval_metric='mlogloss', use_label_encoder=False)\n",
    "    elif model_type == 'lgb' or model_type == 'lightgbm':\n",
    "        from lightgbm import LGBMClassifier\n",
    "        clf = LGBMClassifier(n_estimators=100, random_state=random_state, verbose=-1)\n",
    "    elif model_type == 'rf' or model_type == 'random_forest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
    "    elif model_type == 'et' or model_type == 'extra_trees':\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        clf = ExtraTreesClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
    "    elif model_type == 'gb' or model_type == 'gradient_boosting':\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        clf = GradientBoostingClassifier(n_estimators=100, random_state=random_state)\n",
    "    elif model_type == 'lr' or model_type == 'logistic_regression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        clf = LogisticRegression(random_state=random_state, max_iter=1000, solver='lbfgs')\n",
    "    elif model_type == 'svm':\n",
    "        from sklearn.svm import SVC\n",
    "        clf = SVC(probability=True, random_state=random_state)\n",
    "    elif model_type == 'mlp':\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        clf = MLPClassifier(random_state=random_state, max_iter=500)\n",
    "    else:\n",
    "        print(f\"   âš ï¸ æ¨¡å‹é¡å‹ {model_type} ä¸æ”¯æ´ SFIï¼Œè·³é\")\n",
    "        return None\n",
    "    \n",
    "    # å°æ¯å€‹ç‰¹å¾µå–®ç¨è¨“ç·´æ¨¡å‹\n",
    "    sfi_scores = {}\n",
    "    \n",
    "    for i, feature in enumerate(X_train.columns, 1):\n",
    "        # åªä½¿ç”¨å–®ä¸€ç‰¹å¾µ\n",
    "        X_single = X_train[[feature]]\n",
    "        \n",
    "        # äº¤å‰é©—è­‰\n",
    "        try:\n",
    "            cv = purged_time_series_split(X_single, y_train, t1_train, \n",
    "                                         n_splits=cv_splits, pct_embargo=pct_embargo)\n",
    "            scores = cv_score_single_feature(clf, X_single, y_train, sample_weight_train, \n",
    "                                           scoring, cv)\n",
    "            sfi_scores[feature] = scores.mean()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ ç‰¹å¾µ {feature} è¨ˆç®—å¤±æ•—: {str(e)}\")\n",
    "            sfi_scores[feature] = -np.inf\n",
    "        \n",
    "        if (i - 1) % 50 == 0 or i == len(X_train.columns):\n",
    "            print(f\"   é€²åº¦: {i}/{len(X_train.columns)} ({i/len(X_train.columns)*100:.1f}%)\")\n",
    "    \n",
    "    # å‰µå»º Series\n",
    "    sfi_importance = pd.Series(sfi_scores)\n",
    "    sfi_importance = sfi_importance.sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"   âœ… SFI è¨ˆç®—å®Œæˆ\")\n",
    "    print(f\"   å‰10å€‹é‡è¦ç‰¹å¾µ:\")\n",
    "    for i, (feat, imp) in enumerate(sfi_importance.head(10).items(), 1):\n",
    "        print(f\"     {i:2d}. {feat}: {imp:.6f}\")\n",
    "    \n",
    "    return sfi_importance\n",
    "\n",
    "# =====================================================\n",
    "# 4ï¸âƒ£ PCA æ­£äº¤åŒ–ç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "def calculate_pca_importance(X_train, X_test, explained_variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— PCA æ­£äº¤åŒ–ç‰¹å¾µçš„é‡è¦æ€§\n",
    "    \"\"\"\n",
    "    print(f\"\\n4ï¸âƒ£ è¨ˆç®— PCA æ­£äº¤åŒ–ç‰¹å¾µ...\")\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # PCA åˆ†è§£\n",
    "    pca = PCA()\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # è¨ˆç®—ç´¯ç©è§£é‡‹è®Šç•°\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # æ‰¾å‡ºé”åˆ°é–¾å€¼çš„ä¸»æˆåˆ†æ•¸é‡\n",
    "    n_components = np.argmax(cumulative_variance >= explained_variance_threshold) + 1\n",
    "    \n",
    "    print(f\"   ç¸½ç‰¹å¾µæ•¸: {len(X_train.columns)}\")\n",
    "    print(f\"   é”åˆ° {explained_variance_threshold*100}% è§£é‡‹è®Šç•°éœ€è¦ {n_components} å€‹ä¸»æˆåˆ†\")\n",
    "    print(f\"   å‰10å€‹ä¸»æˆåˆ†çš„è§£é‡‹è®Šç•°æ¯”ä¾‹:\")\n",
    "    for i in range(min(10, len(explained_variance_ratio))):\n",
    "        print(f\"     PC{i+1}: {explained_variance_ratio[i]:.4f} ({cumulative_variance[i]*100:.2f}%)\")\n",
    "    \n",
    "    # å‰µå»ºä¸»æˆåˆ† DataFrame\n",
    "    pca_components = pd.DataFrame(\n",
    "        pca.components_[:n_components].T,\n",
    "        index=X_train.columns,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # å‰µå»ºé‡è¦æ€§ Seriesï¼ˆä½¿ç”¨è§£é‡‹è®Šç•°æ¯”ä¾‹ï¼‰\n",
    "    pca_importance = pd.Series(\n",
    "        explained_variance_ratio[:n_components],\n",
    "        index=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # è½‰æ›ç‚º DataFrame\n",
    "    X_train_pca = pd.DataFrame(\n",
    "        X_train_pca[:, :n_components],\n",
    "        index=X_train.index,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    X_test_pca = pd.DataFrame(\n",
    "        X_test_pca[:, :n_components],\n",
    "        index=X_test.index,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… PCA è¨ˆç®—å®Œæˆ\")\n",
    "    \n",
    "    return pca_components, pca_importance, X_train_pca, X_test_pca\n",
    "\n",
    "# =====================================================\n",
    "# 5ï¸âƒ£ Weighted Kendall's Tau\n",
    "# =====================================================\n",
    "\n",
    "def calculate_weighted_kendall_tau(importance_rank, pca_rank):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Weighted Kendall's Tau\n",
    "    \"\"\"\n",
    "    # ç¢ºä¿ç´¢å¼•ä¸€è‡´\n",
    "    common_idx = importance_rank.index.intersection(pca_rank.index)\n",
    "    \n",
    "    if len(common_idx) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    importance_values = importance_rank.loc[common_idx].values\n",
    "    pca_values = pca_rank.loc[common_idx].values\n",
    "    \n",
    "    # è¨ˆç®— Weighted Kendall's Tau\n",
    "    tau, p_value = weightedtau(importance_values, pca_values)\n",
    "    \n",
    "    return tau, p_value\n",
    "\n",
    "# =====================================================\n",
    "# ä¸»ç¨‹å¼ï¼šåŸ·è¡Œæ‰€æœ‰ç‰¹å¾µé‡è¦æ€§åˆ†æ\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"é–‹å§‹ç‰¹å¾µé‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹ï¼‰...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç²å–å·²è¨“ç·´çš„æ¨¡å‹\n",
    "trained_model = pipeline.model\n",
    "model_type = pipeline.model_type\n",
    "\n",
    "# å¦‚æœæ¨¡å‹åœ¨ scaler ä¹‹å¾Œï¼Œéœ€è¦å…ˆæ¨™æº–åŒ–\n",
    "if pipeline.scaler is not None:\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        pipeline.scaler.transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        pipeline.scaler.transform(X_test),\n",
    "        index=X_test.index,\n",
    "        columns=X_test.columns\n",
    "    )\n",
    "else:\n",
    "    X_train_scaled = X_train\n",
    "    X_test_scaled = X_test\n",
    "\n",
    "# 1. MDIï¼ˆå¾å·²è¨“ç·´çš„æ¨¡å‹ç²å–ï¼‰\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"1ï¸âƒ£ MDI åˆ†æ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "mdi_importance = get_mdi_from_model(trained_model, X_train)\n",
    "\n",
    "# 2. MDAï¼ˆä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹ï¼‰\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"2ï¸âƒ£ MDA åˆ†æ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "mda_importance = calculate_mda_from_model(\n",
    "    model=trained_model,\n",
    "    X_test=X_test_scaled,\n",
    "    y_test=y_test,\n",
    "    sample_weight_test=sample_weight_test,\n",
    "    scoring='neg_log_loss',\n",
    "    n_permutations=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. SFIï¼ˆä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é¡å‹ï¼‰\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"3ï¸âƒ£ SFI åˆ†æ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "sfi_importance = calculate_sfi_from_model_type(\n",
    "    model_type=model_type,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    t1_train=t1_train,\n",
    "    sample_weight_train=sample_weight_train,\n",
    "    cv_splits=5,\n",
    "    pct_embargo=0.01,\n",
    "    scoring='neg_log_loss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. PCA æ­£äº¤åŒ–\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"4ï¸âƒ£ PCA æ­£äº¤åŒ–åˆ†æ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "pca_components, pca_importance, X_train_pca, X_test_pca = calculate_pca_importance(\n",
    "    X_train, X_test,\n",
    "    explained_variance_threshold=0.95\n",
    ")\n",
    "\n",
    "# 5. Weighted Kendall's Tauï¼ˆä¸€è‡´æ€§æª¢æŸ¥ï¼‰\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"5ï¸âƒ£ Weighted Kendall's Tau ä¸€è‡´æ€§æª¢æŸ¥\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if pca_importance is not None:\n",
    "    consistency_results = {}\n",
    "    \n",
    "    # MDI vs PCA\n",
    "    if mdi_importance is not None:\n",
    "        try:\n",
    "            mdi_pca_mapped = {}\n",
    "            for feat in mdi_importance.index:\n",
    "                if feat in pca_components.index:\n",
    "                    loadings = pca_components.loc[feat].abs() * pca_importance.values\n",
    "                    mdi_pca_mapped[feat] = loadings.sum()\n",
    "            \n",
    "            if len(mdi_pca_mapped) > 0:\n",
    "                mdi_pca_series = pd.Series(mdi_pca_mapped)\n",
    "                tau, p_value = calculate_weighted_kendall_tau(mdi_importance, mdi_pca_series)\n",
    "                consistency_results['MDI vs PCA'] = {'tau': tau, 'p_value': p_value}\n",
    "                print(f\"   MDI vs PCA: Ï„ = {tau:.4f}, p-value = {p_value:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ MDI vs PCA è¨ˆç®—å¤±æ•—: {str(e)}\")\n",
    "    \n",
    "    # MDA vs PCA\n",
    "    if mda_importance is not None:\n",
    "        try:\n",
    "            mda_pca_mapped = {}\n",
    "            for feat in mda_importance.index:\n",
    "                if feat in pca_components.index:\n",
    "                    loadings = pca_components.loc[feat].abs() * pca_importance.values\n",
    "                    mda_pca_mapped[feat] = loadings.sum()\n",
    "            \n",
    "            if len(mda_pca_mapped) > 0:\n",
    "                mda_pca_series = pd.Series(mda_pca_mapped)\n",
    "                tau, p_value = calculate_weighted_kendall_tau(mda_importance, mda_pca_series)\n",
    "                consistency_results['MDA vs PCA'] = {'tau': tau, 'p_value': p_value}\n",
    "                print(f\"   MDA vs PCA: Ï„ = {tau:.4f}, p-value = {p_value:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ MDA vs PCA è¨ˆç®—å¤±æ•—: {str(e)}\")\n",
    "    \n",
    "    # SFI vs PCA\n",
    "    if sfi_importance is not None:\n",
    "        try:\n",
    "            sfi_pca_mapped = {}\n",
    "            for feat in sfi_importance.index:\n",
    "                if feat in pca_components.index:\n",
    "                    loadings = pca_components.loc[feat].abs() * pca_importance.values\n",
    "                    sfi_pca_mapped[feat] = loadings.sum()\n",
    "            \n",
    "            if len(sfi_pca_mapped) > 0:\n",
    "                sfi_pca_series = pd.Series(sfi_pca_mapped)\n",
    "                tau, p_value = calculate_weighted_kendall_tau(sfi_importance, sfi_pca_series)\n",
    "                consistency_results['SFI vs PCA'] = {'tau': tau, 'p_value': p_value}\n",
    "                print(f\"   SFI vs PCA: Ï„ = {tau:.4f}, p-value = {p_value:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ SFI vs PCA è¨ˆç®—å¤±æ•—: {str(e)}\")\n",
    "\n",
    "# =====================================================\n",
    "# å¯è¦–åŒ–çµæœ\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š å¯è¦–åŒ–ç‰¹å¾µé‡è¦æ€§çµæœ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. MDI Top 20\n",
    "if mdi_importance is not None:\n",
    "    top_mdi = mdi_importance.head(20)\n",
    "    axes[0, 0].barh(range(len(top_mdi)), top_mdi.values)\n",
    "    axes[0, 0].set_yticks(range(len(top_mdi)))\n",
    "    axes[0, 0].set_yticklabels(top_mdi.index, fontsize=8)\n",
    "    axes[0, 0].set_xlabel('MDI Importance', fontsize=12)\n",
    "    axes[0, 0].set_title('MDI: Top 20 Features', fontsize=14)\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'MDI not available\\n(éæ¨¹æ¨¡å‹)', \n",
    "                   ha='center', va='center', fontsize=12)\n",
    "    axes[0, 0].set_title('MDI: Top 20 Features', fontsize=14)\n",
    "\n",
    "# 2. MDA Top 20\n",
    "if mda_importance is not None:\n",
    "    top_mda = mda_importance.head(20)\n",
    "    axes[0, 1].barh(range(len(top_mda)), top_mda.values)\n",
    "    axes[0, 1].set_yticks(range(len(top_mda)))\n",
    "    axes[0, 1].set_yticklabels(top_mda.index, fontsize=8)\n",
    "    axes[0, 1].set_xlabel('MDA Importance', fontsize=12)\n",
    "    axes[0, 1].set_title('MDA: Top 20 Features', fontsize=14)\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. SFI Top 20\n",
    "if sfi_importance is not None:\n",
    "    top_sfi = sfi_importance.head(20)\n",
    "    axes[1, 0].barh(range(len(top_sfi)), top_sfi.values)\n",
    "    axes[1, 0].set_yticks(range(len(top_sfi)))\n",
    "    axes[1, 0].set_yticklabels(top_sfi.index, fontsize=8)\n",
    "    axes[1, 0].set_xlabel('SFI Importance', fontsize=12)\n",
    "    axes[1, 0].set_title('SFI: Top 20 Features', fontsize=14)\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. PCA è§£é‡‹è®Šç•°\n",
    "if pca_importance is not None:\n",
    "    top_pca = pca_importance.head(20)\n",
    "    axes[1, 1].bar(range(len(top_pca)), top_pca.values)\n",
    "    axes[1, 1].set_xticks(range(len(top_pca)))\n",
    "    axes[1, 1].set_xticklabels(top_pca.index, rotation=45, ha='right', fontsize=8)\n",
    "    axes[1, 1].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "    axes[1, 1].set_title('PCA: Top 20 Components', fontsize=14)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… ç‰¹å¾µé‡è¦æ€§åˆ†æå®Œæˆ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9f6b8",
   "metadata": {},
   "source": [
    "## è‡ªå‹•åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f611be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2014-01-01'\n",
    "test_horizon_start_year = 2020\n",
    "rolling_years = 6          # 2020~2024\n",
    "model_types = ['lr', 'xgboost', 'catboost']  # éœ€è¦è·‘çš„æ¨¡å‹ï¼Œå¯è‡ªè¡Œèª¿æ•´\n",
    "results_dir = Path('pred_results')\n",
    "results_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b470c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pred_df(test_idx, t1_series, events_df, y_true_series,\n",
    "                  pred_series, prob_array, class_mapping):\n",
    "    pred_df = pd.DataFrame(index=test_idx)\n",
    "    pred_df['t1'] = t1_series.reindex(test_idx)\n",
    "    pred_df['side'] = events_df.loc[test_idx, 'side']\n",
    "    pred_df['true_label'] = y_true_series.reindex(test_idx).map(class_mapping)\n",
    "    pred_df['predicted_label'] = pred_series.loc[test_idx].values\n",
    "\n",
    "    for i, class_id in enumerate(sorted(class_mapping.values())):\n",
    "        prob_col = f'prob_class_{class_id}'\n",
    "        pred_df[prob_col] = prob_array[:, i]\n",
    "\n",
    "    pred_df['prediction_confidence'] = np.max(prob_array, axis=1)\n",
    "    pred_df['is_correct'] = (\n",
    "        pred_df['predicted_label'] == pred_df['true_label']\n",
    "    )\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_walk_forward(model_type, first_test_year=2020, n_years=5):\n",
    "    yearly_pred_dfs = []\n",
    "    tz_info = X.index.tz  # è‹¥ index æœ‰æ™‚å€ï¼Œç¢ºä¿ä¸€è‡´\n",
    "\n",
    "    for year in range(first_test_year, first_test_year + n_years):\n",
    "        test_start_dt = pd.Timestamp(f'{year}-01-01', tz=tz_info)\n",
    "        test_end_dt = test_start_dt + pd.DateOffset(years=1)\n",
    "\n",
    "        train_mask = (X.index >= train_start) & (X.index < test_start_dt)\n",
    "        test_mask = (X.index >= test_start_dt) & (X.index < test_end_dt)\n",
    "\n",
    "        if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
    "            print(f'âš ï¸ {model_type}: {year} ç„¡å¯ç”¨è³‡æ–™ï¼Œè·³é')\n",
    "            continue\n",
    "\n",
    "        X_train = X.loc[train_mask]\n",
    "        y_train = y_mapped.loc[train_mask]\n",
    "        t1_train = t1.loc[train_mask]\n",
    "        sw_train = sample_weight_series.loc[train_mask] if sample_weight_series is not None else None\n",
    "\n",
    "        X_test = X.loc[test_mask]\n",
    "        y_test = y_mapped.loc[test_mask]\n",
    "        t1_test = t1.loc[test_mask]\n",
    "        sw_test = sample_weight_series.loc[test_mask] if sample_weight_series is not None else None\n",
    "\n",
    "        print('=' * 70)\n",
    "        print(f'ğŸ¯ Model: {model_type.upper()} | Test year: {year}')\n",
    "        print(f'Train window: {X_train.index.min()} â†’ {X_train.index.max()} ({len(X_train)} rows)')\n",
    "        print(f'Test window : {X_test.index.min()} â†’ {X_test.index.max()} ({len(X_test)} rows)')\n",
    "\n",
    "        pipeline = PredictionPipeline(\n",
    "            model_type=model_type,\n",
    "            cv_scoring='f1_weighted',\n",
    "            use_hyperopt=True,\n",
    "            use_cv=True,\n",
    "            cv_splits=5,\n",
    "            pct_embargo=0.01,\n",
    "            hyperopt_method='grid',\n",
    "            n_iter=20,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        pipeline.fit(X_train, y_train, t1_train, sw_train)\n",
    "\n",
    "        preds_mapped, probs = pipeline.predict(X_test)\n",
    "        preds_original = (\n",
    "            pd.Series(np.ravel(preds_mapped), index=X_test.index)\n",
    "            .map(reverse_mapping)\n",
    "        )\n",
    "\n",
    "        pred_df = build_pred_df(\n",
    "            test_idx=X_test.index,\n",
    "            t1_series=t1,\n",
    "            events_df=events,\n",
    "            y_true_series=y,\n",
    "            pred_series=preds_original,\n",
    "            prob_array=probs,\n",
    "            class_mapping=reverse_mapping,\n",
    "        )\n",
    "        pred_df['model_type'] = model_type\n",
    "        pred_df['test_year'] = year\n",
    "        pred_df['train_window_start'] = pd.Timestamp(train_start)\n",
    "        pred_df['train_window_end'] = test_start_dt - pd.Timedelta(seconds=1)\n",
    "\n",
    "        yearly_pred_dfs.append(pred_df)\n",
    "\n",
    "        # è‹¥éœ€è¦å³æ™‚æª¢è¦–ï¼Œå¯åœ¨æ­¤å‘¼å« pipeline.plot_results(X_test, y_test, sw_test)\n",
    "\n",
    "    if not yearly_pred_dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out_df = pd.concat(yearly_pred_dfs).sort_index()\n",
    "    out_path = results_dir / f'pred_{model_type}_{first_test_year}_{first_test_year + len(yearly_pred_dfs) - 1}.csv'\n",
    "    out_df.to_csv(out_path)\n",
    "    print(f'âœ… {model_type} èµ°å‹¢é æ¸¬å®Œæˆï¼Œè¼¸å‡º {out_path}')\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rolling_predictions = {}\n",
    "\n",
    "for m in model_types:\n",
    "    rolling_predictions[m] = run_walk_forward(\n",
    "        model_type=m,\n",
    "        first_test_year=test_horizon_start_year,\n",
    "        n_years=rolling_years,\n",
    "    )\n",
    "\n",
    "display({m: df.tail() for m, df in rolling_predictions.items() if not df.empty})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f757995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a460c50f",
   "metadata": {},
   "source": [
    "## å…¨è‡ªå‹•åŸ·è¡Œå•†å“Xæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130b4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from scipy.stats import rv_continuous, kstest\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Any, List# =====================================================\n",
    "# 3ï¸âƒ£ Meta-Labeling Class\n",
    "# =====================================================\n",
    "class MetaLabeling:\n",
    "    \"\"\"\n",
    "    Meta-Labelingï¼šTriple Barrier å’Œæ¨™ç±¤ç”Ÿæˆ\n",
    "    \n",
    "    Triple Barrier Method èªªæ˜ï¼š\n",
    "    ============================\n",
    "    Triple Barrier Method æ˜¯ä¸€ç¨®æ¨™ç±¤æ–¹æ³•ï¼Œæ ¹æ“šä¸‰å€‹å±éšœä¸­ç¬¬ä¸€å€‹è¢«è§¸åŠçš„\n",
    "    å±éšœä¾†æ¨™è¨˜è§€æ¸¬å€¼ã€‚\n",
    "    \n",
    "    ç”¨é€”ï¼š\n",
    "    -----\n",
    "    1. å‹•æ…‹è¨­å®šæ­¢ç›ˆæ­¢æï¼šæ ¹æ“šä¼°è¨ˆçš„æ³¢å‹•ç‡ï¼ˆå·²å¯¦ç¾æˆ–éš±å«ï¼‰å‹•æ…‹èª¿æ•´\n",
    "    2. é¿å…ç›®æ¨™éé«˜æˆ–éä½ï¼šè€ƒæ…®ç•¶å‰æ³¢å‹•ç‡ï¼Œè¨­å®šåˆç†çš„æ­¢ç›ˆæ­¢ææ°´å¹³\n",
    "    3. è·¯å¾‘ä¾è³´æ¨™ç±¤ï¼šè€ƒæ…®å¾ [t_i,0, t_i,0 + h] çš„å®Œæ•´åƒ¹æ ¼è·¯å¾‘\n",
    "    \n",
    "    ä¸‰å€‹å±éšœï¼š\n",
    "    --------\n",
    "    1. ä¸Šæ–¹æ°´å¹³å±éšœï¼ˆProfit Takingï¼‰ï¼šæ­¢ç›ˆé™åˆ¶\n",
    "    2. ä¸‹æ–¹æ°´å¹³å±éšœï¼ˆStop Lossï¼‰ï¼šæ­¢æé™åˆ¶\n",
    "    3. å‚ç›´å±éšœï¼ˆVertical Barrierï¼‰ï¼šæ™‚é–“åˆ°æœŸé™åˆ¶ï¼ˆæŒæœ‰æœŸï¼‰\n",
    "    \n",
    "    å±éšœé…ç½®ï¼š\n",
    "    --------\n",
    "    ç”¨ä¸‰å…ƒçµ„ [pt, sl, t1] è¡¨ç¤ºï¼Œå…¶ä¸­ï¼š\n",
    "    - 0 è¡¨ç¤ºå±éšœæœªå•Ÿç”¨\n",
    "    - 1 è¡¨ç¤ºå±éšœå•Ÿç”¨\n",
    "    \n",
    "    å¸¸ç”¨é…ç½®ï¼š\n",
    "    - [1,1,1]: æ¨™æº–è¨­å®šï¼ˆæ­¢ç›ˆã€æ­¢æã€æ™‚é–“åˆ°æœŸï¼‰\n",
    "    - [0,1,1]: åªæœ‰æ­¢æå’Œæ™‚é–“åˆ°æœŸ\n",
    "    - [1,1,0]: åªæœ‰æ­¢ç›ˆå’Œæ­¢æï¼ˆç„¡æ™‚é–“é™åˆ¶ï¼‰\n",
    "    \n",
    "    æ•¸å­¸åŸç†ï¼š\n",
    "    ---------\n",
    "    å°æ–¼æ¯å€‹äº‹ä»¶æ™‚é–“é» t_i,0ï¼š\n",
    "    \n",
    "    1. ä¸Šæ–¹å±éšœï¼ˆæ­¢ç›ˆï¼‰ï¼š\n",
    "       PT = t_i,0 + pt Ã— Ïƒ_t_i,0\n",
    "       å…¶ä¸­ pt æ˜¯æ­¢ç›ˆå€æ•¸ï¼ŒÏƒ_t_i,0 æ˜¯ç›®æ¨™æ³¢å‹•ç‡\n",
    "    \n",
    "    2. ä¸‹æ–¹å±éšœï¼ˆæ­¢æï¼‰ï¼š\n",
    "       SL = t_i,0 - sl Ã— Ïƒ_t_i,0\n",
    "       å…¶ä¸­ sl æ˜¯æ­¢æå€æ•¸\n",
    "    \n",
    "    3. å‚ç›´å±éšœï¼ˆæ™‚é–“åˆ°æœŸï¼‰ï¼š\n",
    "       t_i,1 = t_i,0 + h\n",
    "       å…¶ä¸­ h æ˜¯æŒæœ‰æœŸï¼ˆæœŸæ•¸ï¼‰\n",
    "    \n",
    "    4. ç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼š\n",
    "       t_i,1 = min{PT_touch, SL_touch, t_i,0 + h}\n",
    "    \n",
    "    5. æ¨™ç±¤ç”Ÿæˆï¼š\n",
    "       - bin = 1:  è§¸åŠä¸Šæ–¹å±éšœï¼ˆæˆåŠŸï¼‰\n",
    "       - bin = -1: è§¸åŠä¸‹æ–¹å±éšœï¼ˆå¤±æ•—ï¼‰\n",
    "       - bin = 0:  è§¸åŠå‚ç›´å±éšœï¼ˆæ™‚é–“åˆ°æœŸï¼‰\n",
    "    \n",
    "    è·¯å¾‘ä¾è³´æ€§ï¼š\n",
    "    ----------\n",
    "    ç‚ºäº†æ¨™è¨˜è§€æ¸¬å€¼ï¼Œå¿…é ˆè€ƒæ…®å¾ t_i,0 åˆ° t_i,0 + h çš„å®Œæ•´åƒ¹æ ¼è·¯å¾‘ã€‚\n",
    "    é€™ä½¿å¾—æ¨™ç±¤æ˜¯è·¯å¾‘ä¾è³´çš„ï¼Œè€Œéåƒ…ä¾è³´çµ‚é»åƒ¹æ ¼ã€‚\n",
    "    \n",
    "    å‹•æ…‹é–¾å€¼ï¼š\n",
    "    --------\n",
    "    ä½¿ç”¨ getDailyVol() è¨ˆç®—æ—¥æ³¢å‹•ç‡ï¼Œä½œç‚ºå‹•æ…‹èª¿æ•´æ­¢ç›ˆæ­¢æçš„åŸºç¤ï¼š\n",
    "    \n",
    "    Ïƒ_t = EWM_std(returns, span=span0)\n",
    "    \n",
    "    å…¶ä¸­ returns æ˜¯æ—¥æ”¶ç›Šç‡åºåˆ—ã€‚\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Advances in Financial Machine Learning, Chapter 3\n",
    "    - Snippet 3.1: getDailyVol - è¨ˆç®—æ—¥æ³¢å‹•ç‡\n",
    "    - Snippet 3.2: applyPtSlOnT1 - Triple Barrier æ¨™ç±¤æ–¹æ³•\n",
    "    - Snippet 3.3: getEvents - å–å¾—ç¬¬ä¸€å€‹å±éšœè§¸åŠæ™‚é–“\n",
    "    - Snippet 3.4: getVerticalBarriers - è¨­å®šå‚ç›´å±éšœ\n",
    "    - Snippet 3.5: getBins - ç”Ÿæˆæ¨™ç±¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ptSl: List[float] = [1, 2], numPeriods: int = 100, \n",
    "                 minRet: float = 0.01, min_label_pct: float = 0.05):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        ptSl : list\n",
    "            [æ­¢ç›ˆå€æ•¸, æ­¢æå€æ•¸]\n",
    "            - ptSl[0]: ä¸Šæ–¹å±éšœçš„å€æ•¸ï¼ˆProfit Takingï¼‰\n",
    "            - ptSl[1]: ä¸‹æ–¹å±éšœçš„å€æ•¸ï¼ˆStop Lossï¼‰\n",
    "            - å¦‚æœç‚º 0ï¼Œå‰‡è©²å±éšœä¸å•Ÿç”¨\n",
    "        numPeriods : int\n",
    "            å‚ç›´å±éšœæœŸæ•¸ï¼ˆä¾‹å¦‚ï¼š100 æ ¹ K ç·šï¼‰\n",
    "            å°æ‡‰è«–æ–‡ä¸­çš„ hï¼ˆæŒæœ‰æœŸï¼‰\n",
    "        minRet : float\n",
    "            æœ€å°æ³¢å‹•ç‡é–¾å€¼\n",
    "            åªæœ‰ç•¶ trgt > minRet æ™‚æ‰æœƒç”Ÿæˆäº‹ä»¶\n",
    "        min_label_pct : float\n",
    "            æœ€å°æ¨™ç±¤æ¯”ä¾‹ï¼ˆç”¨æ–¼ dropLabelsï¼‰\n",
    "            ä½æ–¼æ­¤æ¯”ä¾‹çš„æ¨™ç±¤æœƒè¢«ç§»é™¤\n",
    "        \"\"\"\n",
    "        self.ptSl = ptSl\n",
    "        self.numPeriods = numPeriods\n",
    "        self.minRet = minRet\n",
    "        self.min_label_pct = min_label_pct\n",
    "        self.events = None\n",
    "        self.bins = None\n",
    "        \n",
    "    def get_vertical_barriers(self, close: pd.Series, tEvents: pd.DatetimeIndex) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨­å®šå‚ç›´æ™‚é–“å±éšœï¼ˆä»¥æœŸæ•¸ç‚ºå–®ä½ï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼ˆåŸºæ–¼ Snippet 3.4ï¼‰ï¼š\n",
    "        ----------------------------\n",
    "        å°æ¯å€‹äº‹ä»¶æ™‚é–“é» t_i,0ï¼Œæ‰¾åˆ° numPeriods æœŸå¾Œçš„æ™‚é–“é»ã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        t_i,1 = close.index[position(t_i,0) + numPeriods]\n",
    "        \n",
    "        å¦‚æœè¶…å‡ºæ•¸æ“šç¯„åœï¼Œå‰‡ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“é»ã€‚\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        t1 : pd.Series\n",
    "            å‚ç›´å±éšœæ™‚é–“é»ï¼ˆindex=äº‹ä»¶èµ·é», value=å±éšœæ™‚é–“é»ï¼‰\n",
    "        \"\"\"\n",
    "        t1 = []\n",
    "        close_index = close.index\n",
    "        \n",
    "        for tEvent in tEvents:\n",
    "            # æ‰¾åˆ° tEvent åœ¨ close ä¸­çš„ä½ç½®\n",
    "            try:\n",
    "                event_pos = close_index.get_loc(tEvent)\n",
    "                # è¨ˆç®— numPeriods æœŸå¾Œçš„ä½ç½®\n",
    "                barrier_pos = event_pos + self.numPeriods\n",
    "                \n",
    "                # ç¢ºä¿ä¸è¶…éæ•¸æ“šç¯„åœ\n",
    "                if barrier_pos < len(close_index):\n",
    "                    t1.append(close_index[barrier_pos])\n",
    "                else:\n",
    "                    # å¦‚æœè¶…å‡ºç¯„åœï¼Œä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“é»\n",
    "                    t1.append(close_index[-1])\n",
    "            except KeyError:\n",
    "                # å¦‚æœæ‰¾ä¸åˆ°è©²æ™‚é–“é»ï¼Œè·³é\n",
    "                continue\n",
    "        \n",
    "        if len(t1) == 0:\n",
    "            return pd.Series(dtype='datetime64[ns]')\n",
    "        \n",
    "        # ç¢ºä¿ t1 çš„ç´¢å¼•èˆ‡ tEvents å°æ‡‰\n",
    "        t1_series = pd.Series(t1, index=tEvents[:len(t1)])\n",
    "        return t1_series\n",
    "    \n",
    "    def apply_pt_sl_on_t1(self, close: pd.Series, events: pd.DataFrame, \n",
    "                          molecule: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        å¥—ç”¨æ­¢ç›ˆæ­¢æï¼ˆåŸºæ–¼ Snippet 3.2ï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ï¼Œè¨ˆç®—å¾äº‹ä»¶é–‹å§‹åˆ°å‚ç›´å±éšœæœŸé–“çš„åƒ¹æ ¼è·¯å¾‘ï¼š\n",
    "        \n",
    "        path_prices = close[loc:t1]\n",
    "        path_returns = (path_prices / close[loc] - 1) Ã— side\n",
    "        \n",
    "        ç„¶å¾Œæ‰¾å‡ºï¼š\n",
    "        - æœ€æ—©è§¸åŠæ­¢ç›ˆçš„æ™‚é–“ï¼špath_returns > pt Ã— trgt\n",
    "        - æœ€æ—©è§¸åŠæ­¢æçš„æ™‚é–“ï¼špath_returns < -sl Ã— trgt\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        - PT_level = pt Ã— trgt\n",
    "        - SL_level = -sl Ã— trgt\n",
    "        - path_returns = (close[t] / close[loc] - 1) Ã— side\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        events : pd.DataFrame\n",
    "            äº‹ä»¶ DataFrameï¼ˆåŒ…å« t1, trgt, sideï¼‰\n",
    "        molecule : pd.DatetimeIndex\n",
    "            è¦è™•ç†çš„äº‹ä»¶å­é›†ï¼ˆç”¨æ–¼ä¸¦è¡Œè¨ˆç®—ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        out : pd.DataFrame\n",
    "            åŒ…å«æ¯å€‹å±éšœè§¸åŠæ™‚é–“çš„ DataFrame\n",
    "        \"\"\"\n",
    "        events_ = events.loc[molecule]\n",
    "        out = events_[['t1']].copy(deep=True)\n",
    "        \n",
    "        # è¨ˆç®—æ­¢ç›ˆæ°´å¹³\n",
    "        if self.ptSl[0] > 0:\n",
    "            pt = self.ptSl[0] * events_['trgt']  # PT = pt Ã— Ïƒ\n",
    "        else:\n",
    "            pt = pd.Series(index=events.index)  # æœªå•Ÿç”¨\n",
    "        \n",
    "        # è¨ˆç®—æ­¢ææ°´å¹³\n",
    "        if self.ptSl[1] > 0:\n",
    "            sl = -self.ptSl[1] * events_['trgt']  # SL = -sl Ã— Ïƒ\n",
    "        else:\n",
    "            sl = pd.Series(index=events.index)  # æœªå•Ÿç”¨\n",
    "        \n",
    "        # å°æ¯å€‹äº‹ä»¶è¨ˆç®—è·¯å¾‘\n",
    "        for loc, t1 in events_['t1'].fillna(close.index[-1]).items():\n",
    "            # å–å¾—åƒ¹æ ¼è·¯å¾‘\n",
    "            df0 = close[loc:t1]\n",
    "            # è¨ˆç®—è·¯å¾‘å ±é…¬ï¼ˆè€ƒæ…®æ–¹å‘ï¼‰\n",
    "            df0 = (df0 / close[loc] - 1) * events_.at[loc, 'side']\n",
    "            \n",
    "            # æ‰¾å‡ºæœ€æ—©è§¸åŠæ­¢æçš„æ™‚é–“\n",
    "            out.loc[loc, 'sl'] = df0[df0 < sl[loc]].index.min()\n",
    "            # æ‰¾å‡ºæœ€æ—©è§¸åŠæ­¢ç›ˆçš„æ™‚é–“\n",
    "            out.loc[loc, 'pt'] = df0[df0 > pt[loc]].index.min()\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_events(self, close: pd.Series, tEvents: pd.DatetimeIndex, \n",
    "                   trgt: pd.Series, side: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆäº‹ä»¶ä¸¦å¥—ç”¨ Triple Barrierï¼ˆåŸºæ–¼ Snippet 3.3ï¼‰\n",
    "        \n",
    "        å¯¦ç¾æµç¨‹ï¼š\n",
    "        ---------\n",
    "        1. éæ¿¾ç›®æ¨™ï¼šåªä¿ç•™ trgt > minRet çš„äº‹ä»¶\n",
    "        2. è¨­å®šå‚ç›´å±éšœï¼šè¨ˆç®—æ¯å€‹äº‹ä»¶çš„æ™‚é–“åˆ°æœŸé»\n",
    "        3. å¥—ç”¨ Triple Barrierï¼šè¨ˆç®—æ­¢ç›ˆæ­¢æè§¸åŠæ™‚é–“\n",
    "        4. æ‰¾å‡ºç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼št1 = min{pt_touch, sl_touch, vertical_barrier}\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "        tEvents : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»ï¼ˆä¾†è‡ª CUSUM éæ¿¾æˆ–ç­–ç•¥ä¿¡è™Ÿï¼‰\n",
    "        trgt : pd.Series\n",
    "            ç›®æ¨™æ³¢å‹•ç‡ï¼ˆdaily_volï¼Œç”¨æ–¼å‹•æ…‹èª¿æ•´æ­¢ç›ˆæ­¢æï¼‰\n",
    "        side : pd.Series\n",
    "            æ–¹å‘ï¼ˆ1 ç‚ºåšå¤šï¼Œ-1 ç‚ºåšç©ºï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        events : pd.DataFrame\n",
    "            åŒ…å« t1, trgt, side, pt, sl çš„äº‹ä»¶ DataFrame\n",
    "            - t1: ç¬¬ä¸€å€‹å±éšœè§¸åŠæ™‚é–“\n",
    "            - trgt: ç›®æ¨™æ³¢å‹•ç‡\n",
    "            - side: æ–¹å‘\n",
    "            - pt: æ­¢ç›ˆå€æ•¸\n",
    "            - sl: æ­¢æå€æ•¸\n",
    "        \"\"\"\n",
    "        # 1. éæ¿¾ç›®æ¨™ï¼ˆåªä¿ç•™æ³¢å‹•ç‡è¶³å¤ å¤§çš„äº‹ä»¶ï¼‰\n",
    "        trgt = trgt.loc[tEvents]\n",
    "        trgt = trgt[trgt > self.minRet]\n",
    "        \n",
    "        # 2. è¨­å®šå‚ç›´å±éšœ\n",
    "        t1 = self.get_vertical_barriers(close, tEvents)\n",
    "        \n",
    "        # 3. å–å¾—æ–¹å‘\n",
    "        side_ = side.loc[trgt.index]\n",
    "        \n",
    "        # 4. çµ„åˆäº‹ä»¶ç‰©ä»¶\n",
    "        events = pd.concat({\n",
    "            't1': t1.loc[trgt.index], \n",
    "            'trgt': trgt, \n",
    "            'side': side_\n",
    "        }, axis=1)\n",
    "        events = events.dropna(subset=['trgt'])\n",
    "        \n",
    "        # 5. è¨ˆç®—æ­¢ç›ˆæ­¢æè§¸åŠæ™‚é–“\n",
    "        df0 = self.apply_pt_sl_on_t1(close, events, events.index)\n",
    "        \n",
    "        # 6. æ‰¾å‡ºç¬¬ä¸€å€‹è§¸åŠçš„å±éšœï¼ˆæœ€æ—©çš„æ™‚é–“ï¼‰\n",
    "        events['t1'] = df0.dropna(how='all').min(axis=1)\n",
    "        \n",
    "        # 7. å„²å­˜å±éšœé…ç½®\n",
    "        events['pt'] = self.ptSl[0]\n",
    "        events['sl'] = self.ptSl[1]\n",
    "        \n",
    "        self.events = events\n",
    "        return events\n",
    "    \n",
    "    def get_bins(self, close: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ¨™ç±¤ï¼ˆåŸºæ–¼ Snippet 3.5ï¼Œä¸¦æ“´å±•ç‚ºæ”¯æ´ Meta-Labelingï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        1. å°é½Šåƒ¹æ ¼ï¼šå–å¾—äº‹ä»¶é–‹å§‹å’ŒçµæŸæ™‚é–“çš„æ‰€æœ‰åƒ¹æ ¼é»\n",
    "        2. è¨ˆç®—å¯¦éš›å ±é…¬ï¼š\n",
    "           ret = (close[t1] / close[t0] - 1) Ã— side\n",
    "        3. åˆ¤æ–·è§¸åŠçš„å±éšœï¼š\n",
    "           - å¦‚æœ ret > 0 ä¸” ret > pt Ã— trgt â†’ bin = 1ï¼ˆè§¸åŠæ­¢ç›ˆï¼‰\n",
    "           - å¦‚æœ ret < 0 ä¸” ret < -sl Ã— trgt â†’ bin = -1ï¼ˆè§¸åŠæ­¢æï¼‰\n",
    "           - å¦å‰‡ â†’ bin = 0ï¼ˆè§¸åŠå‚ç›´å±éšœï¼‰\n",
    "        \n",
    "        æ¨™ç±¤å®šç¾©ï¼š\n",
    "        --------\n",
    "        - bin = 1:  æˆåŠŸï¼ˆè§¸åŠæ­¢ç›ˆå±éšœï¼‰\n",
    "        - bin = 0:  æ™‚é–“åˆ°æœŸï¼ˆè§¸åŠå‚ç›´å±éšœï¼‰\n",
    "        - bin = -1: è™§æï¼ˆè§¸åŠæ­¢æå±éšœï¼‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            æ”¶ç›¤åƒ¹åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        bins : pd.DataFrame\n",
    "            åŒ…å« ret, trgt, bin, side çš„æ¨™ç±¤ DataFrame\n",
    "        \"\"\"\n",
    "        if self.events is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_events()\")\n",
    "            return None\n",
    "        \n",
    "        events_ = self.events.dropna(subset=['t1'])\n",
    "        px = events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "        px = close.reindex(px, method='bfill')\n",
    "        \n",
    "        out = pd.DataFrame(index=events_.index)\n",
    "        out['ret'] = px.loc[events_['t1'].values].values / px.loc[events_.index] - 1\n",
    "        out['ret'] *= events_['side']\n",
    "        out['trgt'] = events_['trgt']\n",
    "        \n",
    "        # åˆ¤æ–·è§¸åŠå“ªå€‹å±éšœ\n",
    "        out['bin'] = 0  # é è¨­ç‚ºæ™‚é–“åˆ°æœŸ\n",
    "        for date_time, values in out.iterrows():\n",
    "            ret = values['ret']\n",
    "            target = values['trgt']\n",
    "            pt_level = ret > target * self.events.loc[date_time, 'pt']\n",
    "            sl_level = ret < -target * self.events.loc[date_time, 'sl']\n",
    "            \n",
    "            if ret > 0.0 and pt_level:\n",
    "                # è§¸åŠæ­¢ç›ˆå±éšœ â†’ bin = 1 (æˆåŠŸ)\n",
    "                out.loc[date_time, 'bin'] = 1\n",
    "            elif ret < 0.0 and sl_level:\n",
    "                # è§¸åŠæ­¢æå±éšœ â†’ bin = -1 (è™§æ)\n",
    "                out.loc[date_time, 'bin'] = -1\n",
    "            else:\n",
    "                # æ™‚é–“åˆ°æœŸï¼ˆå‚ç›´å±éšœï¼‰â†’ bin = 0\n",
    "                out.loc[date_time, 'bin'] = 0\n",
    "        \n",
    "        # å¦‚æœæœ‰ sideï¼ˆMeta-Labelingï¼‰ï¼Œä¿ç•™ side è³‡è¨Š\n",
    "        if 'side' in events_:\n",
    "            out['side'] = events_['side']\n",
    "        \n",
    "        self.bins = out\n",
    "        return out\n",
    "    \n",
    "    def drop_rare_labels(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç§»é™¤ç¨€æœ‰æ¨™ç±¤ï¼ˆåŸºæ–¼ Snippet 3.8ï¼‰\n",
    "        \n",
    "        ç›®çš„ï¼š\n",
    "        -----\n",
    "        ç§»é™¤æ¨£æœ¬æ•¸éå°‘çš„æ¨™ç±¤é¡åˆ¥ï¼Œé¿å…æ¨¡å‹å­¸ç¿’åæ–œã€‚\n",
    "        \n",
    "        æµç¨‹ï¼š\n",
    "        -----\n",
    "        1. è¨ˆç®—æ¯å€‹æ¨™ç±¤çš„æ¯”ä¾‹\n",
    "        2. å¦‚æœæœ€å°æ¯”ä¾‹ < min_label_pctï¼Œç§»é™¤è©²æ¨™ç±¤\n",
    "        3. é‡è¤‡ç›´åˆ°æ‰€æœ‰æ¨™ç±¤æ¯”ä¾‹éƒ½ >= min_label_pct\n",
    "        \"\"\"\n",
    "        if self.bins is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_bins()\")\n",
    "            return None\n",
    "        \n",
    "        bins = self.bins.copy()\n",
    "        \n",
    "        while True:\n",
    "            df0 = bins['bin'].value_counts(normalize=True)\n",
    "            if df0.min() > self.min_label_pct or df0.shape[0] < 3:\n",
    "                break\n",
    "            print(f\"Dropped label {df0.idxmin()}, percentage: {df0.min():.4f}\")\n",
    "            bins = bins[bins['bin'] != df0.idxmin()]\n",
    "        \n",
    "        self.bins = bins\n",
    "        self.events = self.events.loc[bins.index]\n",
    "        \n",
    "        return bins\n",
    "    \n",
    "    def get_label_stats(self) -> dict:\n",
    "        \"\"\"å–å¾—æ¨™ç±¤çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if self.bins is None:\n",
    "            return None\n",
    "        \n",
    "        label_counts = self.bins['bin'].value_counts()\n",
    "        \n",
    "        return {\n",
    "            'total_labels': len(self.bins),\n",
    "            'label_distribution': label_counts.to_dict(),\n",
    "            'avg_return': self.bins['ret'].mean(),\n",
    "            'win_rate': (self.bins['bin'] == 1).sum() / len(self.bins),\n",
    "            'loss_rate': (self.bins['bin'] == -1).sum() / len(self.bins),\n",
    "            'timeout_rate': (self.bins['bin'] == 0).sum() / len(self.bins)\n",
    "        }\n",
    "    \n",
    "    def plot_labels(self):\n",
    "        \"\"\"ç¹ªè£½æ¨™ç±¤åˆ†å¸ƒåœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.bins is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ get_bins()\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # æ¨™ç±¤åˆ†å¸ƒ\n",
    "        label_counts = self.bins['bin'].value_counts().sort_index()\n",
    "        colors = {-1: 'red', 0: 'gray', 1: 'green'}\n",
    "        label_colors = [colors.get(idx, 'steelblue') for idx in label_counts.index]\n",
    "        \n",
    "        label_counts.plot(kind='bar', ax=axes[0], color=label_colors)\n",
    "        axes[0].set_title('Label Distribution', fontsize=12)\n",
    "        axes[0].set_xlabel('Label (-1: Loss, 0: Timeout, 1: Win)')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # å›å ±åˆ†å¸ƒ\n",
    "        self.bins['ret'].hist(bins=50, ax=axes[1], alpha=0.7, color='steelblue')\n",
    "        axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Return')\n",
    "        axes[1].set_title('Return Distribution', fontsize=12)\n",
    "        axes[1].set_xlabel('Return')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d82aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4ï¸âƒ£ æ¨£æœ¬æ¬Šé‡ Classï¼ˆä¸¦ç™¼åº¦ã€å”¯ä¸€æ€§ã€æ™‚é–“è¡°æ¸›ã€é¡åˆ¥å¹³è¡¡ï¼‰\n",
    "# =====================================================\n",
    "class SampleWeight:\n",
    "    \"\"\"\n",
    "    æ¨£æœ¬æ¬Šé‡è¨ˆç®—ï¼šä¸¦ç™¼åº¦ã€å”¯ä¸€æ€§ã€æ™‚é–“è¡°æ¸›ã€é¡åˆ¥å¹³è¡¡\n",
    "    \n",
    "    Chapter 4: Sample Weights èªªæ˜\n",
    "    ===============================\n",
    "    \n",
    "    å•é¡ŒèƒŒæ™¯ï¼š\n",
    "    --------\n",
    "    åœ¨é‡‘èæ‡‰ç”¨ä¸­ï¼Œè§€æ¸¬å€¼ä¸æ˜¯ç”±ç¨ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰éç¨‹ç”Ÿæˆçš„ã€‚\n",
    "    ç•¶å…©å€‹æ¨™ç±¤ y_i å’Œ y_j çš„æ™‚é–“å€é–“æœ‰é‡ç–Šæ™‚ï¼ˆt_i,1 > t_j,0ï¼‰ï¼Œ\n",
    "    å®ƒå€‘æœƒä¾è³´å…±åŒçš„å ±é…¬ r_{t_j,0, min{t_i,1, t_j,1}}ã€‚\n",
    "    \n",
    "    é€™å°è‡´æ¨™ç±¤åºåˆ— {y_i}_{i=1,...,I} ä¸æ˜¯ IIDï¼Œé•åäº†å¤§å¤šæ•¸ ML ç®—æ³•çš„å‡è¨­ã€‚\n",
    "    \n",
    "    è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "    --------\n",
    "    é€šéè¨­è¨ˆæ¡æ¨£å’ŒåŠ æ¬Šæ–¹æ¡ˆä¾†ç³¾æ­£é‡ç–Šçµæœçš„ä¸ç•¶å½±éŸ¿ï¼š\n",
    "    1. è¨ˆç®—ä¸¦ç™¼åº¦ï¼ˆConcurrencyï¼‰ï¼šæ¯å€‹æ™‚é–“é»æœ‰å¤šå°‘æ¨™ç±¤åŒæ™‚å­˜æ´»\n",
    "    2. è¨ˆç®—å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰ï¼šæ¯å€‹æ¨™ç±¤çš„å¹³å‡å”¯ä¸€æ€§\n",
    "    3. æ‡‰ç”¨æ™‚é–“è¡°æ¸›ï¼ˆTime Decayï¼‰ï¼šè®“èˆŠæ¨£æœ¬æ¬Šé‡é™ä½\n",
    "    4. é¡åˆ¥å¹³è¡¡ï¼ˆClass Balanceï¼‰ï¼šå¹³è¡¡ä¸åŒé¡åˆ¥çš„æ¨£æœ¬æ¬Šé‡\n",
    "    5. çµ„åˆæ¬Šé‡ï¼šæœ€çµ‚æ¬Šé‡ = å”¯ä¸€æ€§ Ã— æ™‚é–“è¡°æ¸› Ã— é¡åˆ¥å¹³è¡¡ï¼ˆå¯é¸ï¼šÃ— å ±é…¬æ­¸å› ï¼‰\n",
    "    \n",
    "    ä¸¦ç™¼åº¦ï¼ˆConcurrencyï¼‰ï¼š\n",
    "    ---------------------\n",
    "    å®šç¾©ï¼šå…©å€‹æ¨™ç±¤ y_i å’Œ y_j åœ¨æ™‚é–“ t æ˜¯ä¸¦ç™¼çš„ï¼Œç•¶å®ƒå€‘éƒ½ä¾è³´è‡³å°‘ä¸€å€‹\n",
    "    å…±åŒçš„å ±é…¬ r_{t-1,t} = p_t / p_{t-1} - 1ã€‚\n",
    "    \n",
    "    è¨ˆç®—æ–¹å¼ï¼š\n",
    "    å°æ¯å€‹æ™‚é–“é» t = 1,...,Tï¼Œå½¢æˆäºŒå…ƒé™£åˆ— {1_{t,i}}_{i=1,...,I}ï¼š\n",
    "    - 1_{t,i} = 1ï¼šå¦‚æœ [t_i,0, t_i,1] èˆ‡ [t-1, t] é‡ç–Š\n",
    "    - 1_{t,i} = 0ï¼šå¦å‰‡\n",
    "    \n",
    "    ä¸¦ç™¼åº¦ï¼šc_t = Î£_{i=1}^I 1_{t,i}\n",
    "    \n",
    "    å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰ï¼š\n",
    "    --------------------\n",
    "    å®šç¾©ï¼šæ¨™ç±¤ i åœ¨æ™‚é–“ t çš„å”¯ä¸€æ€§ç‚º u_{t,i} = 1_{t,i} / c_t\n",
    "    \n",
    "    å¹³å‡å”¯ä¸€æ€§ï¼šÅ«_i = (Î£_{t=1}^T 1_{t,i})^{-1} Ã— (Î£_{t=1}^T u_{t,i})\n",
    "    \n",
    "    ä¹Ÿå¯ä»¥è§£é‡‹ç‚ºï¼šæ¨™ç±¤ i å­˜æ´»æœŸé–“çš„ä¸¦ç™¼åº¦å€’æ•¸çš„èª¿å’Œå¹³å‡ã€‚\n",
    "    \n",
    "    å ±é…¬æ­¸å› ï¼ˆReturn Attributionï¼‰ï¼š\n",
    "    -----------------------------\n",
    "    ç•¶æ¨™ç±¤æ˜¯å ±é…¬ç¬¦è™Ÿçš„å‡½æ•¸æ™‚ï¼ˆ{-1,1} æˆ– {0,1}ï¼‰ï¼Œæ¨£æœ¬æ¬Šé‡å¯ä»¥å®šç¾©ç‚ºï¼š\n",
    "    \n",
    "    Ìƒw_i = |Î£_{t=t_i,0}^{t_i,1} (r_{t-1,t} / c_t)|\n",
    "    \n",
    "    å…¶ä¸­ r_{t-1,t} æ˜¯å°æ•¸å ±é…¬ï¼Œc_t æ˜¯æ™‚é–“ t çš„ä¸¦ç™¼åº¦ã€‚\n",
    "    \n",
    "    ç„¶å¾Œæ¨™æº–åŒ–ï¼šw_i = Ìƒw_i Ã— I / (Î£_{j=1}^I Ìƒw_j)\n",
    "    \n",
    "    æ™‚é–“è¡°æ¸›ï¼ˆTime Decayï¼‰ï¼š\n",
    "    ----------------------\n",
    "    å¸‚å ´æ˜¯é©æ‡‰æ€§ç³»çµ±ï¼ŒèˆŠæ¨£æœ¬ä¸å¦‚æ–°æ¨£æœ¬ç›¸é—œã€‚\n",
    "    \n",
    "    ç·šæ€§æ™‚é–“è¡°æ¸›ï¼š\n",
    "    d[x] = max{0, a + bx}\n",
    "    \n",
    "    é‚Šç•Œæ¢ä»¶ï¼š\n",
    "    1. d[Î£_{i=1}^I Å«_i] = 1ï¼ˆæœ€æ–°æ¨£æœ¬æ¬Šé‡ç‚º 1ï¼‰\n",
    "    2. d[0] = cï¼ˆæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º cï¼Œc âˆˆ [0,1]ï¼‰\n",
    "    \n",
    "    åƒæ•¸ c çš„æ„ç¾©ï¼š\n",
    "    - c = 1ï¼šç„¡æ™‚é–“è¡°æ¸›\n",
    "    - 0 < c < 1ï¼šç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\n",
    "    - c = 0ï¼šæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "    - c < 0ï¼šæœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "    \n",
    "    æ³¨æ„ï¼šæ™‚é–“è¡°æ¸›æ˜¯åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ï¼Œè€Œéæ™‚é–“é †åºï¼Œå› ç‚ºåœ¨å­˜åœ¨å†—é¤˜\n",
    "    è§€æ¸¬å€¼çš„æƒ…æ³ä¸‹ï¼ŒæŒ‰æ™‚é–“é †åºè¡°æ¸›æœƒä½¿æ¬Šé‡é™ä½å¤ªå¿«ã€‚\n",
    "    \n",
    "    é¡åˆ¥å¹³è¡¡ï¼ˆClass Balanceï¼‰ï¼š\n",
    "    ------------------------\n",
    "    ç•¶æ¨™ç±¤åˆ†å¸ƒä¸å¹³è¡¡æ™‚ï¼ˆä¾‹å¦‚ï¼šä¸Šæ¼²æ¨™ç±¤å¤šæ–¼ä¸‹è·Œæ¨™ç±¤ï¼‰ï¼Œéœ€è¦å°å°‘æ•¸é¡åˆ¥\n",
    "    çµ¦äºˆæ›´é«˜çš„æ¬Šé‡ï¼Œä»¥å¹³è¡¡æ¨¡å‹è¨“ç·´ã€‚\n",
    "    \n",
    "    è¨ˆç®—æ–¹å¼ï¼š\n",
    "    w_class = n_samples / (n_classes Ã— n_class_samples)\n",
    "    \n",
    "    é€™æ¨£å¯ä»¥è®“å°‘æ•¸é¡åˆ¥çš„æ¨£æœ¬æ¬Šé‡æ›´é«˜ï¼Œå¹³è¡¡é¡åˆ¥åˆ†å¸ƒã€‚\n",
    "    \n",
    "    åƒè€ƒæ–‡ç»ï¼š\n",
    "    --------\n",
    "    - Advances in Financial Machine Learning, Chapter 4\n",
    "    - Snippet 4.1: mpNumCoEvents - è¨ˆç®—ä¸¦ç™¼åº¦\n",
    "    - Snippet 4.2: mpSampleTW - è¨ˆç®—å¹³å‡å”¯ä¸€æ€§\n",
    "    - Snippet 4.10: mpSampleW - å ±é…¬æ­¸å› æ¬Šé‡\n",
    "    - Snippet 4.11: getTimeDecay - æ™‚é–“è¡°æ¸›å› å­\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_co_events = None\n",
    "        self.uniqueness = None\n",
    "        self.time_decay = None\n",
    "        self.class_balance_weights = None\n",
    "        self.sample_weights = None\n",
    "        \n",
    "    def compute_num_co_events(self, close_idx: pd.DatetimeIndex, \n",
    "                              t1: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸ï¼ˆConcurrencyï¼‰\n",
    "        åŸºæ–¼ Snippet 4.1: mpNumCoEvents\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹æ™‚é–“é» tï¼Œè¨ˆç®—æœ‰å¤šå°‘å€‹äº‹ä»¶çš„å­˜æ´»æœŸé–“ [t_i,0, t_i,1] \n",
    "        èˆ‡æ™‚é–“é» t é‡ç–Šã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        c_t = Î£_{i=1}^I 1_{t,i}\n",
    "        \n",
    "        å…¶ä¸­ 1_{t,i} = 1 å¦‚æœ [t_i,0, t_i,1] èˆ‡ [t-1, t] é‡ç–Šï¼Œå¦å‰‡ç‚º 0ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        close_idx : pd.DatetimeIndex\n",
    "            åƒ¹æ ¼æ•¸æ“šçš„ç´¢å¼•\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        num_co_events : pd.Series\n",
    "            æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸\n",
    "        \"\"\"\n",
    "        t1 = t1.fillna(close_idx[-1])  # æœªçµæŸäº‹ä»¶ä»è¦è¨ˆç®—\n",
    "        \n",
    "        # åˆå§‹åŒ–è¨ˆæ•¸åºåˆ—\n",
    "        iloc = close_idx.searchsorted(np.array([t1.index[0], t1.max()]))\n",
    "        count = pd.Series(0, index=close_idx[iloc[0]:iloc[1]+1])\n",
    "        \n",
    "        # å°æ¯å€‹äº‹ä»¶ï¼Œåœ¨å…¶å­˜æ´»æœŸé–“ +1\n",
    "        for tIn, tOut in t1.items():\n",
    "            count.loc[tIn:tOut] += 1.0\n",
    "        \n",
    "        self.num_co_events = count\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š ä¸¦ç™¼åº¦çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½æ™‚é–“é»æ•¸: {len(count):,}\")\n",
    "        print(f\"å¹³å‡ä¸¦ç™¼äº‹ä»¶æ•¸: {count.mean():.2f}\")\n",
    "        print(f\"æœ€å¤§ä¸¦ç™¼äº‹ä»¶æ•¸: {count.max():.0f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - ä¸¦ç™¼åº¦è¡¡é‡æ¯å€‹æ™‚é–“é»æœ‰å¤šå°‘å€‹æ¨™ç±¤åŒæ™‚å­˜æ´»\")\n",
    "        print(f\"  - é«˜ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ä¹‹é–“é‡ç–Šå¤šï¼Œè³‡è¨Šå†—é¤˜\")\n",
    "        print(f\"  - ä½ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ç›¸å°ç¨ç«‹\")\n",
    "        print(f\"ä¸¦ç™¼åº¦åˆ†å¸ƒ:\")\n",
    "        print(count.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return count\n",
    "\n",
    "    def compute_uniqueness(self, t1: pd.Series, \n",
    "                          num_co_events: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¯å€‹äº‹ä»¶çš„å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰\n",
    "        åŸºæ–¼ Snippet 4.2: mpSampleTW\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ iï¼Œè¨ˆç®—å…¶åœ¨å­˜æ´»æœŸé–“çš„å¹³å‡å”¯ä¸€æ€§ï¼š\n",
    "        \n",
    "        u_{t,i} = 1_{t,i} / c_t  ï¼ˆæ™‚é–“ t çš„å”¯ä¸€æ€§ï¼‰\n",
    "        Å«_i = (1 / T_i) Ã— Î£_{t=t_i,0}^{t_i,1} u_{t,i}\n",
    "        \n",
    "        å…¶ä¸­ T_i æ˜¯äº‹ä»¶ i çš„å­˜æ´»æœŸæ•¸ã€‚\n",
    "        \n",
    "        ä¹Ÿå¯ä»¥è§£é‡‹ç‚ºï¼šäº‹ä»¶å­˜æ´»æœŸé–“çš„ä¸¦ç™¼åº¦å€’æ•¸çš„èª¿å’Œå¹³å‡ã€‚\n",
    "        \n",
    "        å…¬å¼ï¼š\n",
    "        Å«_i = mean(1 / c_t) for t âˆˆ [t_i,0, t_i,1]\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "        num_co_events : pd.Series\n",
    "            æ¯å€‹æ™‚é–“é»çš„ä¸¦ç™¼äº‹ä»¶æ•¸åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        uniqueness : pd.Series\n",
    "            æ¯å€‹äº‹ä»¶çš„å”¯ä¸€æ€§ï¼ˆ1 / å¹³å‡ä¸¦ç™¼åº¦ï¼‰\n",
    "        \"\"\"\n",
    "        wght = pd.Series(index=t1.index)\n",
    "        \n",
    "        for tIn, tOut in t1.items():\n",
    "            # è¨ˆç®—äº‹ä»¶å­˜æ´»æœŸé–“çš„å¹³å‡ä¸¦ç™¼åº¦å€’æ•¸\n",
    "            avg_co_events = num_co_events.loc[tIn:tOut].mean()\n",
    "            wght.loc[tIn] = 1.0 / avg_co_events if avg_co_events > 0 else 0\n",
    "        \n",
    "        self.uniqueness = wght\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å”¯ä¸€æ€§çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½äº‹ä»¶æ•¸: {len(wght):,}\")\n",
    "        print(f\"å¹³å‡å”¯ä¸€æ€§: {wght.mean():.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¡¡é‡æ¯å€‹æ¨™ç±¤çš„éé‡ç–Šç¨‹åº¦\")\n",
    "        print(f\"  - å”¯ä¸€æ€§ = 1 / å¹³å‡ä¸¦ç™¼åº¦\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¶Šé«˜ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤è¶Šç¨ç«‹ï¼Œæ¬Šé‡æ‡‰è©²è¶Šå¤§\")\n",
    "        print(f\"  - å”¯ä¸€æ€§è¶Šä½ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤èˆ‡å…¶ä»–æ¨™ç±¤é‡ç–Šå¤šï¼Œæ¬Šé‡æ‡‰è©²è¶Šå°\")\n",
    "        print(f\"å”¯ä¸€æ€§åˆ†å¸ƒ:\")\n",
    "        print(wght.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return wght\n",
    "\n",
    "    def compute_return_attribution(self, \n",
    "                                  close: pd.Series,\n",
    "                                  t1: pd.Series,\n",
    "                                  num_co_events: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆReturn Attributionï¼‰\n",
    "        åŸºæ–¼ Snippet 4.10: mpSampleW\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹äº‹ä»¶ï¼Œè¨ˆç®—å…¶å­˜æ´»æœŸé–“çš„æ­¸å› å ±é…¬ï¼š\n",
    "        \n",
    "        Ìƒw_i = |Î£_{t=t_i,0}^{t_i,1} (r_{t-1,t} / c_t)|\n",
    "        \n",
    "        å…¶ä¸­ï¼š\n",
    "        - r_{t-1,t} = log(close_t / close_{t-1}) æ˜¯å°æ•¸å ±é…¬\n",
    "        - c_t æ˜¯æ™‚é–“ t çš„ä¸¦ç™¼åº¦\n",
    "        \n",
    "        ç„¶å¾Œæ¨™æº–åŒ–ï¼šw_i = Ìƒw_i Ã— I / (Î£_{j=1}^I Ìƒw_j)\n",
    "        \n",
    "        é€™æ¨£è¨­è¨ˆçš„æ„ç¾©ï¼š\n",
    "        - å°‡å ±é…¬æŒ‰ä¸¦ç™¼åº¦åˆ†é…ï¼Œé¿å…é‡ç–Šäº‹ä»¶é‡è¤‡è¨ˆç®—å ±é…¬\n",
    "        - çµ•å°å ±é…¬å¤§çš„äº‹ä»¶æ¬Šé‡æ›´é«˜\n",
    "        - æ¨™æº–åŒ–å¾Œæ¬Šé‡ç¸½å’Œç‚º Iï¼ˆæ¨£æœ¬æ•¸ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        close : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“\n",
    "        num_co_events : pd.Series\n",
    "            ä¸¦ç™¼åº¦åºåˆ—\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        return_weights : pd.Series\n",
    "            å ±é…¬æ­¸å› æ¬Šé‡\n",
    "        \"\"\"\n",
    "        ret = np.log(close).diff()  # å°æ•¸å ±é…¬\n",
    "        wght = pd.Series(index=t1.index)\n",
    "        \n",
    "        for tIn, tOut in t1.items():\n",
    "            # è¨ˆç®—æ­¸å› å ±é…¬ï¼šå ±é…¬ / ä¸¦ç™¼åº¦\n",
    "            attributed_ret = (ret.loc[tIn:tOut] / \n",
    "                            num_co_events.loc[tIn:tOut]).sum()\n",
    "            wght.loc[tIn] = abs(attributed_ret)\n",
    "        \n",
    "        # æ¨™æº–åŒ–ï¼šæ¬Šé‡ç¸½å’Œ = æ¨£æœ¬æ•¸\n",
    "        wght = wght * len(wght) / wght.sum()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å ±é…¬æ­¸å› æ¬Šé‡çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½äº‹ä»¶æ•¸: {len(wght):,}\")\n",
    "        print(f\"å¹³å‡æ¬Šé‡: {wght.mean():.4f}\")\n",
    "        print(f\"æ¬Šé‡ç¸½å’Œ: {wght.sum():.0f} (æ‡‰ç­‰æ–¼æ¨£æœ¬æ•¸)\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - æ ¹æ“šæ­¸å› çš„çµ•å°å ±é…¬ä¾†åŠ æ¬Š\")\n",
    "        print(f\"  - å ±é…¬æŒ‰ä¸¦ç™¼åº¦åˆ†é…ï¼Œé¿å…é‡ç–Šäº‹ä»¶é‡è¤‡è¨ˆç®—\")\n",
    "        print(f\"  - çµ•å°å ±é…¬å¤§çš„äº‹ä»¶æ¬Šé‡æ›´é«˜\")\n",
    "        print(f\"æ¬Šé‡åˆ†å¸ƒ:\")\n",
    "        print(wght.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return wght\n",
    "\n",
    "    def compute_class_balance_weight(self, labels: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        å°æ¯å€‹é¡åˆ¥ï¼Œè¨ˆç®—å…¶æ¬Šé‡ç‚ºï¼š\n",
    "        w_class = n_samples / (n_classes Ã— n_class_samples)\n",
    "        \n",
    "        é€™æ¨£å¯ä»¥è®“å°‘æ•¸é¡åˆ¥çš„æ¨£æœ¬æ¬Šé‡æ›´é«˜ï¼Œå¹³è¡¡é¡åˆ¥åˆ†å¸ƒã€‚\n",
    "        \n",
    "        ä½¿ç”¨ sklearn çš„ compute_sample_weight('balanced', labels) å¯¦ç¾ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        labels : pd.Series\n",
    "            æ¨™ç±¤åºåˆ—ï¼ˆindex å¿…é ˆèˆ‡æ¨£æœ¬æ¬Šé‡çš„ index ä¸€è‡´ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        class_weights : pd.Series\n",
    "            æ¯å€‹æ¨£æœ¬çš„é¡åˆ¥å¹³è¡¡æ¬Šé‡\n",
    "        \"\"\"\n",
    "        from sklearn.utils.class_weight import compute_sample_weight\n",
    "        \n",
    "        # è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡\n",
    "        class_weights = compute_sample_weight('balanced', labels)\n",
    "        class_weights_series = pd.Series(class_weights, index=labels.index)\n",
    "        \n",
    "        self.class_balance_weights = class_weights_series\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š é¡åˆ¥å¹³è¡¡æ¬Šé‡çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "        label_counts = labels.value_counts().sort_index()\n",
    "        print(label_counts)\n",
    "        print(f\"\\né¡åˆ¥å¹³è¡¡æ¬Šé‡:\")\n",
    "        for label in sorted(labels.unique()):\n",
    "            mask = labels == label\n",
    "            avg_weight = class_weights_series[mask].mean()\n",
    "            count = mask.sum()\n",
    "            pct = count / len(labels) * 100\n",
    "            print(f\"  æ¨™ç±¤ {label}: æ¨£æœ¬æ•¸ = {count:,} ({pct:.2f}%), å¹³å‡æ¬Šé‡ = {avg_weight:.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        print(f\"  - é¡åˆ¥å¹³è¡¡æ¬Šé‡ç”¨æ–¼å¹³è¡¡ä¸åŒé¡åˆ¥çš„æ¨£æœ¬åˆ†å¸ƒ\")\n",
    "        print(f\"  - å°‘æ•¸é¡åˆ¥çš„æ¨£æœ¬æœƒç²å¾—æ›´é«˜çš„æ¬Šé‡\")\n",
    "        print(f\"  - å¤šæ•¸é¡åˆ¥çš„æ¨£æœ¬æœƒç²å¾—è¼ƒä½çš„æ¬Šé‡\")\n",
    "        print(f\"  - æ¬Šé‡è¨ˆç®—å…¬å¼: w = n_samples / (n_classes Ã— n_class_samples)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return class_weights_series\n",
    "\n",
    "    def get_time_decay_linear(self, tW: pd.Series, \n",
    "                              clf_last_w: float = 1.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        ç·šæ€§æ™‚é–“è¡°æ¸›ï¼ˆåŸºæ–¼ Snippet 4.11: getTimeDecayï¼‰\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        æ‡‰ç”¨åˆ†æ®µç·šæ€§è¡°æ¸›åˆ°è§€æ¸¬çš„å”¯ä¸€æ€§ï¼ˆtWï¼‰ï¼š\n",
    "        \n",
    "        d[x] = max{0, a + bx}\n",
    "        \n",
    "        é‚Šç•Œæ¢ä»¶ï¼š\n",
    "        1. d[Î£_{i=1}^I Å«_i] = 1  ï¼ˆæœ€æ–°æ¨£æœ¬æ¬Šé‡ç‚º 1ï¼‰\n",
    "        2. d[0] = clf_last_w    ï¼ˆæœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º clf_last_wï¼‰\n",
    "        \n",
    "        æ±‚è§£ï¼š\n",
    "        a = 1 - b Ã— Î£_{i=1}^I Å«_i\n",
    "        b = (1 - clf_last_w) / Î£_{i=1}^I Å«_i  ï¼ˆç•¶ clf_last_w >= 0ï¼‰\n",
    "        \n",
    "        æ³¨æ„ï¼šæ™‚é–“è¡°æ¸›æ˜¯åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ x âˆˆ [0, Î£_{i=1}^I Å«_i]ï¼Œ\n",
    "        è€Œéæ™‚é–“é †åºï¼Œå› ç‚ºåœ¨å­˜åœ¨å†—é¤˜è§€æ¸¬å€¼çš„æƒ…æ³ä¸‹ï¼ŒæŒ‰æ™‚é–“é †åº\n",
    "        è¡°æ¸›æœƒä½¿æ¬Šé‡é™ä½å¤ªå¿«ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        tW : pd.Series\n",
    "            å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé€šå¸¸æ˜¯ uniquenessï¼‰\n",
    "        clf_last_w : float\n",
    "            æœ€èˆŠæ¨£æœ¬çš„æ¬Šé‡ï¼ˆé è¨­ 1.0ï¼Œè¡¨ç¤ºä¸è¡°æ¸›ï¼‰\n",
    "            - c = 1: ç„¡æ™‚é–“è¡°æ¸›\n",
    "            - 0 < c < 1: ç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\n",
    "            - c = 0: æœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "            - c < 0: æœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        time_decay : pd.Series\n",
    "            æ™‚é–“è¡°æ¸›æ¬Šé‡\n",
    "        \"\"\"\n",
    "        clfW = tW.sort_index().cumsum()\n",
    "        \n",
    "        if clfW.iloc[-1] > 0:\n",
    "            if clf_last_w >= 0:\n",
    "                # ç·šæ€§è¡°æ¸›ï¼šd[0] = clf_last_w, d[Î£Å«] = 1\n",
    "                slope = (1.0 - clf_last_w) / clfW.iloc[-1]\n",
    "            else:\n",
    "                # ç•¶ clf_last_w < 0 æ™‚çš„ç‰¹æ®Šè™•ç†\n",
    "                slope = 1 / ((clf_last_w + 1) * clfW.iloc[-1])\n",
    "        else:\n",
    "            slope = 0\n",
    "        \n",
    "        const = 1.0 - slope * clfW.iloc[-1]\n",
    "        clfW = const + slope * clfW\n",
    "        clfW[clfW < 0] = 0\n",
    "        \n",
    "        self.time_decay = clfW\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š ç·šæ€§æ™‚é–“è¡°æ¸›\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"æ™‚é–“è¡°æ¸›åƒæ•¸: const={const:.4f}, slope={slope:.6f}\")\n",
    "        print(f\"æœ€èˆŠæ¨£æœ¬æ¬Šé‡ (c): {clf_last_w:.4f}\")\n",
    "        print(f\"æœ€æ–°æ¨£æœ¬æ¬Šé‡: 1.0\")\n",
    "        print(f\"å¹³å‡æ™‚é–“è¡°æ¸›æ¬Šé‡: {clfW.mean():.4f}\")\n",
    "        print(f\"\\nèªªæ˜:\")\n",
    "        if clf_last_w == 1.0:\n",
    "            print(f\"  - c = 1: ç„¡æ™‚é–“è¡°æ¸›\")\n",
    "        elif 0 < clf_last_w < 1:\n",
    "            print(f\"  - 0 < c < 1: ç·šæ€§è¡°æ¸›ï¼Œæ‰€æœ‰æ¨£æœ¬æ¬Šé‡ > 0\")\n",
    "        elif clf_last_w == 0:\n",
    "            print(f\"  - c = 0: æœ€èˆŠæ¨£æœ¬æ¬Šé‡ç‚º 0\")\n",
    "        elif clf_last_w < 0:\n",
    "            print(f\"  - c < 0: æœ€èˆŠçš„ |c|Ã—T éƒ¨åˆ†æ¨£æœ¬æ¬Šé‡ç‚º 0\")\n",
    "        print(f\"  - è¡°æ¸›åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ï¼Œè€Œéæ™‚é–“é †åº\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return clfW\n",
    "    \n",
    "    def get_time_decay_exp(self, tW: pd.Series, \n",
    "                          decay_rate: float = 1.0,\n",
    "                          percent_of_zero_wts: float = 0.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        æŒ‡æ•¸æ™‚é–“è¡°æ¸›\n",
    "        \n",
    "        å¯¦ç¾åŸç†ï¼š\n",
    "        ---------\n",
    "        ä½¿ç”¨æŒ‡æ•¸å‡½æ•¸é€²è¡Œæ™‚é–“è¡°æ¸›ï¼š\n",
    "        \n",
    "        d[x] = exp((decay_rate - 1) Ã— (Î£Å« - x))\n",
    "        \n",
    "        å…¶ä¸­ x æ˜¯ç´¯ç©å”¯ä¸€æ€§ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        tW : pd.Series\n",
    "            å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé€šå¸¸æ˜¯ uniquenessï¼‰\n",
    "        decay_rate : float\n",
    "            è¡°æ¸›ç‡ï¼ˆ>1 è¡¨ç¤ºè¡°æ¸›æ›´å¿«ï¼Œ<1 è¡¨ç¤ºè¡°æ¸›æ›´æ…¢ï¼‰\n",
    "        percent_of_zero_wts : float\n",
    "            æœ€èˆŠæ¨£æœ¬ä¸­è¨­ç‚º 0 çš„æ¯”ä¾‹\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        time_decay : pd.Series\n",
    "            æ™‚é–“è¡°æ¸›æ¬Šé‡\n",
    "        \"\"\"\n",
    "        clf_w = tW.sort_index().cumsum()\n",
    "        last_value = clf_w.iloc[-1]\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ•¸è¡°æ¸›æ¬Šé‡\n",
    "        out_wts = []\n",
    "        zero_threshold = int(round(len(clf_w) * percent_of_zero_wts))\n",
    "        \n",
    "        for i in range(len(clf_w)):\n",
    "            if i < zero_threshold:\n",
    "                out_wts.append(0.0)\n",
    "            else:\n",
    "                # æŒ‡æ•¸è¡°æ¸›ï¼šexp((decay_rate - 1) * (last_value - current_value))\n",
    "                decay_factor = np.exp((decay_rate - 1.0) * (last_value - clf_w.iloc[i]))\n",
    "                out_wts.append(decay_factor)\n",
    "        \n",
    "        time_decay = pd.Series(out_wts, index=clf_w.index)\n",
    "        self.time_decay = time_decay\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æŒ‡æ•¸æ™‚é–“è¡°æ¸›çµ±è¨ˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"è¡°æ¸›ç‡ (decay_rate): {decay_rate}\")\n",
    "        print(f\"é›¶æ¬Šé‡æ¨£æœ¬æ¯”ä¾‹: {percent_of_zero_wts:.2%}\")\n",
    "        print(f\"å¹³å‡æ™‚é–“è¡°æ¸›æ¬Šé‡: {time_decay.mean():.4f}\")\n",
    "        print(f\"æ™‚é–“è¡°æ¸›åˆ†å¸ƒ:\")\n",
    "        print(time_decay.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return time_decay\n",
    "    \n",
    "    def compute_sample_weights(self, \n",
    "                              t1: pd.Series,\n",
    "                              close_idx: pd.DatetimeIndex,\n",
    "                              labels: Optional[pd.Series] = None,\n",
    "                              close: Optional[pd.Series] = None,\n",
    "                              use_uniqueness: bool = True,\n",
    "                              use_time_decay: bool = True,\n",
    "                              use_return_attribution: bool = False,\n",
    "                              use_class_balance: bool = False,\n",
    "                              time_decay_type: str = 'linear',\n",
    "                              time_decay_params: dict = None) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æœ€çµ‚æ¨£æœ¬æ¬Šé‡ï¼ˆå¯é¸æ“‡ä½¿ç”¨å“ªäº›æ©Ÿåˆ¶ï¼‰\n",
    "        \n",
    "        æ¬Šé‡çµ„åˆæ–¹å¼ï¼š\n",
    "        ------------\n",
    "        1. åŸºç¤æ¬Šé‡ = å”¯ä¸€æ€§ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        2. å¯é¸ï¼šÃ— å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        3. å¯é¸ï¼šÃ— æ™‚é–“è¡°æ¸›ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        4. å¯é¸ï¼šÃ— é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "        \n",
    "        æœ€çµ‚æ¬Šé‡ = uniqueness Ã— (return_attribution) Ã— (time_decay) Ã— (class_balance)\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        t1 : pd.Series\n",
    "            äº‹ä»¶çµæŸæ™‚é–“ï¼ˆindex ç‚ºäº‹ä»¶é–‹å§‹æ™‚é–“ï¼‰\n",
    "        close_idx : pd.DatetimeIndex\n",
    "            åƒ¹æ ¼æ•¸æ“šçš„ç´¢å¼•\n",
    "        labels : pd.Series, optional\n",
    "            æ¨™ç±¤åºåˆ—ï¼ˆç”¨æ–¼è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼Œindex å¿…é ˆèˆ‡ t1 ä¸€è‡´ï¼‰\n",
    "        close : pd.Series, optional\n",
    "            åƒ¹æ ¼åºåˆ—ï¼ˆç”¨æ–¼è¨ˆç®—å ±é…¬æ­¸å› ï¼‰\n",
    "        use_uniqueness : bool\n",
    "            æ˜¯å¦ä½¿ç”¨å”¯ä¸€æ€§æ¬Šé‡ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_time_decay : bool\n",
    "            æ˜¯å¦ä½¿ç”¨æ™‚é–“è¡°æ¸›ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_return_attribution : bool\n",
    "            æ˜¯å¦ä½¿ç”¨å ±é…¬æ­¸å› æ¬Šé‡ï¼ˆé è¨­ Falseï¼‰\n",
    "        use_class_balance : bool\n",
    "            æ˜¯å¦ä½¿ç”¨é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼ˆé è¨­ Falseï¼‰\n",
    "        time_decay_type : str\n",
    "            æ™‚é–“è¡°æ¸›é¡å‹ï¼š'linear' æˆ– 'exp'ï¼ˆé è¨­ 'linear'ï¼‰\n",
    "        time_decay_params : dict, optional\n",
    "            æ™‚é–“è¡°æ¸›åƒæ•¸\n",
    "            - linear: {'clf_last_w': 0.5}\n",
    "            - exp: {'decay_rate': 1.2, 'percent_of_zero_wts': 0.0}\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        sample_weights : pd.Series\n",
    "            æœ€çµ‚æ¨£æœ¬æ¬Šé‡ï¼ˆå¦‚æœéƒ½ä¸ä½¿ç”¨ï¼Œè¿”å›å…¨ç‚º 1 çš„æ¬Šé‡ï¼‰\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡è¨ˆç®—é…ç½®\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ä½¿ç”¨å”¯ä¸€æ€§: {use_uniqueness}\")\n",
    "        print(f\"ä½¿ç”¨å ±é…¬æ­¸å› : {use_return_attribution}\")\n",
    "        print(f\"ä½¿ç”¨æ™‚é–“è¡°æ¸›: {use_time_decay}\")\n",
    "        print(f\"ä½¿ç”¨é¡åˆ¥å¹³è¡¡: {use_class_balance}\")\n",
    "        if use_time_decay:\n",
    "            print(f\"æ™‚é–“è¡°æ¸›é¡å‹: {time_decay_type}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # å¦‚æœéƒ½ä¸ä½¿ç”¨ï¼Œè¿”å›å‡å‹»æ¬Šé‡\n",
    "        if not use_uniqueness and not use_time_decay and not use_return_attribution and not use_class_balance:\n",
    "            sample_weights = pd.Series(1.0, index=t1.index)\n",
    "            self.sample_weights = sample_weights\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡çµ±è¨ˆï¼ˆå‡å‹»æ¬Šé‡ï¼‰\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"ç¸½æ¨£æœ¬æ•¸: {len(sample_weights):,}\")\n",
    "            print(f\"æ‰€æœ‰æ¨£æœ¬æ¬Šé‡: 1.0\")\n",
    "            print(\"=\" * 60)\n",
    "            return sample_weights\n",
    "        \n",
    "        # è¨ˆç®—ä¸¦ç™¼åº¦ï¼ˆå”¯ä¸€æ€§å’Œå ±é…¬æ­¸å› éƒ½éœ€è¦ï¼‰\n",
    "        if use_uniqueness or use_return_attribution:\n",
    "            num_co_events = self.compute_num_co_events(close_idx, t1)\n",
    "        else:\n",
    "            num_co_events = None\n",
    "        \n",
    "        # è¨ˆç®—å”¯ä¸€æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_uniqueness:\n",
    "            uniqueness = self.compute_uniqueness(t1, num_co_events)\n",
    "        else:\n",
    "            uniqueness = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨å”¯ä¸€æ€§ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—å ±é…¬æ­¸å› ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_return_attribution:\n",
    "            if close is None:\n",
    "                raise ValueError(\"ä½¿ç”¨å ±é…¬æ­¸å› æ™‚éœ€è¦æä¾› close åƒæ•¸\")\n",
    "            return_weights = self.compute_return_attribution(close, t1, num_co_events)\n",
    "        else:\n",
    "            return_weights = pd.Series(1.0, index=t1.index)\n",
    "        \n",
    "        # è¨ˆç®—æ™‚é–“è¡°æ¸›ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_time_decay:\n",
    "            if time_decay_params is None:\n",
    "                time_decay_params = {}\n",
    "            \n",
    "            if time_decay_type == 'linear':\n",
    "                clf_last_w = time_decay_params.get('clf_last_w', 1.0)\n",
    "                time_decay = self.get_time_decay_linear(uniqueness, clf_last_w)\n",
    "            elif time_decay_type == 'exp':\n",
    "                decay_rate = time_decay_params.get('decay_rate', 1.0)\n",
    "                percent_of_zero_wts = time_decay_params.get('percent_of_zero_wts', 0.0)\n",
    "                time_decay = self.get_time_decay_exp(uniqueness, decay_rate, percent_of_zero_wts)\n",
    "            else:\n",
    "                raise ValueError(f\"ä¸æ”¯æ´çš„æ™‚é–“è¡°æ¸›é¡å‹: {time_decay_type}\")\n",
    "        else:\n",
    "            time_decay = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨æ™‚é–“è¡°æ¸›ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—é¡åˆ¥å¹³è¡¡æ¬Šé‡ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "        if use_class_balance:\n",
    "            if labels is None:\n",
    "                raise ValueError(\"ä½¿ç”¨é¡åˆ¥å¹³è¡¡æ™‚éœ€è¦æä¾› labels åƒæ•¸\")\n",
    "            # ç¢ºä¿ labels çš„ index èˆ‡ t1 ä¸€è‡´\n",
    "            if not labels.index.equals(t1.index):\n",
    "                # å˜—è©¦å°é½Š\n",
    "                common_idx = labels.index.intersection(t1.index)\n",
    "                if len(common_idx) == 0:\n",
    "                    raise ValueError(\"labels å’Œ t1 çš„ index æ²’æœ‰äº¤é›†\")\n",
    "                labels = labels.loc[common_idx]\n",
    "                t1 = t1.loc[common_idx]\n",
    "                uniqueness = uniqueness.loc[common_idx]\n",
    "                return_weights = return_weights.loc[common_idx]\n",
    "                time_decay = time_decay.loc[common_idx]\n",
    "            class_balance_weights = self.compute_class_balance_weight(labels)\n",
    "        else:\n",
    "            class_balance_weights = pd.Series(1.0, index=t1.index)\n",
    "            print(\"âš ï¸ æœªä½¿ç”¨é¡åˆ¥å¹³è¡¡ï¼Œè¨­ç‚º 1.0\")\n",
    "        \n",
    "        # è¨ˆç®—æœ€çµ‚æ¬Šé‡ï¼ˆæ‰€æœ‰æ¬Šé‡ç›¸ä¹˜ï¼‰\n",
    "        common_idx = (uniqueness.index\n",
    "                     .intersection(time_decay.index)\n",
    "                     .intersection(return_weights.index)\n",
    "                     .intersection(class_balance_weights.index))\n",
    "        \n",
    "        sample_weights = (uniqueness.loc[common_idx] * \n",
    "                          return_weights.loc[common_idx] * \n",
    "                          time_decay.loc[common_idx] *\n",
    "                          class_balance_weights.loc[common_idx])\n",
    "        \n",
    "        self.sample_weights = sample_weights\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š æ¨£æœ¬æ¬Šé‡çµ±è¨ˆï¼ˆæœ€çµ‚ï¼‰\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½æ¨£æœ¬æ•¸: {len(sample_weights):,}\")\n",
    "        print(f\"å¹³å‡æ¬Šé‡: {sample_weights.mean():.4f}\")\n",
    "        print(f\"æ¬Šé‡åˆ†å¸ƒ:\")\n",
    "        print(sample_weights.describe())\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return sample_weights\n",
    "    \n",
    "    def plot_concurrency_vs_volatility(self, daily_vol: pd.Series):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ä¸¦ç™¼åº¦ vs æ³¢å‹•ç‡çš„é—œä¿‚åœ–\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        daily_vol : pd.Series\n",
    "            æ—¥æ³¢å‹•ç‡åºåˆ—\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        if self.num_co_events is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_num_co_events()\")\n",
    "            return\n",
    "        \n",
    "        # å°é½Šæ•¸æ“šï¼ˆä½¿ç”¨åŸå§‹æ™‚é–“é»ï¼‰\n",
    "        to_plot = pd.DataFrame({\n",
    "            'conc_events': self.num_co_events,\n",
    "            'daily_vol': daily_vol\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(to_plot) == 0:\n",
    "            print(\"âš ï¸ å°é½Šå¾Œç„¡æœ‰æ•ˆæ•¸æ“š\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # 1. æ™‚é–“åºåˆ—åœ–\n",
    "        ax1 = axes[0]\n",
    "        ax1_twin = ax1.twinx()\n",
    "        \n",
    "        line1 = ax1.plot(to_plot.index, to_plot['conc_events'], \n",
    "                        label='Concurrent Events', color='blue', linewidth=1.5)\n",
    "        line2 = ax1_twin.plot(to_plot.index, to_plot['daily_vol'], \n",
    "                            label='Daily Volatility', color='red', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        ax1.set_xlabel('Time', fontsize=12)\n",
    "        ax1.set_ylabel('Concurrent Events', color='blue', fontsize=12)\n",
    "        ax1_twin.set_ylabel('Daily Volatility', color='red', fontsize=12)\n",
    "        ax1.set_title('Concurrent Events vs Daily Volatility', fontsize=14)\n",
    "        ax1.tick_params(axis='y', labelcolor='blue')\n",
    "        ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1_twin.legend(loc='upper right')\n",
    "        \n",
    "        # 2. æ•£é»åœ–\n",
    "        sns.regplot(x=to_plot['conc_events'], y=to_plot['daily_vol'], \n",
    "                ax=axes[1], scatter_kws={'alpha': 0.6, 's': 50})\n",
    "        axes[1].set_xlabel('Concurrent Events', fontsize=12)\n",
    "        axes[1].set_ylabel('Daily Volatility', fontsize=12)\n",
    "        axes[1].set_title('Concurrent Events vs Daily Volatility (Scatter Plot)', fontsize=14)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # æ‰“å°ç›¸é—œæ€§\n",
    "        correlation = to_plot['conc_events'].corr(to_plot['daily_vol'])\n",
    "        print(f\"\\nğŸ“Š ä¸¦ç™¼åº¦èˆ‡æ³¢å‹•ç‡çš„ç›¸é—œä¿‚æ•¸: {correlation:.4f}\")\n",
    "    \n",
    "    def plot_weights_distribution(self):\n",
    "        \"\"\"ç¹ªè£½æ¬Šé‡åˆ†å¸ƒåœ–\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if self.sample_weights is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_sample_weights()\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. å”¯ä¸€æ€§åˆ†å¸ƒ\n",
    "        if self.uniqueness is not None:\n",
    "            self.uniqueness.hist(bins=50, ax=axes[0, 0], alpha=0.7, color='blue')\n",
    "            axes[0, 0].set_title('Uniqueness Distribution')\n",
    "            axes[0, 0].set_xlabel('Uniqueness')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. æ™‚é–“è¡°æ¸›åˆ†å¸ƒ\n",
    "        if self.time_decay is not None:\n",
    "            self.time_decay.hist(bins=50, ax=axes[0, 1], alpha=0.7, color='green')\n",
    "            axes[0, 1].set_title('Time Decay Distribution')\n",
    "            axes[0, 1].set_xlabel('Time Decay Weight')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. æ¨£æœ¬æ¬Šé‡åˆ†å¸ƒ\n",
    "        self.sample_weights.hist(bins=50, ax=axes[1, 0], alpha=0.7, color='orange')\n",
    "        axes[1, 0].set_title('Sample Weights Distribution')\n",
    "        axes[1, 0].set_xlabel('Sample Weight')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. æ¬Šé‡éš¨æ™‚é–“è®ŠåŒ–\n",
    "        if len(self.sample_weights) > 0:\n",
    "            sorted_weights = self.sample_weights.sort_index()\n",
    "            axes[1, 1].plot(sorted_weights.index, sorted_weights.values, \n",
    "                           linewidth=1, alpha=0.7, color='purple')\n",
    "            axes[1, 1].set_title('Sample Weights Over Time')\n",
    "            axes[1, 1].set_xlabel('Time')\n",
    "            axes[1, 1].set_ylabel('Sample Weight')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_uniqueness_autocorr(self, lag: int = 1) -> float:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å”¯ä¸€æ€§çš„åºåˆ—ç›¸é—œæ€§ï¼ˆAR(1)ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        lag : int\n",
    "            æ»¯å¾ŒæœŸæ•¸ï¼ˆé è¨­ 1ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        autocorr : float\n",
    "            åºåˆ—ç›¸é—œæ€§\n",
    "        \"\"\"\n",
    "        if self.uniqueness is None:\n",
    "            print(\"âš ï¸ è«‹å…ˆåŸ·è¡Œ compute_uniqueness()\")\n",
    "            return None\n",
    "        \n",
    "        autocorr = self.uniqueness.autocorr(lag=lag)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š å”¯ä¸€æ€§åºåˆ—ç›¸é—œæ€§\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"AR({lag}) è‡ªç›¸é—œä¿‚æ•¸: {autocorr:.4f}\")\n",
    "        \n",
    "        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "        n = len(self.uniqueness)\n",
    "        critical_value = 1.96 / np.sqrt(n)  # 95% ä¿¡å¿ƒæ°´æº–\n",
    "        \n",
    "        if abs(autocorr) > critical_value:\n",
    "            print(f\"âœ… çµ±è¨ˆé¡¯è‘— (|{autocorr:.4f}| > {critical_value:.4f})\")\n",
    "            print(f\"\\nè§£é‡‹:\")\n",
    "            print(f\"  - å”¯ä¸€æ€§å­˜åœ¨åºåˆ—ç›¸é—œæ€§ï¼Œè¡¨ç¤ºå¸‚å ´ç‹€æ…‹å…·æœ‰æŒçºŒæ€§\")\n",
    "            print(f\"  - é€™åœ¨é‡‘èæ•¸æ“šä¸­æ˜¯å¸¸è¦‹ä¸”é æœŸçš„ç¾è±¡\")\n",
    "            print(f\"  - æ™‚é–“è¡°æ¸›æ©Ÿåˆ¶æœ‰åŠ©æ–¼é™ä½èˆŠæ•¸æ“šçš„å½±éŸ¿\")\n",
    "        else:\n",
    "            print(f\"âŒ çµ±è¨ˆä¸é¡¯è‘— (|{autocorr:.4f}| <= {critical_value:.4f})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return autocorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d68cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# å®Œæ•´ä»£ç¢¼ï¼šç‚ºæ‰€æœ‰å•†å“ç”¢ç”Ÿåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# 2. ä¿®æ­£å¾Œçš„ FractionalDiff classï¼ˆç¢ºä¿ä¿ç•™æ‰€æœ‰ indexï¼‰\n",
    "# =====================================================\n",
    "\n",
    "class FractionalDiff:\n",
    "    \"\"\"åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimal_d = None\n",
    "        self.fracdiff_series = None\n",
    "        self.features = None\n",
    "        \n",
    "    def get_weights(self, d: float, size: int) -> np.ndarray:\n",
    "        \"\"\"è¨ˆç®—æ¬Šé‡ï¼ˆæ“´å±•çª—å£ï¼‰\"\"\"\n",
    "        w = [1.0]\n",
    "        for k in range(1, size):\n",
    "            w_ = -w[-1] / k * (d - k + 1)\n",
    "            w.append(float(w_))\n",
    "        return np.array(w[::-1]).reshape(-1, 1)\n",
    "    \n",
    "    def get_weights_FFD(self, d: float, thres: float = 1e-5) -> np.ndarray:\n",
    "        \"\"\"è¨ˆç®— FFD æ¬Šé‡ï¼ˆå›ºå®šçª—å£ï¼‰\"\"\"\n",
    "        w = [1.0]\n",
    "        k = 1\n",
    "        while True:\n",
    "            w_ = -w[-1] / k * (d - k + 1)\n",
    "            if abs(w_) < thres:\n",
    "                break\n",
    "            w.append(float(w_))\n",
    "            k += 1\n",
    "            if k > 10000:\n",
    "                break\n",
    "        return np.array(w[::-1]).reshape(-1, 1)\n",
    "    \n",
    "    def frac_diff_FFD(self, series: pd.Series, d: float, thres: float = 1e-5) -> pd.Series:\n",
    "        \"\"\"\n",
    "        åˆ†æ•¸éšå·®åˆ†ï¼ˆå›ºå®šçª—å£æ–¹æ³•ï¼ŒFFDï¼‰\n",
    "        \n",
    "        é‡è¦ï¼šä¿ç•™æ‰€æœ‰åŸå§‹ indexï¼Œåªåœ¨å‰ width å€‹é»è¨­ç‚º NaN\n",
    "        \"\"\"\n",
    "        # 1. è¨ˆç®—æ¬Šé‡ï¼ˆå›ºå®šçª—å£ï¼‰\n",
    "        w = self.get_weights_FFD(d, thres)\n",
    "        width = len(w) - 1\n",
    "        \n",
    "        if width < 1:\n",
    "            return pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        # 2. æ‡‰ç”¨æ¬Šé‡åˆ°æ•¸å€¼\n",
    "        # é‡è¦ï¼šå…ˆå¡«å…… NaNï¼Œä½†ä¿ç•™åŸå§‹ index\n",
    "        series_filled = series.fillna(method='ffill')\n",
    "        \n",
    "        # å»ºç«‹çµæœ Seriesï¼Œä½¿ç”¨åŸå§‹ series çš„å®Œæ•´ index\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        # 3. å¾ width é–‹å§‹è¨ˆç®—ï¼ˆå‰ width å€‹é»æœƒæ˜¯ NaNï¼‰\n",
    "        valid_count = 0\n",
    "        for iloc1 in range(width, len(series_filled)):\n",
    "            if iloc1 >= len(series.index):\n",
    "                break\n",
    "                \n",
    "            loc0 = series_filled.index[iloc1 - width]\n",
    "            loc1 = series_filled.index[iloc1]\n",
    "            \n",
    "            # æª¢æŸ¥åŸå§‹ series åœ¨ loc1 æ˜¯å¦æœ‰æœ‰æ•ˆå€¼\n",
    "            if loc1 not in series.index or not np.isfinite(series.loc[loc1]):\n",
    "                continue\n",
    "            \n",
    "            # ä½¿ç”¨å›ºå®šçª—å£ï¼š[X_{t-width}, X_{t-width+1}, ..., X_t]\n",
    "            window_data = series_filled.loc[loc0:loc1].values\n",
    "            \n",
    "            if len(window_data) == len(w):\n",
    "                result = np.dot(w.flatten(), window_data)\n",
    "                df_[loc1] = result\n",
    "                valid_count += 1\n",
    "        \n",
    "        return df_\n",
    "    \n",
    "    def frac_diff(self, series: pd.Series, d: float, thres: float = 0.01) -> pd.Series:\n",
    "        \"\"\"åˆ†æ•¸éšå·®åˆ†ï¼ˆæ“´å±•çª—å£ï¼‰\"\"\"\n",
    "        w = self.get_weights(d, series.shape[0])\n",
    "        w_ = np.cumsum(np.abs(w))\n",
    "        w_ /= w_[-1]\n",
    "        skip = w_[w_ > thres].shape[0]\n",
    "        \n",
    "        series_filled = series.fillna(method='ffill')\n",
    "        df_ = pd.Series(index=series.index, dtype=float)\n",
    "        \n",
    "        for iloc in range(skip, series_filled.shape[0]):\n",
    "            loc = series_filled.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc]):\n",
    "                continue\n",
    "            \n",
    "            window_data = series_filled.loc[:loc].values\n",
    "            w_subset = w[-(iloc + 1):, :].flatten()\n",
    "            \n",
    "            if len(window_data) == len(w_subset):\n",
    "                df_[loc] = np.dot(w_subset, window_data)\n",
    "        \n",
    "        return df_\n",
    "    \n",
    "    def find_optimal_d(self, series: pd.Series, \n",
    "                      d_range: Tuple[float, float] = (0, 1),\n",
    "                      d_step: float = 0.05,\n",
    "                      method: str = 'FFD',\n",
    "                      thres: float = 1e-5,\n",
    "                      target_pvalue: float = 0.05,\n",
    "                      min_corr: float = 0.5) -> dict:\n",
    "        \"\"\"æ‰¾å‡ºæœ€é©åˆçš„ d å€¼\"\"\"\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        from scipy.stats import jarque_bera\n",
    "        \n",
    "        results = []\n",
    "        d_values = np.arange(d_range[0], d_range[1] + d_step, d_step)\n",
    "        \n",
    "        for d in d_values:\n",
    "            try:\n",
    "                if method == 'FFD':\n",
    "                    fracdiff_series = self.frac_diff_FFD(series, d, thres).dropna()\n",
    "                else:\n",
    "                    fracdiff_series = self.frac_diff(series, d, thres).dropna()\n",
    "                \n",
    "                if len(fracdiff_series) < 10:\n",
    "                    continue\n",
    "                \n",
    "                adf_result = adfuller(fracdiff_series, maxlag=1, regression='c', autolag=None)\n",
    "                adf_stat = adf_result[0]\n",
    "                p_value = adf_result[1]\n",
    "                \n",
    "                common_idx = series.index.intersection(fracdiff_series.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    corr = np.corrcoef(\n",
    "                        series.loc[common_idx].values,\n",
    "                        fracdiff_series.loc[common_idx].values\n",
    "                    )[0, 1]\n",
    "                else:\n",
    "                    corr = 0\n",
    "                \n",
    "                jb_stat, jb_pvalue = jarque_bera(fracdiff_series)[:2]\n",
    "                \n",
    "                results.append({\n",
    "                    'd': d,\n",
    "                    'adf_stat': adf_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'correlation': corr,\n",
    "                    'jb_stat': jb_stat,\n",
    "                    'jb_pvalue': jb_pvalue,\n",
    "                    'is_stationary': p_value < target_pvalue,\n",
    "                    'meets_corr': corr >= min_corr,\n",
    "                    'sample_size': len(fracdiff_series)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            return None\n",
    "        \n",
    "        valid_results = results_df[\n",
    "            (results_df['is_stationary']) & \n",
    "            (results_df['meets_corr'])\n",
    "        ]\n",
    "        \n",
    "        if len(valid_results) > 0:\n",
    "            optimal = valid_results.loc[valid_results['p_value'].idxmin()]\n",
    "            self.optimal_d = optimal['d']\n",
    "        else:\n",
    "            optimal = results_df.loc[results_df['p_value'].idxmin()]\n",
    "            self.optimal_d = optimal['d']\n",
    "        \n",
    "        return {\n",
    "            'optimal_d': self.optimal_d,\n",
    "            'results': results_df,\n",
    "            'optimal_stats': optimal.to_dict()\n",
    "        }\n",
    "\n",
    "    def find_min_d_adf(self, series: pd.Series,\n",
    "                    d_range: Tuple[float, float] = (0, 1),\n",
    "                    n_points: int = 11,\n",
    "                    method: str = 'FFD',\n",
    "                    thres: float = 0.01,\n",
    "                    target_pvalue: float = 0.1) -> dict:\n",
    "        \"\"\"\n",
    "        å°‹æ‰¾æœ€å° d å€¼ï¼ˆåŸºæ–¼ Snippet 5.4ï¼‰\n",
    "        \n",
    "        é€™å€‹æ–¹æ³•èˆ‡ find_optimal_d é¡ä¼¼ï¼Œä½†å°ˆé–€ç”¨æ–¼å°‹æ‰¾æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„æœ€å° d å€¼ã€‚\n",
    "        é€™æ˜¯ AFML æ›¸ä¸­ Snippet 5.4 çš„å¯¦ç¾ã€‚\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        series : pd.Series\n",
    "            åƒ¹æ ¼åºåˆ—\n",
    "        d_range : tuple\n",
    "            d å€¼æœå°‹ç¯„åœ (min, max)\n",
    "        n_points : int\n",
    "            æ¸¬è©¦çš„ d å€¼æ•¸é‡ï¼ˆå°æ‡‰ np.linspace(d_range[0], d_range[1], n_points)ï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼ï¼ˆFFD æ–¹æ³•ï¼‰æˆ–ç´¯ç©æ¬Šé‡é–¾å€¼ï¼ˆexpanding æ–¹æ³•ï¼‰\n",
    "        target_pvalue : float\n",
    "            ç›®æ¨™ p-valueï¼ˆADF æª¢é©—ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        result : dict\n",
    "            åŒ…å«ä»¥ä¸‹éµå€¼ï¼š\n",
    "            - 'min_d': æœ€å° d å€¼ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰\n",
    "            - 'results': DataFrameï¼ŒåŒ…å«æ‰€æœ‰æ¸¬è©¦çµæœ\n",
    "            - 'min_d_stats': æœ€å° d å€¼çš„çµ±è¨ˆè³‡è¨Š\n",
    "        \"\"\"\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        \n",
    "        results = []\n",
    "        d_values = np.linspace(d_range[0], d_range[1], n_points)\n",
    "        \n",
    "        print(f\"ğŸ” æœå°‹æœ€å° d å€¼ (ç¯„åœ: {d_range}, æ¸¬è©¦é»æ•¸: {n_points})\")\n",
    "        print(f\"   æ–¹æ³•: {method}, é–¾å€¼: {thres}, ç›®æ¨™ p-value: {target_pvalue}\")\n",
    "        \n",
    "        for i, d in enumerate(d_values):\n",
    "            try:\n",
    "                # è¨ˆç®—åˆ†æ•¸éšå·®åˆ†\n",
    "                if method == 'FFD':\n",
    "                    fracdiff_series = self.frac_diff_FFD(series, d, thres).dropna()\n",
    "                else:\n",
    "                    fracdiff_series = self.frac_diff(series, d, thres).dropna()\n",
    "                \n",
    "                if len(fracdiff_series) < 10:\n",
    "                    continue\n",
    "                \n",
    "                # ADF æª¢é©—\n",
    "                adf_result = adfuller(fracdiff_series, maxlag=1, regression='c', autolag=None)\n",
    "                adf_stat = adf_result[0]\n",
    "                p_value = adf_result[1]\n",
    "                \n",
    "                # è¨ˆç®—èˆ‡åŸå§‹åºåˆ—çš„ç›¸é—œæ€§\n",
    "                common_idx = series.index.intersection(fracdiff_series.index)\n",
    "                if len(common_idx) > 0:\n",
    "                    corr = np.corrcoef(\n",
    "                        series.loc[common_idx].values,\n",
    "                        fracdiff_series.loc[common_idx].values\n",
    "                    )[0, 1]\n",
    "                else:\n",
    "                    corr = 0\n",
    "                \n",
    "                is_stationary = p_value < target_pvalue\n",
    "                \n",
    "                results.append({\n",
    "                    'd': d,\n",
    "                    'adf_stat': adf_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'correlation': corr,\n",
    "                    'is_stationary': is_stationary,\n",
    "                    'sample_size': len(fracdiff_series)\n",
    "                })\n",
    "                \n",
    "                if (i + 1) % 5 == 0 or i == len(d_values) - 1:\n",
    "                    print(f\"   é€²åº¦: {i+1}/{len(d_values)} (d={d:.3f}, p={p_value:.4f}, stationary={is_stationary})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçµæœ\")\n",
    "            return None\n",
    "        \n",
    "        # å°‹æ‰¾æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„æœ€å° d å€¼\n",
    "        stationary_results = results_df[results_df['is_stationary'] == True]\n",
    "        \n",
    "        if len(stationary_results) > 0:\n",
    "            min_d_row = stationary_results.loc[stationary_results['d'].idxmin()]\n",
    "            min_d = min_d_row['d']\n",
    "            self.optimal_d = min_d\n",
    "            \n",
    "            print(f\"\\nâœ… æ‰¾åˆ°æœ€å° d = {min_d:.4f}\")\n",
    "            print(f\"   ADF p-value: {min_d_row['p_value']:.4f}\")\n",
    "            print(f\"   ç›¸é—œæ€§: {min_d_row['correlation']:.4f}\")\n",
    "        else:\n",
    "            # å¦‚æœæ²’æœ‰æ»¿è¶³æ¢ä»¶çš„ï¼Œé¸æ“‡ p-value æœ€å°çš„\n",
    "            min_d_row = results_df.loc[results_df['p_value'].idxmin()]\n",
    "            min_d = min_d_row['d']\n",
    "            self.optimal_d = min_d\n",
    "            \n",
    "            print(f\"\\nâš ï¸ æœªæ‰¾åˆ°æ»¿è¶³å¹³ç©©æ€§æ¢ä»¶çš„ d å€¼\")\n",
    "            print(f\"   ä½¿ç”¨ p-value æœ€å°çš„ d = {min_d:.4f}\")\n",
    "            print(f\"   ADF p-value: {min_d_row['p_value']:.4f} (ç›®æ¨™: < {target_pvalue})\")\n",
    "        \n",
    "        return {\n",
    "            'min_d': min_d,\n",
    "            'results': results_df,\n",
    "            'min_d_stats': min_d_row.to_dict()\n",
    "        }\n",
    "\n",
    "    def _plot_d_search_results(self, result: dict):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ d å€¼æœå°‹çµæœ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        result : dict\n",
    "            find_min_d_adf æˆ– find_optimal_d çš„è¿”å›çµæœ\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if result is None or 'results' not in result:\n",
    "            print(\"âš ï¸ ç„¡æ•ˆçš„çµæœ\")\n",
    "            return\n",
    "        \n",
    "        results_df = result['results']\n",
    "        min_d = result.get('min_d') or result.get('optimal_d')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. ADF p-value vs d\n",
    "        axes[0, 0].plot(results_df['d'], results_df['p_value'], 'o-', linewidth=2, markersize=6)\n",
    "        if min_d is not None:\n",
    "            min_d_pvalue = results_df[results_df['d'] == min_d]['p_value'].values[0]\n",
    "            axes[0, 0].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[0, 0].scatter([min_d], [min_d_pvalue], color='red', s=100, zorder=5)\n",
    "        axes[0, 0].axhline(0.05, color='gray', linestyle=':', alpha=0.5, label='p=0.05')\n",
    "        axes[0, 0].axhline(0.1, color='gray', linestyle=':', alpha=0.5, label='p=0.1')\n",
    "        axes[0, 0].set_xlabel('d', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('ADF p-value', fontsize=12)\n",
    "        axes[0, 0].set_title('ADF Test p-value vs d', fontsize=14)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. ADF Statistic vs d\n",
    "        axes[0, 1].plot(results_df['d'], results_df['adf_stat'], 'o-', linewidth=2, markersize=6, color='green')\n",
    "        if min_d is not None:\n",
    "            min_d_stat = results_df[results_df['d'] == min_d]['adf_stat'].values[0]\n",
    "            axes[0, 1].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[0, 1].scatter([min_d], [min_d_stat], color='red', s=100, zorder=5)\n",
    "        axes[0, 1].axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "        axes[0, 1].set_xlabel('d', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('ADF Statistic', fontsize=12)\n",
    "        axes[0, 1].set_title('ADF Test Statistic vs d', fontsize=14)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Correlation vs d\n",
    "        axes[1, 0].plot(results_df['d'], results_df['correlation'], 'o-', linewidth=2, markersize=6, color='orange')\n",
    "        if min_d is not None:\n",
    "            min_d_corr = results_df[results_df['d'] == min_d]['correlation'].values[0]\n",
    "            axes[1, 0].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[1, 0].scatter([min_d], [min_d_corr], color='red', s=100, zorder=5)\n",
    "        axes[1, 0].set_xlabel('d', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Correlation with Original', fontsize=12)\n",
    "        axes[1, 0].set_title('Correlation vs d', fontsize=14)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Stationarity Status\n",
    "        if 'is_stationary' in results_df.columns:\n",
    "            colors = ['red' if not stat else 'green' for stat in results_df['is_stationary']]\n",
    "            axes[1, 1].scatter(results_df['d'], results_df['is_stationary'].astype(int), \n",
    "                            c=colors, s=100, alpha=0.6)\n",
    "            if min_d is not None:\n",
    "                axes[1, 1].axvline(min_d, color='red', linestyle='--', linewidth=2, label=f'Min d = {min_d:.3f}')\n",
    "            axes[1, 1].set_xlabel('d', fontsize=12)\n",
    "            axes[1, 1].set_ylabel('Is Stationary (1=Yes, 0=No)', fontsize=12)\n",
    "            axes[1, 1].set_title('Stationarity Status vs d', fontsize=14)\n",
    "            axes[1, 1].set_ylim(-0.1, 1.1)\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No stationarity data', \n",
    "                            ha='center', va='center', fontsize=12)\n",
    "            axes[1, 1].set_title('Stationarity Status vs d', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_min_d_results(self, result: dict, d: float):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½ä½¿ç”¨æœ€å° d å€¼å¾Œçš„çµæœ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        result : dict\n",
    "            find_min_d_adf çš„è¿”å›çµæœ\n",
    "        d : float\n",
    "            ä½¿ç”¨çš„ d å€¼\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"âš ï¸ ç„¡æ•ˆçš„çµæœ\")\n",
    "            return\n",
    "        \n",
    "        # é€™å€‹æ–¹æ³•å¯ä»¥ç¹ªè£½ä½¿ç”¨ d å€¼å¾Œçš„æ™‚é–“åºåˆ—å°æ¯”\n",
    "        # ä½†éœ€è¦å¯¦éš›çš„åˆ†æ•¸éšå·®åˆ†åºåˆ—ï¼Œæ‰€ä»¥é€™è£¡åªé¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "        min_d_stats = result.get('min_d_stats', {})\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æœ€å° d å€¼çµ±è¨ˆè³‡è¨Š\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ä½¿ç”¨çš„ d å€¼: {d:.4f}\")\n",
    "        if min_d_stats:\n",
    "            print(f\"ADF Statistic: {min_d_stats.get('adf_stat', 'N/A'):.4f}\")\n",
    "            print(f\"ADF p-value: {min_d_stats.get('p_value', 'N/A'):.4f}\")\n",
    "            print(f\"èˆ‡åŸå§‹åºåˆ—ç›¸é—œæ€§: {min_d_stats.get('correlation', 'N/A'):.4f}\")\n",
    "            print(f\"æ¨£æœ¬æ•¸: {min_d_stats.get('sample_size', 'N/A'):,}\")\n",
    "            print(f\"æ˜¯å¦å¹³ç©©: {min_d_stats.get('is_stationary', 'N/A')}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    def generate_features(self, df: pd.DataFrame, event_indices: pd.DatetimeIndex,\n",
    "                      price_col: str = 'Close', d: Optional[float] = None,\n",
    "                      method: str = 'FFD', thres: float = 1e-5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç‚ºäº‹ä»¶é»ç”Ÿæˆåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            åŒ…å«åƒ¹æ ¼è³‡æ–™çš„ DataFrame\n",
    "        event_indices : pd.DatetimeIndex\n",
    "            äº‹ä»¶æ™‚é–“é»\n",
    "        price_col : str\n",
    "            åƒ¹æ ¼æ¬„ä½åç¨±\n",
    "        d : float, optional\n",
    "            åˆ†æ•¸éšå·®åˆ†éšæ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ self.optimal_dï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        features : pd.DataFrame\n",
    "            åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µï¼ˆindex ç‚º event_indicesï¼‰\n",
    "        \"\"\"\n",
    "        if price_col not in df.columns:\n",
    "            raise ValueError(f\"æ‰¾ä¸åˆ°åƒ¹æ ¼æ¬„ä½: {price_col}\")\n",
    "        \n",
    "        # ä½¿ç”¨æŒ‡å®šçš„ d å€¼æˆ–æœ€å„ª d å€¼\n",
    "        if d is None:\n",
    "            if self.optimal_d is None:\n",
    "                raise ValueError(\"è«‹å…ˆåŸ·è¡Œ find_optimal_d() æˆ– find_min_d_adf()ï¼Œæˆ–æŒ‡å®š d åƒæ•¸\")\n",
    "            d = self.optimal_d\n",
    "        \n",
    "        print(f\"ğŸ“Š ç”Ÿæˆåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ (d={d:.4f}, method={method})\")\n",
    "        \n",
    "        # è¨ˆç®—æ•´å€‹åºåˆ—çš„åˆ†æ•¸éšå·®åˆ†\n",
    "        price_series = df[price_col]\n",
    "        \n",
    "        if method == 'FFD':\n",
    "            fracdiff_series = self.frac_diff_FFD(price_series, d, thres)\n",
    "        else:\n",
    "            fracdiff_series = self.frac_diff(price_series, d, thres)\n",
    "        \n",
    "        # æå–äº‹ä»¶é»çš„ç‰¹å¾µå€¼\n",
    "        features = pd.DataFrame(index=event_indices)\n",
    "        features['fracdiff'] = fracdiff_series.loc[event_indices].values\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_count = features['fracdiff'].notna().sum()\n",
    "        print(f\"âœ… ç‰¹å¾µç”Ÿæˆå®Œæˆ\")\n",
    "        print(f\"   äº‹ä»¶æ•¸: {len(event_indices):,}\")\n",
    "        print(f\"   æœ‰æ•ˆç‰¹å¾µæ•¸: {valid_count:,}\")\n",
    "        print(f\"   NaN æ•¸: {len(event_indices) - valid_count:,}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def plot_price_vs_fracdiff(self, price_series: pd.Series, \n",
    "                            d: Optional[float] = None,\n",
    "                            method: str = 'FFD',\n",
    "                            thres: float = 1e-5,\n",
    "                            n_points: Optional[int] = None,\n",
    "                            normalize: bool = False):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½åŸå§‹åƒ¹æ ¼èˆ‡åˆ†æ•¸éšå·®åˆ†ç‰¹å¾µçš„å°æ¯”åœ–\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        price_series : pd.Series\n",
    "            åŸå§‹åƒ¹æ ¼åºåˆ—\n",
    "        d : float, optional\n",
    "            åˆ†æ•¸éšå·®åˆ†éšæ•¸ï¼ˆå¦‚æœç‚º Noneï¼Œä½¿ç”¨ self.optimal_d æˆ–é è¨­å€¼ 0.5ï¼‰\n",
    "        method : str\n",
    "            æ–¹æ³•ï¼š'FFD' æˆ– 'expanding'\n",
    "        thres : float\n",
    "            æ¬Šé‡æˆªæ–·é–¾å€¼\n",
    "        n_points : int, optional\n",
    "            å¦‚æœæŒ‡å®šï¼Œåªç¹ªè£½æœ€è¿‘ n_points å€‹é»ï¼ˆç”¨æ–¼å¤§é‡æ•¸æ“šæ™‚ï¼‰\n",
    "        normalize : bool\n",
    "            æ˜¯å¦æ¨™æº–åŒ–å¾Œç¹ªè£½åœ¨åŒä¸€å¼µåœ–ä¸Šï¼ˆé è¨­ Falseï¼Œåˆ†é–‹å…©å€‹å­åœ–ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fracdiff_series : pd.Series\n",
    "            è¨ˆç®—å‡ºçš„åˆ†æ•¸éšå·®åˆ†åºåˆ—\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # ç¢ºå®šä½¿ç”¨çš„ d å€¼\n",
    "        if d is None:\n",
    "            if self.optimal_d is not None:\n",
    "                d_value = self.optimal_d\n",
    "            else:\n",
    "                d_value = 0.5\n",
    "                print(f\"âš ï¸ æœªæŒ‡å®š d å€¼ï¼Œä½¿ç”¨é è¨­å€¼ {d_value}\")\n",
    "        else:\n",
    "            d_value = d\n",
    "        \n",
    "        # è¨ˆç®—æ•´å€‹åºåˆ—çš„åˆ†æ•¸éšå·®åˆ†\n",
    "        print(f\"ğŸ“Š è¨ˆç®—åˆ†æ•¸éšå·®åˆ† (d={d_value:.4f}, method={method})...\")\n",
    "        if method == 'FFD':\n",
    "            fracdiff_series = self.frac_diff_FFD(price_series, d_value, thres)\n",
    "        else:\n",
    "            fracdiff_series = self.frac_diff(price_series, d_value, thres)\n",
    "        \n",
    "        # é¸æ“‡è¦ç¹ªè£½çš„æ•¸æ“šç¯„åœ\n",
    "        if n_points is not None and len(price_series) > n_points:\n",
    "            plot_idx = price_series.index[-n_points:]\n",
    "            price_plot = price_series.loc[plot_idx]\n",
    "            fracdiff_plot = fracdiff_series.loc[plot_idx]\n",
    "            title_suffix = f\" (Last {n_points:,} Points)\"\n",
    "        else:\n",
    "            plot_idx = price_series.index\n",
    "            price_plot = price_series\n",
    "            fracdiff_plot = fracdiff_series\n",
    "            title_suffix = \"\"\n",
    "        \n",
    "        # ç¹ªè£½åœ–è¡¨\n",
    "        if normalize:\n",
    "            # æ–¹æ³• 1: æ¨™æº–åŒ–å¾Œç¹ªè£½åœ¨åŒä¸€å¼µåœ–ä¸Š\n",
    "            common_idx = plot_idx.intersection(fracdiff_plot.dropna().index)\n",
    "            if len(common_idx) > 0:\n",
    "                price_normalized = (price_plot.loc[common_idx] - price_plot.loc[common_idx].mean()) / price_plot.loc[common_idx].std()\n",
    "                fracdiff_normalized = (fracdiff_plot.loc[common_idx] - fracdiff_plot.loc[common_idx].mean()) / fracdiff_plot.loc[common_idx].std()\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(15, 6))\n",
    "                \n",
    "                ax.plot(common_idx, price_normalized, \n",
    "                    label='Original Price (Normalized)', linewidth=1.5, color='blue', alpha=0.7)\n",
    "                ax.plot(common_idx, fracdiff_normalized, \n",
    "                    label='Fractional Differentiation (Normalized)', linewidth=1.5, color='red', alpha=0.7)\n",
    "                ax.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "                ax.set_xlabel('Time', fontsize=12)\n",
    "                ax.set_ylabel('Normalized Value', fontsize=12)\n",
    "                ax.set_title(f'Normalized Comparison: Price vs Fractional Differentiation{title_suffix}\\n(d={d_value:.4f}, method={method})', fontsize=14)\n",
    "                ax.legend(loc='upper left')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            # æ–¹æ³• 2: åˆ†é–‹å…©å€‹å­åœ–ï¼ˆé è¨­ï¼‰\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "            \n",
    "            # ä¸Šåœ–ï¼šåŸå§‹åƒ¹æ ¼\n",
    "            axes[0].plot(plot_idx, price_plot.values, \n",
    "                        label='Original Price', linewidth=1.5, color='blue', alpha=0.7)\n",
    "            axes[0].set_ylabel('Price', fontsize=12)\n",
    "            axes[0].set_title(f'Original Price vs Fractional Differentiation{title_suffix}\\n(d={d_value:.4f}, method={method})', fontsize=14)\n",
    "            axes[0].legend(loc='upper left')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # ä¸‹åœ–ï¼šåˆ†æ•¸éšå·®åˆ†ç‰¹å¾µ\n",
    "            valid_fracdiff = fracdiff_plot.dropna()\n",
    "            if len(valid_fracdiff) > 0:\n",
    "                axes[1].plot(valid_fracdiff.index, valid_fracdiff.values, \n",
    "                            label='Fractional Differentiation', linewidth=1.5, color='red', alpha=0.7)\n",
    "            axes[1].axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "            axes[1].set_xlabel('Time', fontsize=12)\n",
    "            axes[1].set_ylabel('Fractional Differentiation', fontsize=12)\n",
    "            axes[1].set_title('Fractional Differentiation Feature', fontsize=14)\n",
    "            axes[1].legend(loc='upper left')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        valid_count = fracdiff_series.notna().sum()\n",
    "        print(f\"âœ… ç¹ªåœ–å®Œæˆ\")\n",
    "        print(f\"   åŸå§‹æ•¸æ“šé»æ•¸: {len(price_series):,}\")\n",
    "        print(f\"   åˆ†æ•¸éšå·®åˆ†æœ‰æ•ˆå€¼: {valid_count:,}\")\n",
    "        print(f\"   NaN å€¼: {len(price_series) - valid_count:,}\")\n",
    "        \n",
    "        return fracdiff_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201b53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ç‰¹å¾µç¯©é¸ Pipeline Class\n",
    "# =====================================================\n",
    "\n",
    "class FeatureFilteringPipeline:\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ç‰¹å¾µç¯©é¸ Pipeline\n",
    "    \n",
    "    åŒ…å«ï¼š\n",
    "    1. Correlation Filteringï¼ˆç›¸é—œæ€§éæ¿¾ï¼‰\n",
    "    2. Variance Thresholdï¼ˆä½è®Šç•°å‰”é™¤ï¼‰\n",
    "    3. IC (Information Coefficient) Ranking\n",
    "    4. ANOVA / t-test / Ï‡Â² çµ±è¨ˆæª¢å®š\n",
    "    5. MDA (Permutation Importance)\n",
    "    6. Stability Selectionï¼ˆå¯é¸ï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 corr_threshold=0.95,\n",
    "                 variance_threshold=0.0,\n",
    "                 ic_threshold=None,\n",
    "                 anova_p_threshold=0.05,\n",
    "                 mda_top_n=None,\n",
    "                 stability_n_iter=50,\n",
    "                 stability_threshold=0.6):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        corr_threshold : float\n",
    "            ç›¸é—œæ€§é–¾å€¼ï¼Œè¶…éæ­¤å€¼è¦–ç‚ºé«˜åº¦ç›¸é—œï¼ˆé è¨­ 0.95ï¼‰\n",
    "        variance_threshold : float\n",
    "            è®Šç•°æ•¸é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼è¦–ç‚ºä½è®Šç•°ï¼ˆé è¨­ 0.0ï¼‰\n",
    "        ic_threshold : float or None\n",
    "            IC é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼è¦–ç‚ºç„¡é æ¸¬åŠ›ï¼ˆNone è¡¨ç¤ºä¿ç•™ top Nï¼‰\n",
    "        anova_p_threshold : float\n",
    "            ANOVA p-value é–¾å€¼ï¼ˆé è¨­ 0.05ï¼‰\n",
    "        mda_top_n : int or None\n",
    "            MDA ä¿ç•™å‰ N å€‹ç‰¹å¾µï¼ˆNone è¡¨ç¤ºä¿ç•™æ‰€æœ‰é€šéæª¢å®šçš„ï¼‰\n",
    "        stability_n_iter : int\n",
    "            Stability Selection è¿­ä»£æ¬¡æ•¸ï¼ˆé è¨­ 50ï¼‰\n",
    "        stability_threshold : float\n",
    "            Stability Selection é¸æ“‡é »ç‡é–¾å€¼ï¼ˆé è¨­ 0.6ï¼‰\n",
    "        \"\"\"\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.ic_threshold = ic_threshold\n",
    "        self.anova_p_threshold = anova_p_threshold\n",
    "        self.mda_top_n = mda_top_n\n",
    "        self.stability_n_iter = stability_n_iter\n",
    "        self.stability_threshold = stability_threshold\n",
    "        \n",
    "        self.selected_features = None\n",
    "        self.filter_results = {}\n",
    "        \n",
    "    def correlation_filter(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        1ï¸âƒ£ Correlation Filteringï¼ˆç›¸é—œæ€§éæ¿¾ï¼‰\n",
    "        \n",
    "        ç§»é™¤é«˜åº¦ç›¸é—œçš„ç‰¹å¾µï¼ˆ|Ï| > thresholdï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"1ï¸âƒ£ Correlation Filtering\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        corr_matrix = X.corr().abs()\n",
    "        \n",
    "        # æ‰¾å‡ºé«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°\n",
    "        upper_triangle = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        \n",
    "        # æ‰¾å‡ºéœ€è¦ç§»é™¤çš„ç‰¹å¾µ\n",
    "        to_remove = set()\n",
    "        for col in upper_triangle.columns:\n",
    "            high_corr = upper_triangle.index[upper_triangle[col] > self.corr_threshold]\n",
    "            if len(high_corr) > 0:\n",
    "                # ä¿ç•™èˆ‡å…¶ä»–ç‰¹å¾µç›¸é—œæ€§è¼ƒé«˜çš„ï¼ˆä¿ç•™æ›´é‡è¦çš„ï¼‰\n",
    "                # ç°¡å–®ç­–ç•¥ï¼šä¿ç•™ç¬¬ä¸€å€‹ï¼Œç§»é™¤å…¶ä»–çš„\n",
    "                to_remove.update(high_corr[1:])\n",
    "        \n",
    "        removed_count = len(to_remove)\n",
    "        remaining_features = [col for col in X.columns if col not in to_remove]\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤é«˜åº¦ç›¸é—œç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(remaining_features):,}\")\n",
    "        print(f\"   ç›¸é—œæ€§é–¾å€¼: {self.corr_threshold}\")\n",
    "        \n",
    "        self.filter_results['correlation'] = {\n",
    "            'removed': list(to_remove),\n",
    "            'remaining': remaining_features,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[remaining_features]\n",
    "    \n",
    "    def variance_filter(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        2ï¸âƒ£ Variance Thresholdï¼ˆä½è®Šç•°å‰”é™¤ï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"2ï¸âƒ£ Variance Threshold\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        selector = VarianceThreshold(threshold=self.variance_threshold)\n",
    "        X_selected = selector.fit_transform(X)\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "        removed_count = len(X.columns) - len(selected_features)\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä½è®Šç•°ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected_features):,}\")\n",
    "        print(f\"   è®Šç•°æ•¸é–¾å€¼: {self.variance_threshold}\")\n",
    "        \n",
    "        self.filter_results['variance'] = {\n",
    "            'removed': [col for col in X.columns if col not in selected_features],\n",
    "            'remaining': list(selected_features),\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(X_selected, index=X.index, columns=selected_features)\n",
    "    \n",
    "    def ic_ranking(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        3ï¸âƒ£ IC (Information Coefficient) Ranking\n",
    "        \n",
    "        è¨ˆç®—æ¯å€‹ç‰¹å¾µèˆ‡æ¨™ç±¤çš„ Rank Correlation\n",
    "        ä½¿ç”¨ |IC| ä¾†åˆ¤æ–·é æ¸¬èƒ½åŠ›ï¼Œæ­£è²  IC éƒ½æœ‰é æ¸¬åƒ¹å€¼\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"3ï¸âƒ£ IC (Information Coefficient) Ranking\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        ic_scores = {}\n",
    "        \n",
    "        for col in X.columns:\n",
    "            try:\n",
    "                # è¨ˆç®— Spearman rank correlation\n",
    "                ic = stats.spearmanr(X[col], y, nan_policy='omit')[0]\n",
    "                if np.isnan(ic):\n",
    "                    ic = 0.0\n",
    "                ic_scores[col] = ic\n",
    "            except:\n",
    "                ic_scores[col] = 0.0\n",
    "        \n",
    "        ic_series = pd.Series(ic_scores)\n",
    "        ic_series = ic_series.sort_values(ascending=False)\n",
    "        \n",
    "        # æ ¹æ“š |IC| é–¾å€¼æˆ–ä¿ç•™ top Nï¼ˆä½¿ç”¨çµ•å°å€¼ï¼‰\n",
    "        if self.ic_threshold is not None:\n",
    "            # ä½¿ç”¨çµ•å°å€¼åˆ¤æ–·é æ¸¬èƒ½åŠ›\n",
    "            selected = ic_series[ic_series.abs() >= self.ic_threshold].index.tolist()\n",
    "        else:\n",
    "            # ä¿ç•™ |IC| å‰ 50% çš„ç‰¹å¾µï¼ˆæŒ‰çµ•å°å€¼æ’åºï¼‰\n",
    "            n_keep = max(1, int(len(ic_series) * 0.5))\n",
    "            selected = ic_series.abs().nlargest(n_keep).index.tolist()\n",
    "        \n",
    "        removed_count = len(X.columns) - len(selected)\n",
    "        \n",
    "        # çµ±è¨ˆè³‡è¨Š\n",
    "        positive_ic = ic_series[ic_series > 0]\n",
    "        negative_ic = ic_series[ic_series < 0]\n",
    "        abs_ic = ic_series.abs()\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä½ |IC| ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected):,}\")\n",
    "        print(f\"   IC é–¾å€¼: |IC| >= {self.ic_threshold if self.ic_threshold is not None else 'Top 50%'}\")\n",
    "        print(f\"\\n   IC çµ±è¨ˆ:\")\n",
    "        print(f\"     å¹³å‡ IC: {ic_series.mean():.4f}\")\n",
    "        print(f\"     å¹³å‡ |IC|: {abs_ic.mean():.4f}\")\n",
    "        print(f\"     ä¸­ä½æ•¸ |IC|: {abs_ic.median():.4f}\")\n",
    "        print(f\"     æœ€å¤§ IC: {ic_series.max():.4f}\")\n",
    "        print(f\"     æœ€å° IC: {ic_series.min():.4f}\")\n",
    "        print(f\"     æœ€å¤§ |IC|: {abs_ic.max():.4f}\")\n",
    "        print(f\"\\n   IC åˆ†å¸ƒ:\")\n",
    "        print(f\"     æ­£ IC ç‰¹å¾µæ•¸: {len(positive_ic):,} (å¹³å‡: {positive_ic.mean():.4f})\")\n",
    "        print(f\"     è²  IC ç‰¹å¾µæ•¸: {len(negative_ic):,} (å¹³å‡: {negative_ic.mean():.4f})\")\n",
    "        print(f\"     é›¶ IC ç‰¹å¾µæ•¸: {len(ic_series[ic_series == 0]):,}\")\n",
    "        \n",
    "        # é¡¯ç¤ºç¯©é¸å¾Œçš„ç‰¹å¾µï¼ˆæŒ‰ |IC| æ’åºï¼‰\n",
    "        selected_ic = ic_series[selected].abs().sort_values(ascending=False)\n",
    "        print(f\"\\n   Top 10 ç¯©é¸å¾Œç‰¹å¾µ (æŒ‰ |IC| æ’åº):\")\n",
    "        for i, (feat, ic_val) in enumerate(selected_ic.head(10).items(), 1):\n",
    "            original_ic = ic_series[feat]\n",
    "            direction = \"æ­£ç›¸é—œ\" if original_ic > 0 else \"è² ç›¸é—œ\"\n",
    "            print(f\"     {i:2d}. {feat}: |IC|={ic_val:.4f} (IC={original_ic:.4f}, {direction})\")\n",
    "        \n",
    "        self.filter_results['ic'] = {\n",
    "            'scores': ic_series.to_dict(),\n",
    "            'abs_scores': abs_ic.to_dict(),\n",
    "            'selected': selected,\n",
    "            'removed_count': removed_count,\n",
    "            'positive_count': len(positive_ic),\n",
    "            'negative_count': len(negative_ic)\n",
    "        }\n",
    "        \n",
    "        return X[selected]\n",
    "    \n",
    "    def anova_filter(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        4ï¸âƒ£ ANOVA / t-test / Ï‡Â² çµ±è¨ˆæª¢å®š\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"4ï¸âƒ£ ANOVA / t-test / Ï‡Â² Statistical Tests\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        selected_features = []\n",
    "        p_values = {}\n",
    "        \n",
    "        for col in X.columns:\n",
    "            try:\n",
    "                # æ ¹æ“šæ¨™ç±¤åˆ†çµ„\n",
    "                groups = [X[col][y == label].dropna() for label in y.unique()]\n",
    "                groups = [g for g in groups if len(g) > 0]\n",
    "                \n",
    "                if len(groups) < 2:\n",
    "                    p_values[col] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                # ä½¿ç”¨ ANOVA F-test\n",
    "                f_stat, p_val = stats.f_oneway(*groups)\n",
    "                p_values[col] = p_val\n",
    "                \n",
    "                if p_val < self.anova_p_threshold:\n",
    "                    selected_features.append(col)\n",
    "            except:\n",
    "                p_values[col] = 1.0\n",
    "        \n",
    "        removed_count = len(X.columns) - len(selected_features)\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤é«˜ p-value ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected_features):,}\")\n",
    "        print(f\"   p-value é–¾å€¼: {self.anova_p_threshold}\")\n",
    "        \n",
    "        # é¡¯ç¤ºçµ±è¨ˆé¡¯è‘—çš„ç‰¹å¾µ\n",
    "        significant = pd.Series(p_values)\n",
    "        significant = significant[significant < self.anova_p_threshold].sort_values()\n",
    "        print(f\"\\n   Top 10 æœ€é¡¯è‘—ç‰¹å¾µ:\")\n",
    "        for i, (feat, p_val) in enumerate(significant.head(10).items(), 1):\n",
    "            print(f\"     {i:2d}. {feat}: p={p_val:.6f}\")\n",
    "        \n",
    "        self.filter_results['anova'] = {\n",
    "            'p_values': p_values,\n",
    "            'selected': selected_features,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[selected_features]\n",
    "    \n",
    "    def mda_importance(self, X: pd.DataFrame, y: pd.Series, \n",
    "                       sample_weight=None, n_estimators=100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        5ï¸âƒ£ MDA (Permutation Importance)\n",
    "        \n",
    "        ä½¿ç”¨ Random Forest è¨ˆç®— Permutation Importance\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"5ï¸âƒ£ MDA (Permutation Importance)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"   è¨“ç·´ Random Forest æ¨¡å‹...\")\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        rf.fit(X, y, sample_weight=sample_weight)\n",
    "        \n",
    "        # è¨ˆç®— baseline score\n",
    "        baseline_score = rf.score(X, y, sample_weight=sample_weight)\n",
    "        \n",
    "        print(f\"   Baseline Score: {baseline_score:.4f}\")\n",
    "        print(f\"   è¨ˆç®— Permutation Importance...\")\n",
    "        \n",
    "        # è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„ permutation importance\n",
    "        importances = {}\n",
    "        for col in X.columns:\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[col] = np.random.permutation(X_permuted[col].values)\n",
    "            permuted_score = rf.score(X_permuted, y, sample_weight=sample_weight)\n",
    "            importance = baseline_score - permuted_score\n",
    "            importances[col] = importance\n",
    "        \n",
    "        importance_series = pd.Series(importances).sort_values(ascending=False)\n",
    "        \n",
    "        # é¸æ“‡é‡è¦ç‰¹å¾µ\n",
    "        if self.mda_top_n is not None:\n",
    "            selected = importance_series.head(self.mda_top_n).index.tolist()\n",
    "        else:\n",
    "            # ä¿ç•™é‡è¦æ€§ > 0 çš„ç‰¹å¾µ\n",
    "            selected = importance_series[importance_series > 0].index.tolist()\n",
    "        \n",
    "        removed_count = len(X.columns) - len(selected)\n",
    "        \n",
    "        print(f\"   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä½é‡è¦æ€§ç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected):,}\")\n",
    "        print(f\"\\n   Top 10 æœ€é‡è¦ç‰¹å¾µ:\")\n",
    "        for i, (feat, imp) in enumerate(importance_series.head(10).items(), 1):\n",
    "            print(f\"     {i:2d}. {feat}: {imp:.6f}\")\n",
    "        \n",
    "        self.filter_results['mda'] = {\n",
    "            'importances': importance_series.to_dict(),\n",
    "            'selected': selected,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[selected]\n",
    "    \n",
    "    def stability_selection(self, X: pd.DataFrame, y: pd.Series,\n",
    "                           sample_weight=None, n_estimators=50):\n",
    "        \"\"\"\n",
    "        6ï¸âƒ£ Stability Selection\n",
    "        \n",
    "        é€šéå¤šæ¬¡å­æŠ½æ¨£å’Œæ¨¡å‹è¨“ç·´ï¼Œçµ±è¨ˆç‰¹å¾µè¢«é¸ä¸­çš„é »ç‡\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"6ï¸âƒ£ Stability Selection\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"   è¿­ä»£æ¬¡æ•¸: {self.stability_n_iter}\")\n",
    "        print(f\"   é¸æ“‡é »ç‡é–¾å€¼: {self.stability_threshold}\")\n",
    "        \n",
    "        feature_selection_counts = {col: 0 for col in X.columns}\n",
    "        \n",
    "        for i in range(self.stability_n_iter):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   é€²åº¦: {i+1}/{self.stability_n_iter}\")\n",
    "            \n",
    "            # å­æŠ½æ¨£ï¼ˆ80% æ•¸æ“šï¼‰\n",
    "            n_samples = int(len(X) * 0.8)\n",
    "            sample_idx = np.random.choice(X.index, size=n_samples, replace=False)\n",
    "            X_sub = X.loc[sample_idx]\n",
    "            y_sub = y.loc[sample_idx]\n",
    "            w_sub = sample_weight.loc[sample_idx] if sample_weight is not None else None\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹ä¸¦ç²å–ç‰¹å¾µé‡è¦æ€§\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=10,\n",
    "                min_samples_split=20,\n",
    "                random_state=i,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            \n",
    "            rf.fit(X_sub, y_sub, sample_weight=w_sub)\n",
    "            \n",
    "            # ç²å–ç‰¹å¾µé‡è¦æ€§ï¼ˆä½¿ç”¨ MDIï¼‰\n",
    "            importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "            top_features = importances.nlargest(int(len(X.columns) * 0.3)).index  # ä¿ç•™å‰30%\n",
    "            \n",
    "            for feat in top_features:\n",
    "                feature_selection_counts[feat] += 1\n",
    "        \n",
    "        # è¨ˆç®—é¸æ“‡é »ç‡\n",
    "        selection_freq = pd.Series(feature_selection_counts) / self.stability_n_iter\n",
    "        selection_freq = selection_freq.sort_values(ascending=False)\n",
    "        \n",
    "        # æ ¹æ“šé–¾å€¼é¸æ“‡ç‰¹å¾µ\n",
    "        selected = selection_freq[selection_freq >= self.stability_threshold].index.tolist()\n",
    "        removed_count = len(X.columns) - len(selected)\n",
    "        \n",
    "        print(f\"\\n   åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"   ç§»é™¤ä¸ç©©å®šç‰¹å¾µ: {removed_count:,}\")\n",
    "        print(f\"   å‰©é¤˜ç‰¹å¾µæ•¸: {len(selected):,}\")\n",
    "        print(f\"\\n   Top 10 æœ€ç©©å®šç‰¹å¾µ:\")\n",
    "        for i, (feat, freq) in enumerate(selection_freq.head(10).items(), 1):\n",
    "            print(f\"     {i:2d}. {feat}: {freq:.4f}\")\n",
    "        \n",
    "        self.filter_results['stability'] = {\n",
    "            'frequencies': selection_freq.to_dict(),\n",
    "            'selected': selected,\n",
    "            'removed_count': removed_count\n",
    "        }\n",
    "        \n",
    "        return X[selected]\n",
    "    \n",
    "    def fit_transform(self, X: pd.DataFrame, y: pd.Series, \n",
    "                     sample_weight=None, use_stability=False):\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œå®Œæ•´çš„ç¯©é¸æµç¨‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "        y : pd.Series\n",
    "            æ¨™ç±¤\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        use_stability : bool\n",
    "            æ˜¯å¦ä½¿ç”¨ Stability Selectionï¼ˆè¼ƒæ…¢ï¼‰\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_selected : pd.DataFrame\n",
    "            ç¯©é¸å¾Œçš„ç‰¹å¾µçŸ©é™£\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ğŸš€ é–‹å§‹åŸ·è¡Œç‰¹å¾µç¯©é¸ Pipeline\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "        print(f\"åŸå§‹æ¨£æœ¬æ•¸: {len(X):,}\")\n",
    "        \n",
    "        X_current = X.copy()\n",
    "        \n",
    "        # 1. Correlation Filter\n",
    "        X_current = self.correlation_filter(X_current)\n",
    "        \n",
    "        # 2. Variance Filter\n",
    "        X_current = self.variance_filter(X_current)\n",
    "        \n",
    "        # 3. IC Ranking\n",
    "        X_current = self.ic_ranking(X_current, y)\n",
    "        \n",
    "        # 4. ANOVA Filter\n",
    "        X_current = self.anova_filter(X_current, y)\n",
    "        \n",
    "        # 5. MDA Importance\n",
    "        X_current = self.mda_importance(X_current, y, sample_weight)\n",
    "        \n",
    "        # 6. Stability Selection (å¯é¸ï¼Œè¼ƒæ…¢)\n",
    "        if use_stability:\n",
    "            X_current = self.stability_selection(X_current, y, sample_weight)\n",
    "        \n",
    "        self.selected_features = list(X_current.columns)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"âœ… ç‰¹å¾µç¯©é¸å®Œæˆ\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"æœ€çµ‚ç‰¹å¾µæ•¸: {len(self.selected_features):,}\")\n",
    "        print(f\"ç‰¹å¾µæ¸›å°‘æ¯”ä¾‹: {(1 - len(self.selected_features) / len(X.columns)) * 100:.2f}%\")\n",
    "        \n",
    "        return X_current\n",
    "\n",
    "# =====================================================\n",
    "# åŸ·è¡Œç‰¹å¾µç¯©é¸\n",
    "# =====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa0490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_continuous\n",
    "import numpy as np\n",
    "\n",
    "class logUniform_gen(rv_continuous):\n",
    "    \"\"\"\n",
    "    Log-uniform distribution random variable generator\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    éš¨æ©Ÿè®Šæ•¸ x åœ¨ [a, b] å€é–“å…§éµå¾ª log-uniform åˆ†å¸ƒï¼Œ\n",
    "    ç•¶ä¸”åƒ…ç•¶ log[x] ~ U[log[a], log[b]]\n",
    "    \n",
    "    é€™å€‹åˆ†å¸ƒå°æ–¼æ¢ç´¢éç·šæ€§éŸ¿æ‡‰çš„åƒæ•¸ç©ºé–“ç‰¹åˆ¥æœ‰æ•ˆï¼Œ\n",
    "    ä¾‹å¦‚ SVC çš„ C åƒæ•¸å’Œ RBF kernel çš„ gamma åƒæ•¸ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cdf(self, x):\n",
    "        \"\"\"ç´¯ç©åˆ†å¸ƒå‡½æ•¸ (CDF)\"\"\"\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def logUniform(a=1, b=np.exp(1)):\n",
    "    \"\"\"\n",
    "    å‰µå»º log-uniform åˆ†å¸ƒ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    a : float\n",
    "        ä¸‹ç•Œï¼ˆå¿…é ˆ > 0ï¼‰\n",
    "    b : float\n",
    "        ä¸Šç•Œï¼ˆå¿…é ˆ > aï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    logUniform_gen : logUniform_gen\n",
    "        Log-uniform åˆ†å¸ƒç”Ÿæˆå™¨\n",
    "    \"\"\"\n",
    "    return logUniform_gen(a=a, b=b, name='logUniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250678c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Purged K-Fold Cross-Validation (Snippet 7.3 & 7.4)\n",
    "# =====================================================\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Generator, Tuple\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "class PurgedKFold(_BaseKFold):\n",
    "    \"\"\"\n",
    "    Extend KFold class to work with labels that span intervals\n",
    "    \n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    é€™å€‹é¡åˆ¥æ“´å±•äº† scikit-learn çš„ KFoldï¼Œç”¨æ–¼è™•ç†æ¨™ç±¤é‡ç–Šçš„æƒ…æ³ã€‚\n",
    "    ç•¶æ¨™ç±¤è·¨è¶Šæ™‚é–“å€é–“æ™‚ï¼ˆä¾‹å¦‚ Triple Barrier Methodï¼‰ï¼Œæ¸¬è©¦é›†çš„æ¨™ç±¤\n",
    "    å¯èƒ½æœƒèˆ‡è¨“ç·´é›†çš„æ¨™ç±¤é‡ç–Šï¼Œå°è‡´æ•¸æ“šæ´©æ¼ã€‚\n",
    "    \n",
    "    è§£æ±ºæ–¹æ³•:\n",
    "    ---------\n",
    "    1. Purgingï¼ˆæ¸…é™¤ï¼‰: å¾è¨“ç·´é›†ä¸­ç§»é™¤èˆ‡æ¸¬è©¦é›†æ¨™ç±¤å€é–“é‡ç–Šçš„è§€æ¸¬å€¼\n",
    "    2. Embargoï¼ˆç¦åˆ¶ï¼‰: åœ¨æ¸¬è©¦é›†çµæŸå¾Œï¼Œé¡å¤–ç§»é™¤ä¸€æ®µæ™‚é–“çš„è¨“ç·´æ¨£æœ¬\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    n_splits : int\n",
    "        K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "    t1 : pd.Series\n",
    "        æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰ï¼Œindex å¿…é ˆèˆ‡ X çš„ index ä¸€è‡´\n",
    "    pctEmbargo : float\n",
    "        ç¦åˆ¶æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œä¾‹å¦‚ 0.01 è¡¨ç¤ºç¦åˆ¶ 1% çš„æ•¸æ“š\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 3, t1: Optional[pd.Series] = None, pctEmbargo: float = 0.0):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– PurgedKFold\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        n_splits : int\n",
    "            K æŠ˜äº¤å‰é©—è­‰çš„æŠ˜æ•¸\n",
    "        t1 : pd.Series, optional\n",
    "            æ¨™ç±¤çš„çµæŸæ™‚é–“ï¼ˆThruDateï¼‰ï¼Œindex å¿…é ˆèˆ‡ X çš„ index ä¸€è‡´\n",
    "        pctEmbargo : float\n",
    "            ç¦åˆ¶æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œä¾‹å¦‚ 0.01 è¡¨ç¤ºç¦åˆ¶ 1% çš„æ•¸æ“š\n",
    "        \"\"\"\n",
    "        if t1 is not None and not isinstance(t1, pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pd.Series')\n",
    "        \n",
    "        super(PurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.t1 = t1\n",
    "        self.pctEmbargo = pctEmbargo\n",
    "    \n",
    "    def split(self, X: pd.DataFrame, y: Optional[pd.Series] = None, \n",
    "              groups: Optional[np.ndarray] = None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆè¨“ç·´/æ¸¬è©¦é›†çš„ç´¢å¼•\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µæ•¸æ“šï¼ˆindex å¿…é ˆèˆ‡ t1 çš„ index ä¸€è‡´ï¼‰\n",
    "        y : pd.Series, optional\n",
    "            æ¨™ç±¤ï¼ˆæœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ç¬¦åˆ scikit-learn æ¥å£ï¼‰\n",
    "        groups : np.ndarray, optional\n",
    "            åˆ†çµ„ï¼ˆæœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ç¬¦åˆ scikit-learn æ¥å£ï¼‰\n",
    "            \n",
    "        Yields:\n",
    "        -------\n",
    "        train_indices : np.ndarray\n",
    "            è¨“ç·´é›†ç´¢å¼•\n",
    "        test_indices : np.ndarray\n",
    "            æ¸¬è©¦é›†ç´¢å¼•\n",
    "        \"\"\"\n",
    "        if self.t1 is None:\n",
    "            raise ValueError('t1 (ThruDate) must be provided')\n",
    "        \n",
    "        # é©—è­‰ X å’Œ t1 çš„ index ä¸€è‡´\n",
    "        if (X.index == self.t1.index).sum() != len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "        \n",
    "        indices = np.arange(X.shape[0])\n",
    "        mbrg = int(X.shape[0] * self.pctEmbargo)  # embargo çš„æ¨£æœ¬æ•¸\n",
    "        \n",
    "        # å°‡æ•¸æ“šåˆ†æˆ n_splits å€‹é€£çºŒçš„æ¸¬è©¦é›†\n",
    "        test_starts = [(i[0], i[-1] + 1) for i in \n",
    "                      np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "        \n",
    "        for i, j in test_starts:\n",
    "            # æ¸¬è©¦é›†çš„èµ·å§‹æ™‚é–“\n",
    "            t0 = self.t1.index[i]  # start of test set\n",
    "            test_indices = indices[i:j]\n",
    "            \n",
    "            # æ‰¾åˆ°æ¸¬è©¦é›†ä¸­æœ€æ™šçš„çµæŸæ™‚é–“\n",
    "            maxT1Idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
    "            \n",
    "            # å·¦å´è¨“ç·´é›†ï¼šçµæŸæ™‚é–“ <= t0 çš„æ‰€æœ‰æ¨£æœ¬\n",
    "            train_indices = self.t1.index.searchsorted(\n",
    "                self.t1[self.t1 <= t0].index\n",
    "            )\n",
    "            \n",
    "            # å³å´è¨“ç·´é›†ï¼šåœ¨æ¸¬è©¦é›†çµæŸå¾Œï¼ŒåŠ ä¸Š embargo æœŸé–“\n",
    "            if maxT1Idx < X.shape[0]:\n",
    "                # right train (with embargo)\n",
    "                train_indices = np.concatenate((\n",
    "                    train_indices, \n",
    "                    indices[maxT1Idx + mbrg:]\n",
    "                ))\n",
    "            \n",
    "            yield train_indices, test_indices\n",
    "\n",
    "\n",
    "def macro_accuracy_score(y_true, y_pred, sample_weight=None, zero_division=0):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Macro Accuracyï¼ˆæ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡ç›´æ¥å¹³å‡ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        çœŸå¯¦æ¨™ç±¤\n",
    "    y_pred : array-like\n",
    "        é æ¸¬æ¨™ç±¤\n",
    "    sample_weight : array-like, optional\n",
    "        æ¨£æœ¬æ¬Šé‡\n",
    "    zero_division : float\n",
    "        ç•¶åˆ†æ¯ç‚º 0 æ™‚è¿”å›çš„å€¼\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    macro_accuracy : float\n",
    "        Macro Accuracyï¼ˆæ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡ç›´æ¥å¹³å‡ï¼‰\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.utils.multiclass import unique_labels\n",
    "    import numpy as np\n",
    "    \n",
    "    # ç²å–æ‰€æœ‰é¡åˆ¥\n",
    "    labels = unique_labels(y_true, y_pred)\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡\n",
    "    per_class_accuracies = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # æ‰¾å‡ºè©²é¡åˆ¥çš„æ‰€æœ‰æ¨£æœ¬\n",
    "        mask = (y_true == label)\n",
    "        \n",
    "        if mask.sum() == 0:\n",
    "            # å¦‚æœè©²é¡åˆ¥åœ¨çœŸå¯¦æ¨™ç±¤ä¸­ä¸å­˜åœ¨ï¼Œè·³é\n",
    "            continue\n",
    "            \n",
    "        # è©²é¡åˆ¥çš„æ¨£æœ¬\n",
    "        y_true_class = y_true[mask]\n",
    "        y_pred_class = y_pred[mask]\n",
    "        \n",
    "        # å¦‚æœæœ‰æ¨£æœ¬æ¬Šé‡\n",
    "        if sample_weight is not None:\n",
    "            sample_weight_class = sample_weight[mask]\n",
    "        else:\n",
    "            sample_weight_class = None\n",
    "        \n",
    "        # è¨ˆç®—è©²é¡åˆ¥çš„æº–ç¢ºç‡\n",
    "        class_accuracy = accuracy_score(\n",
    "            y_true_class, \n",
    "            y_pred_class, \n",
    "            sample_weight=sample_weight_class\n",
    "        )\n",
    "        per_class_accuracies.append(class_accuracy)\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰ä»»ä½•é¡åˆ¥ï¼Œè¿”å› zero_division\n",
    "    if len(per_class_accuracies) == 0:\n",
    "        return zero_division\n",
    "    \n",
    "    # è¿”å›å¹³å‡æº–ç¢ºç‡ï¼ˆMacro Accuracyï¼‰\n",
    "    return np.mean(per_class_accuracies)\n",
    "\n",
    "def cvScore(\n",
    "    clf,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    sample_weight: Optional[pd.Series] = None,\n",
    "    scoring: str = 'f1',\n",
    "    t1: Optional[pd.Series] = None,\n",
    "    cv: Optional[int] = None,\n",
    "    cvGen: Optional[PurgedKFold] = None,\n",
    "    pctEmbargo: Optional[float] = None,) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ PurgedKFold é€²è¡Œäº¤å‰é©—è­‰è©•åˆ†ï¼Œç¢ºä¿æ¯æŠ˜æ¨™ç±¤é‡æ–°ç·¨ç¢¼ç‚ºé€£çºŒæ•´æ•¸ã€‚\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import (\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        accuracy_score,\n",
    "        log_loss,\n",
    "    )\n",
    "\n",
    "    if cvGen is None:\n",
    "        if cv is None:\n",
    "            raise ValueError(\"è«‹æä¾› cv æˆ– cvGen å…¶ä¸­ä¹‹ä¸€\")\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = pd.Series(1.0, index=X.index)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cvGen.split(X=X)):\n",
    "        X_train_fold = X.iloc[train_idx, :]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_test_fold = X.iloc[test_idx, :]\n",
    "        y_test_fold = y.iloc[test_idx]\n",
    "\n",
    "        fold_label_encoder = LabelEncoder()\n",
    "        y_train_fold_encoded = pd.Series(\n",
    "            fold_label_encoder.fit_transform(y_train_fold),\n",
    "            index=y_train_fold.index,\n",
    "        )\n",
    "        y_test_fold_encoded = pd.Series(\n",
    "            fold_label_encoder.transform(y_test_fold),\n",
    "            index=y_test_fold.index,\n",
    "        )\n",
    "\n",
    "        sw_train = sample_weight.iloc[train_idx].values\n",
    "        sw_test = sample_weight.iloc[test_idx].values\n",
    "\n",
    "        fit = clf.fit(\n",
    "            X=X_train_fold,\n",
    "            y=y_train_fold_encoded,\n",
    "            sample_weight=sw_train,\n",
    "        )\n",
    "\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = fit.predict_proba(X_test_fold)\n",
    "            score_ = -log_loss(\n",
    "                y_test_fold_encoded,\n",
    "                prob,\n",
    "                sample_weight=sw_test,\n",
    "            )\n",
    "        elif scoring in ('f1', 'f1_macro'):\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = f1_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'f1_weighted':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = f1_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='weighted',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'precision_macro':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = precision_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'recall_macro':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = recall_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "        elif scoring == 'accuracy':\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = accuracy_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                sample_weight=sw_test,\n",
    "            )\n",
    "        else:\n",
    "            pred = fit.predict(X_test_fold)\n",
    "            score_ = f1_score(\n",
    "                y_test_fold_encoded,\n",
    "                pred,\n",
    "                average='macro',\n",
    "                sample_weight=sw_test,\n",
    "                zero_division=0,\n",
    "            )\n",
    "\n",
    "        scores.append(score_)\n",
    "\n",
    "    return np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab47ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ7ï¼šML Model æ§‹å»ºèˆ‡è¨“ç·´ï¼ˆæ“´å±•ç‰ˆ - æ”¯æ´å¤šç¨®æ¨¡å‹ï¼‰\n",
    "# =====================================================\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              ExtraTreesClassifier, AdaBoostClassifier,\n",
    "                              HistGradientBoostingClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            roc_auc_score, precision_recall_curve, \n",
    "                            roc_curve, accuracy_score, f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, Any, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    æ“´å±•ç‰ˆæ¨¡å‹è¨“ç·´å™¨ï¼šæ”¯æ´å¤šç¨®æ¨¡å‹\n",
    "    \n",
    "    æ”¯æ´çš„æ¨¡å‹é¡å‹:\n",
    "    -----\n",
    "    - 'xgboost': XGBoost\n",
    "    - 'lightgbm': LightGBM\n",
    "    - 'catboost': CatBoost\n",
    "    - 'rf': Random Forest\n",
    "    - 'et': Extra Trees\n",
    "    - 'gb': Gradient Boosting\n",
    "    - 'histgbm': HistGradientBoosting\n",
    "    - 'dt': Decision Tree\n",
    "    - 'lr': Logistic Regression\n",
    "    - 'svm': Support Vector Machine\n",
    "    - 'mlp': Multi-Layer Perceptron\n",
    "    - 'lstm': LSTM (éœ€è¦é¡å¤–è™•ç†)\n",
    "    - 'ada': AdaBoost\n",
    "    - 'knn': K-Nearest Neighbors\n",
    "    - 'nb': Naive Bayes\n",
    "    - 'qda': Quadratic Discriminant Analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'xgboost'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ¨¡å‹è¨“ç·´å™¨\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            æ¨¡å‹é¡å‹\n",
    "        \"\"\"\n",
    "        self.model_type = model_type.lower()\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.cv_scores = None\n",
    "        \n",
    "    def _create_model(self, **kwargs):\n",
    "        \"\"\"\n",
    "        å‰µå»ºæ¨¡å‹å¯¦ä¾‹\n",
    "        \"\"\"\n",
    "        if self.model_type == 'xgboost':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'eval_metric': 'logloss',\n",
    "                'use_label_encoder': False,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return XGBClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'lightgbm':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'verbosity': -1,\n",
    "                'force_col_wise': True\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return LGBMClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'catboost':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'verbose': False,\n",
    "                'thread_count': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return CatBoostClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return RandomForestClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'et':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return ExtraTreesClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'gb':\n",
    "            default_params = {\n",
    "                'random_state': 42\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return GradientBoostingClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'histgbm':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_iter': 100,\n",
    "                'early_stopping': True\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return HistGradientBoostingClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'dt':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_depth': 10\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return DecisionTreeClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'lr':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_iter': 1000,\n",
    "                'solver': 'lbfgs'\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return LogisticRegression(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'svm':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'probability': True,\n",
    "                'kernel': 'rbf'\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return SVC(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'mlp':\n",
    "            default_params = {\n",
    "                'random_state': 42,\n",
    "                'max_iter': 500,\n",
    "                'early_stopping': True,\n",
    "                'validation_fraction': 0.1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return MLPClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'ada':\n",
    "            default_params = {\n",
    "                'random_state': 42\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return AdaBoostClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'knn':\n",
    "            default_params = {\n",
    "                'n_neighbors': 5,\n",
    "                'weights': 'distance',\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(kwargs)\n",
    "            return KNeighborsClassifier(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'nb':\n",
    "            default_params = {}\n",
    "            default_params.update(kwargs)\n",
    "            return GaussianNB(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'qda':\n",
    "            default_params = {}\n",
    "            default_params.update(kwargs)\n",
    "            return QuadraticDiscriminantAnalysis(**default_params)\n",
    "        \n",
    "        elif self.model_type == 'lstm':\n",
    "            # LSTM éœ€è¦ç‰¹æ®Šè™•ç†ï¼Œé€™è£¡å…ˆæ‹‹å‡ºæç¤º\n",
    "            raise NotImplementedError(\n",
    "                \"LSTM éœ€è¦åºåˆ—æ•¸æ“šå’Œç‰¹æ®Šè™•ç†ï¼Œè«‹ä½¿ç”¨å°ˆé–€çš„æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼ˆå¦‚ TensorFlow/Kerasï¼‰\"\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}\")\n",
    "    \n",
    "    def _get_param_grid(self, use_randomized: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å–å¾—åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        print(f\"ğŸš€ å–å¾— {self.model_type} æ¨¡å‹åƒæ•¸ç¶²æ ¼\")\n",
    "        if self.model_type == 'xgboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lightgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10, -1],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__num_leaves': [15, 31, 50, 100],\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_samples': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__num_leaves': [31, 50],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'catboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__l2_leaf_reg': [1, 3, 5, 7],\n",
    "                    f'{self.model_type}__border_count': [32, 64, 128],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__l2_leaf_reg': [3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500, 1000],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': logUniform(1e-3, 1e2),\n",
    "                    f'{self.model_type}__min_samples_leaf': logUniform(1e-3, 1e1),\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                    f'{self.model_type}__bootstrap': [True, False]\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None]\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'et':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'gb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200],\n",
    "                    f'{self.model_type}__max_depth': [3, 5],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'histgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200, 300],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'dt':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "            }\n",
    "            return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lr':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-3, 1e3),\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.01, 0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'svm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-2, 1e2),\n",
    "                    f'{self.model_type}__gamma': logUniform(1e-4, 1e-1),\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'mlp':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                    f'{self.model_type}__alpha': logUniform(1e-5, 1e-1),\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "                    f'{self.model_type}__alpha': [0.0001, 0.001, 0.01],\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'ada':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_estimators': [50, 100, 200],\n",
    "                f'{self.model_type}__learning_rate': [0.01, 0.1, 1.0],\n",
    "            }\n",
    "            return param_grid   \n",
    "        \n",
    "        elif self.model_type == 'knn':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_neighbors': [3, 5, 7, 10, 15],\n",
    "                f'{self.model_type}__weights': ['uniform', 'distance'],\n",
    "                f'{self.model_type}__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "            }\n",
    "            return param_grid\n",
    "        elif self.model_type == 'nb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': logUniform(1e-9, 1e-3),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6],\n",
    "                }\n",
    "                return param_grid\n",
    "        elif self.model_type == 'qda':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': logUniform(1e-3, 1.0),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': [0.0, 0.1, 0.5, 1.0],\n",
    "                }   \n",
    "                return param_grid\n",
    "        else:\n",
    "            # å¦‚æœæ¨¡å‹é¡å‹æ²’æœ‰å°æ‡‰çš„åƒæ•¸ç¶²æ ¼ï¼Œè¿”å›ç©ºå­—å…¸\n",
    "            param_grid = {}\n",
    "            print(f\"âš ï¸ è­¦å‘Š: æ¨¡å‹é¡å‹ '{self.model_type}' æ²’æœ‰å®šç¾©åƒæ•¸ç¶²æ ¼ï¼Œä½¿ç”¨ç©ºå­—å…¸\")\n",
    "        \n",
    "        return param_grid\n",
    "    \n",
    "    def train(self, \n",
    "              X: pd.DataFrame,\n",
    "              y: pd.Series,\n",
    "              t1: pd.Series,\n",
    "              sample_weight: Optional[pd.Series] = None,\n",
    "              cv: int = 5,\n",
    "              pctEmbargo: float = 0.01,\n",
    "              use_hyperopt: bool = True,\n",
    "              use_randomized: bool = False,\n",
    "              rndSearchIter: int = 50,\n",
    "              n_jobs: int = -1,\n",
    "              **model_kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸš€ é–‹å§‹è¨“ç·´ {self.model_type.upper()} æ¨¡å‹\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if sample_weight is None:\n",
    "            sample_weight = pd.Series(1.0, index=X.index)\n",
    "        \n",
    "        # åˆ¤æ–·æ˜¯å¦éœ€è¦æ¨™æº–åŒ–ï¼ˆç·šæ€§æ¨¡å‹å’Œ SVMã€MLP éœ€è¦ï¼‰\n",
    "        needs_scaling = self.model_type in ['lr', 'svm', 'mlp', 'knn', 'nb', 'qda']\n",
    "        \n",
    "        if use_hyperopt:\n",
    "            print(f\"\\nğŸ“Š ä½¿ç”¨ {'RandomizedSearchCV' if use_randomized else 'GridSearchCV'} é€²è¡Œè¶…åƒæ•¸èª¿å„ª...\")\n",
    "            \n",
    "            # å‰µå»º pipeline\n",
    "            steps = []\n",
    "            if needs_scaling:\n",
    "                steps.append(('scaler', StandardScaler()))\n",
    "            steps.append((self.model_type, self._create_model(**model_kwargs)))\n",
    "            \n",
    "            pipe_clf = MyPipeline(steps)\n",
    "            \n",
    "            param_grid = self._get_param_grid(use_randomized=use_randomized)\n",
    "            \n",
    "            # æº–å‚™ fit_paramsï¼ˆæŸäº›æ¨¡å‹ä¸æ”¯æ´ sample_weightï¼‰\n",
    "            fit_params = {}\n",
    "            if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda']:\n",
    "                fit_params[f'{self.model_type}__sample_weight'] = sample_weight\n",
    "            \n",
    "            best_pipeline = clfHyperFit(\n",
    "                feat=X,\n",
    "                lbl=y,\n",
    "                t1=t1,\n",
    "                pipe_clf=pipe_clf,\n",
    "                param_grid=param_grid,\n",
    "                cv=cv,\n",
    "                bagging=[0, None, 1.],\n",
    "                rndSearchIter=rndSearchIter if use_randomized else 0,\n",
    "                n_jobs=n_jobs,\n",
    "                pctEmbargo=pctEmbargo,\n",
    "                **fit_params\n",
    "            )\n",
    "            \n",
    "            self.model = best_pipeline.named_steps[self.model_type]\n",
    "            \n",
    "            if hasattr(best_pipeline, 'best_params_'):\n",
    "                self.best_params = best_pipeline.best_params_\n",
    "            \n",
    "            print(f\"\\nâœ… è¶…åƒæ•¸èª¿å„ªå®Œæˆ\")\n",
    "            if self.best_params:\n",
    "                print(f\"æœ€ä½³åƒæ•¸:\")\n",
    "                for key, value in list(self.best_params.items())[:5]:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                if len(self.best_params) > 5:\n",
    "                    print(f\"  ... (å…± {len(self.best_params)} å€‹åƒæ•¸)\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ“Š ç›´æ¥è¨“ç·´æ¨¡å‹ï¼ˆä¸ä½¿ç”¨è¶…åƒæ•¸èª¿å„ªï¼‰...\")\n",
    "            \n",
    "            inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "            \n",
    "            # å°æ–¼éœ€è¦æ¨™æº–åŒ–çš„æ¨¡å‹ï¼Œä½¿ç”¨ Pipeline\n",
    "            if needs_scaling:\n",
    "                from sklearn.pipeline import Pipeline\n",
    "                clf = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('clf', self._create_model(**model_kwargs))\n",
    "                ])\n",
    "                fit_params = {}\n",
    "                if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda']:\n",
    "                    fit_params['clf__sample_weight'] = sample_weight\n",
    "            else:\n",
    "                clf = self._create_model(**model_kwargs)\n",
    "                fit_params = {}\n",
    "                if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda']:\n",
    "                    fit_params['sample_weight'] = sample_weight\n",
    "            \n",
    "            self.cv_scores = cvScore(\n",
    "                clf=clf,\n",
    "                X=X,\n",
    "                y=y,\n",
    "                sample_weight=sample_weight if self.model_type not in ['lr', 'svm', 'knn', 'nb', 'qda'] else None,\n",
    "                scoring='f1',\n",
    "                cvGen=inner_cv\n",
    "            )\n",
    "            \n",
    "            print(f\"\\näº¤å‰é©—è­‰åˆ†æ•¸: {self.cv_scores}\")\n",
    "            print(f\"å¹³å‡åˆ†æ•¸: {self.cv_scores.mean():.4f} Â± {self.cv_scores.std():.4f}\")\n",
    "            \n",
    "            if needs_scaling:\n",
    "                from sklearn.pipeline import Pipeline\n",
    "                self.model = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('clf', self._create_model(**model_kwargs))\n",
    "                ])\n",
    "                self.model.fit(X, y, **fit_params)\n",
    "            else:\n",
    "                self.model = clf.fit(X, y, **fit_params)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"é€²è¡Œé æ¸¬\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        predictions = self.model.predict(X)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def evaluate(self, \n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight: Optional[pd.Series] = None) -> Dict[str, float]:\n",
    "        \"\"\"è©•ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        y_pred, y_proba = self.predict(X)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred, sample_weight=sample_weight),\n",
    "            'f1': f1_score(y, y_pred, sample_weight=sample_weight, average='weighted'),\n",
    "        }\n",
    "        \n",
    "        if len(np.unique(y)) == 2:\n",
    "            metrics['roc_auc'] = roc_auc_score(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            metrics['f1_binary'] = f1_score(y, y_pred, sample_weight=sample_weight)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self, \n",
    "                    X: pd.DataFrame,\n",
    "                    y: pd.Series,\n",
    "                    sample_weight: Optional[pd.Series] = None):\n",
    "        \"\"\"ç¹ªè£½æ¨¡å‹è©•ä¼°çµæœï¼ˆèˆ‡åŸç‰ˆç›¸åŒï¼‰\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        y_pred, y_proba = self.predict(X)\n",
    "        metrics = self.evaluate(X, y, sample_weight)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. æ··æ·†çŸ©é™£\n",
    "        cm = confusion_matrix(y, y_pred, sample_weight=sample_weight)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "        axes[0, 0].set_ylabel('True Label', fontsize=12)\n",
    "        axes[0, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "        \n",
    "        # 2. ROC æ›²ç·š\n",
    "        if len(np.unique(y)) == 2:\n",
    "            fpr, tpr, _ = roc_curve(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.4f})')\n",
    "            axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n",
    "            axes[0, 1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "            axes[0, 1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "            axes[0, 1].set_title('ROC Curve', fontsize=14)\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'ROC Curve\\n(Only for binary classification)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 1].set_title('ROC Curve', fontsize=14)\n",
    "        \n",
    "        # 3. Precision-Recall æ›²ç·š\n",
    "        if len(np.unique(y)) == 2:\n",
    "            precision, recall, _ = precision_recall_curve(y, y_proba[:, 1], sample_weight=sample_weight)\n",
    "            axes[1, 0].plot(recall, precision, linewidth=2)\n",
    "            axes[1, 0].set_xlabel('Recall', fontsize=12)\n",
    "            axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
    "            axes[1, 0].set_title('Precision-Recall Curve', fontsize=14)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Precision-Recall Curve\\n(Only for binary classification)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 0].set_title('Precision-Recall Curve', fontsize=14)\n",
    "        \n",
    "        # 4. è©•ä¼°æŒ‡æ¨™æ‘˜è¦\n",
    "        axes[1, 1].axis('off')\n",
    "        metrics_text = f\"\"\"\n",
    "        Model: {self.model_type.upper()}\n",
    "        \n",
    "        Evaluation Metrics:\n",
    "        --------------------\n",
    "        Accuracy: {metrics['accuracy']:.4f}\n",
    "        F1 Score: {metrics['f1']:.4f}\n",
    "        \"\"\"\n",
    "        if 'roc_auc' in metrics:\n",
    "            metrics_text += f\"ROC AUC: {metrics['roc_auc']:.4f}\\n\"\n",
    "        if 'f1_binary' in metrics:\n",
    "            metrics_text += f\"F1 (Binary): {metrics['f1_binary']:.4f}\\n\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, \n",
    "                        verticalalignment='center', family='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š è©³ç´°åˆ†é¡å ±å‘Š\")\n",
    "        print(\"=\" * 60)\n",
    "        print(classification_report(y, y_pred, sample_weight=sample_weight))\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353ab8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Hyperparameter Tuning with Purged K-Fold CV\n",
    "# Snippet 9.1, 9.2, 9.3, 9.4\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from scipy.stats import rv_continuous, kstest\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.2: Enhanced Pipeline Class\n",
    "# =====================================================\n",
    "\n",
    "class MyPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Enhanced Pipeline class that handles sample_weight argument\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    sklearn çš„ Pipeline çš„ fit æ–¹æ³•ä¸æ¥å— sample_weight åƒæ•¸ï¼Œ\n",
    "    è€Œæ˜¯æœŸæœ› fit_params é—œéµå­—åƒæ•¸ã€‚é€™æ˜¯ä¸€å€‹å·²çŸ¥çš„ bugã€‚\n",
    "    é€™å€‹é¡åˆ¥æ“´å±•äº† Pipelineï¼Œé‡å¯« fit æ–¹æ³•ä»¥è™•ç† sample_weightã€‚\n",
    "    \n",
    "    åƒè€ƒ:\n",
    "    -----\n",
    "    - GitHub issue: sklearn Pipeline sample_weight bug\n",
    "    - Stackoverflow: Understanding Python super with __init__ methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \"\"\"\n",
    "        é‡å¯« fit æ–¹æ³•ä»¥è™•ç† sample_weight\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            ç‰¹å¾µæ•¸æ“š\n",
    "        y : array-like\n",
    "            æ¨™ç±¤\n",
    "        sample_weight : array-like, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        **fit_params : dict\n",
    "            å…¶ä»– fit åƒæ•¸\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : MyPipeline\n",
    "            æ“¬åˆå¾Œçš„ pipeline\n",
    "        \"\"\"\n",
    "        if sample_weight is not None:\n",
    "            # å°‡ sample_weight å‚³éçµ¦æœ€å¾Œä¸€å€‹æ­¥é©Ÿï¼ˆé€šå¸¸æ˜¯åˆ†é¡å™¨ï¼‰\n",
    "            last_step_name = self.steps[-1][0]\n",
    "            fit_params[f'{last_step_name}__sample_weight'] = sample_weight\n",
    "        \n",
    "        return super(MyPipeline, self).fit(X, y, **fit_params)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.4: Log-Uniform Distribution\n",
    "# =====================================================\n",
    "\n",
    "class logUniform_gen(rv_continuous):\n",
    "    \"\"\"\n",
    "    Log-uniform distribution random variable generator\n",
    "    \n",
    "    èªªæ˜:\n",
    "    -----\n",
    "    éš¨æ©Ÿè®Šæ•¸ x åœ¨ [a, b] å€é–“å…§éµå¾ª log-uniform åˆ†å¸ƒï¼Œ\n",
    "    ç•¶ä¸”åƒ…ç•¶ log[x] ~ U[log[a], log[b]]\n",
    "    \n",
    "    é€™å€‹åˆ†å¸ƒå°æ–¼æ¢ç´¢éç·šæ€§éŸ¿æ‡‰çš„åƒæ•¸ç©ºé–“ç‰¹åˆ¥æœ‰æ•ˆï¼Œ\n",
    "    ä¾‹å¦‚ SVC çš„ C åƒæ•¸å’Œ RBF kernel çš„ gamma åƒæ•¸ã€‚\n",
    "    \n",
    "    æ•¸å­¸å®šç¾©:\n",
    "    ---------\n",
    "    CDF: F[x] = log[x] - log[a] / log[b] - log[a]  for a â‰¤ x â‰¤ b\n",
    "    PDF: f[x] = 1 / (x * log[b/a])                 for a â‰¤ x â‰¤ b\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cdf(self, x):\n",
    "        \"\"\"ç´¯ç©åˆ†å¸ƒå‡½æ•¸ (CDF)\"\"\"\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def logUniform(a=1, b=np.exp(1)):\n",
    "    \"\"\"\n",
    "    å‰µå»º log-uniform åˆ†å¸ƒ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    a : float\n",
    "        ä¸‹ç•Œï¼ˆå¿…é ˆ > 0ï¼‰\n",
    "    b : float\n",
    "        ä¸Šç•Œï¼ˆå¿…é ˆ > aï¼‰\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    logUniform_gen : logUniform_gen\n",
    "        Log-uniform åˆ†å¸ƒç”Ÿæˆå™¨\n",
    "    \"\"\"\n",
    "    return logUniform_gen(a=a, b=b, name='logUniform')\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Snippet 9.1 & 9.3: Hyperparameter Fitting Function\n",
    "# =====================================================\n",
    "\n",
    "def clfHyperFit(feat: pd.DataFrame, \n",
    "                lbl: pd.Series,\n",
    "                t1: pd.Series,\n",
    "                pipe_clf: Pipeline,\n",
    "                param_grid: Dict[str, List[Any]],\n",
    "                cv: int = 3,\n",
    "                bagging: List[Any] = [0, None, 1.],\n",
    "                rndSearchIter: int = 0,\n",
    "                n_jobs: int = -1,\n",
    "                pctEmbargo: float = 0.0,\n",
    "                scoring: str = 'f1_macro',  # â­ æ–°å¢åƒæ•¸\n",
    "                **fit_params) -> Pipeline:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Purged K-Fold äº¤å‰é©—è­‰é€²è¡Œè¶…åƒæ•¸èª¿å„ª\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    -----------\n",
    "    scoring : str\n",
    "        è©•åˆ†æ–¹æ³•ï¼ˆç”¨æ–¼ GridSearchCV/RandomizedSearchCVï¼‰\n",
    "        - 'f1_macro': Macro-F1\n",
    "        - 'f1_weighted': Weighted-F1\n",
    "        - 'accuracy_macro': Macro Accuracy\n",
    "        - 'precision_macro': Macro Precision\n",
    "        - 'recall_macro': Macro Recall\n",
    "        - 'accuracy': æº–ç¢ºç‡\n",
    "        - 'neg_log_loss': è² å°æ•¸æå¤±\n",
    "    **fit_params : dict\n",
    "        å…¶ä»– fit åƒæ•¸ï¼ˆä¾‹å¦‚ sample_weightï¼‰ï¼Œä½†ä¸åŒ…æ‹¬ scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    # â­ é‡è¦ï¼šå¾ fit_params ä¸­ç§»é™¤ scoringï¼ˆå¦‚æœå­˜åœ¨çš„è©±ï¼‰\n",
    "    # å› ç‚º scoring æ˜¯ GridSearchCV çš„åƒæ•¸ï¼Œä¸æ˜¯ Pipeline.fit() çš„åƒæ•¸\n",
    "    fit_params_clean = {k: v for k, v in fit_params.items() if k != 'scoring'}\n",
    "    \n",
    "    # 1) è¶…åƒæ•¸æœå°‹ï¼Œåœ¨è¨“ç·´æ•¸æ“šä¸Š\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
    "    \n",
    "    # âš ï¸ æ³¨æ„ï¼šsklearn çš„ GridSearchCV/RandomizedSearchCV ä¸ç›´æ¥æ”¯æ´è‡ªå®šç¾©çš„ scoring å­—ç¬¦ä¸²\n",
    "    # éœ€è¦å‰µå»ºä¸€å€‹è‡ªå®šç¾©çš„ scorer æˆ–ä½¿ç”¨ make_scorer\n",
    "    # ä½†ç”±æ–¼æˆ‘å€‘ä½¿ç”¨çš„æ˜¯ PurgedKFoldï¼Œå¯èƒ½éœ€è¦è‡ªå®šç¾©è©•åˆ†é‚è¼¯\n",
    "    \n",
    "    # æš«æ™‚ä½¿ç”¨ scoring åƒæ•¸ï¼ˆå¦‚æœ sklearn æ”¯æ´çš„è©±ï¼‰\n",
    "    # å¦‚æœä¸æ”¯æ´ï¼Œéœ€è¦å‰µå»ºè‡ªå®šç¾© scorer\n",
    "    from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score\n",
    "    \n",
    "    # å‰µå»ºè‡ªå®šç¾© scorerï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "    if scoring == 'f1_macro':\n",
    "        scorer = make_scorer(f1_score, average='macro', zero_division=0)\n",
    "    elif scoring == 'f1_weighted':\n",
    "        scorer = make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "    elif scoring == 'precision_macro':\n",
    "        scorer = make_scorer(precision_score, average='macro', zero_division=0)\n",
    "    elif scoring == 'recall_macro':\n",
    "        scorer = make_scorer(recall_score, average='macro', zero_division=0)\n",
    "    elif scoring == 'accuracy_macro':\n",
    "        # ä½¿ç”¨è‡ªå®šç¾©çš„ macro_accuracy_score\n",
    "        scorer = make_scorer(macro_accuracy_score, zero_division=0)\n",
    "    elif scoring == 'accuracy':\n",
    "        scorer = make_scorer(accuracy_score)\n",
    "    elif scoring == 'neg_log_loss':\n",
    "        from sklearn.metrics import log_loss\n",
    "        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "    else:\n",
    "        # é è¨­ä½¿ç”¨ f1_macro\n",
    "        scorer = make_scorer(f1_score, average='macro', zero_division=0)\n",
    "    \n",
    "    if rndSearchIter == 0:\n",
    "        gs = GridSearchCV(\n",
    "            estimator=pipe_clf,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scorer,  # â­ ä½¿ç”¨è‡ªå®šç¾© scorer\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "    else:\n",
    "        gs = RandomizedSearchCV(\n",
    "            estimator=pipe_clf,\n",
    "            param_distributions=param_grid,\n",
    "            scoring=scorer,  # â­ ä½¿ç”¨è‡ªå®šç¾© scorer\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs,\n",
    "            n_iter=rndSearchIter\n",
    "        )\n",
    "    \n",
    "    # â­ ä½¿ç”¨æ¸…ç†å¾Œçš„ fit_paramsï¼ˆä¸åŒ…å« scoringï¼‰\n",
    "    gs = gs.fit(feat, lbl, **fit_params_clean).best_estimator_\n",
    "    \n",
    "    # 2) åœ¨å…¨éƒ¨æ•¸æ“šä¸Šæ“¬åˆé©—è­‰å¾Œçš„æ¨¡å‹\n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(\n",
    "            base_estimator=MyPipeline(gs.steps),\n",
    "            n_estimators=int(bagging[0]),\n",
    "            max_samples=float(bagging[1]),\n",
    "            max_features=float(bagging[2]),\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        gs = gs.fit(feat, lbl, **fit_params_clean)\n",
    "    \n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2218b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# æ­¥é©Ÿ10ï¼šPredictionPipeline - æ•´åˆæ­¥é©Ÿ7ã€8ã€9\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any, Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PredictionPipeline:\n",
    "    \"\"\"\n",
    "    é æ¸¬ Pipelineï¼šæ•´åˆæ¨¡å‹è¨“ç·´ã€äº¤å‰é©—è­‰ã€è¶…åƒæ•¸èª¿æ•´\n",
    "    \n",
    "    åŠŸèƒ½:\n",
    "    -----\n",
    "    1. æ¨¡å‹è¨“ç·´ï¼ˆæ­¥é©Ÿ7ï¼‰\n",
    "    2. PurgedKFold äº¤å‰é©—è­‰ï¼ˆæ­¥é©Ÿ8ï¼‰\n",
    "    3. è¶…åƒæ•¸èª¿æ•´ï¼ˆæ­¥é©Ÿ9ï¼‰\n",
    "    4. æ¨¡å‹é æ¸¬\n",
    "    5. çµæœè©•ä¼°å’Œå¯è¦–åŒ–\n",
    "    \n",
    "    ä½¿ç”¨æ–¹å¼:\n",
    "    --------\n",
    "    pipeline = PredictionPipeline(\n",
    "        model_type='xgboost',  # æˆ– 'rf'\n",
    "        use_hyperopt=True,     # æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´\n",
    "        use_cv=True            # æ˜¯å¦ä½¿ç”¨äº¤å‰é©—è­‰\n",
    "    )\n",
    "    \n",
    "    # å‚³å…¥æº–å‚™å¥½çš„æ•¸æ“š\n",
    "    pipeline.fit(\n",
    "        X=X,                    # ç‰¹å¾µçŸ©é™£\n",
    "        y=y,                    # æ¨™ç±¤\n",
    "        t1=t1,                  # æ¨™ç±¤çµæŸæ™‚é–“\n",
    "        sample_weight=weights   # æ¨£æœ¬æ¬Šé‡ï¼ˆå¯é¸ï¼‰\n",
    "    )\n",
    "    \n",
    "    # é æ¸¬\n",
    "    predictions, probabilities = pipeline.predict(X_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_type: str = 'xgboost',\n",
    "                 cv_scoring: str = 'f1_macro',  # â­ æ–°å¢é€™å€‹åƒæ•¸\n",
    "                 use_hyperopt: bool = True,\n",
    "                 use_cv: bool = True,\n",
    "                 cv_splits: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 hyperopt_method: str = 'randomized',  # 'randomized' æˆ– 'grid'\n",
    "                 n_iter: int = 30,\n",
    "                 bagging: Optional[List[Any]] = None,\n",
    "                 n_jobs: int = -1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–é æ¸¬ Pipeline\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            æ¨¡å‹é¡å‹ï¼š'xgboost' æˆ– 'rf'\n",
    "        use_hyperopt : bool\n",
    "            æ˜¯å¦ä½¿ç”¨è¶…åƒæ•¸èª¿æ•´ï¼ˆé è¨­ Trueï¼‰\n",
    "        use_cv : bool\n",
    "            æ˜¯å¦ä½¿ç”¨äº¤å‰é©—è­‰ï¼ˆé è¨­ Trueï¼‰\n",
    "        cv_splits : int\n",
    "            äº¤å‰é©—è­‰æŠ˜æ•¸ï¼ˆé è¨­ 5ï¼‰\n",
    "        pct_embargo : float\n",
    "            ç¦åˆ¶æ¯”ä¾‹ï¼ˆé è¨­ 0.01ï¼‰\n",
    "        hyperopt_method : str\n",
    "            è¶…åƒæ•¸èª¿æ•´æ–¹æ³•ï¼š'randomized' æˆ– 'grid'ï¼ˆé è¨­ 'randomized'ï¼‰\n",
    "        n_iter : int\n",
    "            éš¨æ©Ÿæœå°‹è¿­ä»£æ¬¡æ•¸ï¼ˆé è¨­ 30ï¼‰\n",
    "        bagging : list, optional\n",
    "            Bagging åƒæ•¸ [n_estimators, max_samples, max_features]\n",
    "        n_jobs : int\n",
    "            ä¸¦è¡Œä½œæ¥­æ•¸ï¼ˆé è¨­ -1ï¼‰\n",
    "        \"\"\"\n",
    "        self.model_type = model_type.lower()\n",
    "        self.cv_scoring = cv_scoring  # â­ æ–°å¢é€™è¡Œ\n",
    "        self.use_hyperopt = use_hyperopt\n",
    "        self.use_cv = use_cv\n",
    "        self.cv_splits = cv_splits\n",
    "        self.pct_embargo = pct_embargo\n",
    "        self.hyperopt_method = hyperopt_method\n",
    "        self.n_iter = n_iter\n",
    "        self.bagging = bagging if bagging is not None else [0, None, 1.]\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        # å…§éƒ¨è®Šé‡\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.best_params = None\n",
    "        self.cv_scores = None\n",
    "        self.best_cv_scores = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.t1_train = None\n",
    "        self.sample_weight_train = None\n",
    "        self.feature_columns = None\n",
    "        \n",
    "    # åœ¨ PredictionPipeline é¡ä¸­ï¼Œä¿®æ”¹ _create_pipeline æ–¹æ³•\n",
    "    def _create_pipeline(self, n_classes: Optional[int] = None, **model_kwargs) -> MyPipeline:\n",
    "        \"\"\"\n",
    "        å»ºç«‹æ¨¡å‹ Pipelineï¼Œæœƒä¾ç…§å¯¦éš›é¡åˆ¥æ•¸èª¿æ•´æ¨¡å‹ç›®æ¨™å‡½å¼ã€‚\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        if self.model_type == 'xgboost':\n",
    "            from xgboost import XGBClassifier\n",
    "            if n_classes is None or n_classes == 2:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'eval_metric': 'logloss',\n",
    "                    'use_label_encoder': False,\n",
    "                    'objective': 'binary:logistic',\n",
    "                }\n",
    "            else:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'eval_metric': 'mlogloss',\n",
    "                    'use_label_encoder': False,\n",
    "                    'objective': 'multi:softprob',\n",
    "                    'num_class': n_classes,\n",
    "                }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = XGBClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type in ('lightgbm', 'lgb'):\n",
    "            from lightgbm import LGBMClassifier\n",
    "            if n_classes is None or n_classes == 2:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': -1,\n",
    "                    'force_col_wise': True,\n",
    "                    'objective': 'binary',\n",
    "                }\n",
    "            else:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': -1,\n",
    "                    'force_col_wise': True,\n",
    "                    'objective': 'multiclass',\n",
    "                    'num_class': n_classes,\n",
    "                }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = LGBMClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'catboost':\n",
    "            from catboost import CatBoostClassifier\n",
    "            if n_classes is None or n_classes == 2:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbose': False,\n",
    "                    'thread_count': -1,\n",
    "                    'loss_function': 'Logloss',\n",
    "                }\n",
    "            else:\n",
    "                default_params = {\n",
    "                    'random_state': 42,\n",
    "                    'verbose': False,\n",
    "                    'thread_count': -1,\n",
    "                    'loss_function': 'MultiClass',\n",
    "                }\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = CatBoostClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'rf':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            default_params = {'random_state': 42, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = RandomForestClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'et':\n",
    "            from sklearn.ensemble import ExtraTreesClassifier\n",
    "            default_params = {'random_state': 42, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = ExtraTreesClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'gb':\n",
    "            from sklearn.ensemble import GradientBoostingClassifier\n",
    "            default_params = {'random_state': 42}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = GradientBoostingClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'histgbm':\n",
    "            from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "            default_params = {'random_state': 42}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = HistGradientBoostingClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'dt':\n",
    "            from sklearn.tree import DecisionTreeClassifier\n",
    "            default_params = {'random_state': 42, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = DecisionTreeClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'lr':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            default_params = {'random_state': 42, 'max_iter': 1000, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = LogisticRegression(**default_params)\n",
    "\n",
    "        elif self.model_type == 'svm':\n",
    "            from sklearn.svm import SVC\n",
    "            default_params = {'random_state': 42, 'probability': True, 'class_weight': 'balanced'}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = SVC(**default_params)\n",
    "\n",
    "        elif self.model_type == 'mlp':\n",
    "            from sklearn.neural_network import MLPClassifier\n",
    "            default_params = {'random_state': 42, 'max_iter': 1000}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = MLPClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'ada':\n",
    "            from sklearn.ensemble import AdaBoostClassifier\n",
    "            default_params = {'random_state': 42}\n",
    "            default_params.update(model_kwargs)\n",
    "            clf = AdaBoostClassifier(**default_params)\n",
    "\n",
    "        elif self.model_type == 'nb':\n",
    "            from sklearn.naive_bayes import GaussianNB\n",
    "            clf = GaussianNB()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}\")\n",
    "\n",
    "        needs_scaling = self.model_type in ['lr', 'svm', 'mlp', 'knn', 'nb']\n",
    "        if needs_scaling:\n",
    "            pipe = MyPipeline([('scaler', StandardScaler()), (self.model_type, clf)])\n",
    "        else:\n",
    "            pipe = MyPipeline([(self.model_type, clf)])\n",
    "\n",
    "        return pipe\n",
    "    \n",
    "    def _get_param_grid(self, use_randomized: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å–å¾—åƒæ•¸ç¶²æ ¼æˆ–åƒæ•¸åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        print(f\"ğŸš€ å–å¾— {self.model_type} æ¨¡å‹åƒæ•¸ç¶²æ ¼\")\n",
    "        if self.model_type == 'xgboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_weight': [1, 3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lightgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10, -1],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__num_leaves': [15, 31, 50, 100],\n",
    "                    f'{self.model_type}__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                    f'{self.model_type}__min_child_samples': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__num_leaves': [31, 50],\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'catboost':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__l2_leaf_reg': [1, 3, 5, 7],\n",
    "                    f'{self.model_type}__border_count': [32, 64, 128],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__iterations': [100, 200, 300],\n",
    "                    f'{self.model_type}__depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__l2_leaf_reg': [3, 5],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'rf':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500, 1000],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': logUniform(1e-3, 1e2),\n",
    "                    f'{self.model_type}__min_samples_leaf': logUniform(1e-3, 1e1),\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                    f'{self.model_type}__bootstrap': [True, False]\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None]\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'et':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300, 500],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [5, 10, 15, None],\n",
    "                    f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                    f'{self.model_type}__max_features': ['sqrt', 'log2'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'gb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200, 300],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__subsample': [0.8, 0.9, 1.0],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__n_estimators': [100, 200],\n",
    "                    f'{self.model_type}__max_depth': [3, 5],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'histgbm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200, 300],\n",
    "                    f'{self.model_type}__learning_rate': logUniform(1e-3, 1e-1),\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7, 10],\n",
    "                    f'{self.model_type}__min_samples_leaf': [10, 20, 30],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__max_iter': [100, 200],\n",
    "                    f'{self.model_type}__learning_rate': [0.01, 0.05, 0.1],\n",
    "                    f'{self.model_type}__max_depth': [3, 5, 7],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'dt':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__max_depth': [5, 10, 15, 20, None],\n",
    "                f'{self.model_type}__min_samples_split': [2, 5, 10],\n",
    "                f'{self.model_type}__min_samples_leaf': [1, 2, 4],\n",
    "                f'{self.model_type}__max_features': ['sqrt', 'log2', None],\n",
    "            }\n",
    "            return param_grid\n",
    "        \n",
    "        elif self.model_type == 'lr':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-3, 1e3),\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.01, 0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__penalty': ['l2'],  # lbfgs åªæ”¯æŒ l2\n",
    "                    f'{self.model_type}__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],  # æ”¯æŒ multinomial çš„ solver\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'svm':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': logUniform(1e-2, 1e2),\n",
    "                    f'{self.model_type}__gamma': logUniform(1e-4, 1e-1),\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__C': [0.1, 1.0, 10.0],\n",
    "                    f'{self.model_type}__gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "                    f'{self.model_type}__kernel': ['rbf', 'poly'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'mlp':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                    f'{self.model_type}__alpha': logUniform(1e-5, 1e-1),\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "                    f'{self.model_type}__alpha': [0.0001, 0.001, 0.01],\n",
    "                    f'{self.model_type}__learning_rate': ['constant', 'adaptive'],\n",
    "                }\n",
    "                return param_grid\n",
    "        \n",
    "        elif self.model_type == 'ada':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_estimators': [50, 100, 200],\n",
    "                f'{self.model_type}__learning_rate': [0.01, 0.1, 1.0],\n",
    "            }\n",
    "            return param_grid   \n",
    "        \n",
    "        elif self.model_type == 'knn':\n",
    "            param_grid = {\n",
    "                f'{self.model_type}__n_neighbors': [3, 5, 7, 10, 15],\n",
    "                f'{self.model_type}__weights': ['uniform', 'distance'],\n",
    "                f'{self.model_type}__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "            }\n",
    "            return param_grid\n",
    "        elif self.model_type == 'nb':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': logUniform(1e-9, 1e-3),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6],\n",
    "                }\n",
    "                return param_grid\n",
    "        elif self.model_type == 'qda':\n",
    "            if use_randomized:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': logUniform(1e-3, 1.0),\n",
    "                }\n",
    "                return param_grid\n",
    "            else:\n",
    "                param_grid = {\n",
    "                    f'{self.model_type}__reg_param': [0.0, 0.1, 0.5, 1.0],\n",
    "                }   \n",
    "                return param_grid\n",
    "        else:\n",
    "            # å¦‚æœæ¨¡å‹é¡å‹æ²’æœ‰å°æ‡‰çš„åƒæ•¸ç¶²æ ¼ï¼Œè¿”å›ç©ºå­—å…¸\n",
    "            param_grid = {}\n",
    "            print(f\"âš ï¸ è­¦å‘Š: æ¨¡å‹é¡å‹ '{self.model_type}' æ²’æœ‰å®šç¾©åƒæ•¸ç¶²æ ¼ï¼Œä½¿ç”¨ç©ºå­—å…¸\")\n",
    "        \n",
    "        return param_grid\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        t1: Optional[pd.Series],\n",
    "        sample_weight: Optional[pd.Series] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> \"PredictionPipeline\":\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¨¡å‹ï¼ˆæ•´åˆ PurgedKFoldã€è¶…åƒæ•¸æœå°‹ï¼‰\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ ({self.model_type.upper()})\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # æ­¥é©Ÿ0ï¼šæ¨™ç±¤é‡æ–°ç·¨ç¢¼\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\")\n",
    "        print(\"=\" * 60)\n",
    "        original_unique = sorted(y.unique())\n",
    "        print(f\"åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: {original_unique}\")\n",
    "        print(f\"åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {y.value_counts().to_dict()}\")\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = pd.Series(label_encoder.fit_transform(y), index=y.index)\n",
    "        self.label_encoder_ = label_encoder\n",
    "\n",
    "        print(f\"ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: {sorted(y_encoded.unique())}\")\n",
    "        print(f\"ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {y_encoded.value_counts().to_dict()}\")\n",
    "        print(f\"ç·¨ç¢¼æ˜ å°„: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "        n_classes = len(label_encoder.classes_)\n",
    "        print(f\"å¯¦éš›é¡åˆ¥æ•¸: {n_classes}\")\n",
    "\n",
    "        y = y_encoded\n",
    "\n",
    "        # æ­¥é©Ÿ1ï¼šç´¢å¼•å°é½Š\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        common_idx = X.index.intersection(y.index)\n",
    "        if t1 is not None:\n",
    "            common_idx = common_idx.intersection(t1.index)\n",
    "        if sample_weight is not None:\n",
    "            common_idx = common_idx.intersection(sample_weight.index)\n",
    "\n",
    "        X = X.loc[common_idx]\n",
    "        y = y.loc[common_idx]\n",
    "        if t1 is not None:\n",
    "            t1 = t1.loc[common_idx]\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = sample_weight.loc[common_idx]\n",
    "\n",
    "        print(f\"å°é½Šå¾Œæ¨£æœ¬æ•¸: {len(X):,}\")\n",
    "        print(f\"å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {y.value_counts().to_dict()}\")\n",
    "\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.t1_train = t1\n",
    "        self.sample_weight_train = sample_weight\n",
    "        self.feature_columns = X.columns.tolist()\n",
    "\n",
    "        # æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\")\n",
    "        print(\"=\" * 60)\n",
    "        baseline_pipe = self._create_pipeline(n_classes=n_classes, **model_kwargs)\n",
    "\n",
    "        # æ­¥é©Ÿ3ï¼šPurgedKFold äº¤å‰é©—è­‰\n",
    "        if self.use_cv:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            cv = PurgedKFold(\n",
    "                n_splits=self.cv_splits,\n",
    "                t1=self.t1_train,\n",
    "                pctEmbargo=self.pct_embargo,\n",
    "            )\n",
    "            scoring = self.cv_scoring\n",
    "            print(\"äº¤å‰é©—è­‰é…ç½®:\")\n",
    "            print(f\"  K æŠ˜æ•¸: {self.cv_splits}\")\n",
    "            print(f\"  ç¦åˆ¶æ¯”ä¾‹: {self.pct_embargo:.2%}\")\n",
    "            print(f\"  è©•åˆ†æ–¹æ³•: {scoring}\")\n",
    "\n",
    "            self.cv_scores = cvScore(\n",
    "                clf=baseline_pipe,\n",
    "                X=self.X_train,\n",
    "                y=self.y_train,\n",
    "                sample_weight=self.sample_weight_train,\n",
    "                scoring=scoring,\n",
    "                cvGen=cv,\n",
    "            )\n",
    "            print(\"\\nâœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\")\n",
    "            print(f\"   åˆ†æ•¸: {self.cv_scores}\")\n",
    "\n",
    "        # æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
    "        if self.use_hyperopt:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            param_grid = self._get_param_grid(\n",
    "                use_randomized=(self.hyperopt_method == 'randomized')\n",
    "            )\n",
    "            print(f\"è¶…åƒæ•¸æœå°‹æ–¹å¼: {self.hyperopt_method}\")\n",
    "            print(f\"åƒæ•¸ç©ºé–“å¤§å°: {len(param_grid)}\")\n",
    "\n",
    "            pipe_for_search = self._create_pipeline(n_classes=n_classes, **model_kwargs)\n",
    "            self.model = clfHyperFit(\n",
    "                feat=self.X_train,\n",
    "                lbl=self.y_train,\n",
    "                t1=self.t1_train,\n",
    "                pipe_clf=pipe_for_search,\n",
    "                param_grid=param_grid,\n",
    "                cv=self.cv_splits,\n",
    "                bagging=self.bagging,\n",
    "                rndSearchIter=self.n_iter if self.hyperopt_method == 'randomized' else 0,\n",
    "                n_jobs=self.n_jobs,\n",
    "                pctEmbargo=self.pct_embargo,\n",
    "                scoring=self.cv_scoring,\n",
    "                sample_weight=(\n",
    "                    self.sample_weight_train.values\n",
    "                    if self.sample_weight_train is not None\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "            print(\"\\nâœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ4ï¼šä½¿ç”¨é è¨­è¶…åƒæ•¸è¨“ç·´\")\n",
    "            print(\"=\" * 60)\n",
    "            self.model = baseline_pipe\n",
    "            self.model.fit(\n",
    "                self.X_train,\n",
    "                self.y_train,\n",
    "                **(\n",
    "                    {}\n",
    "                    if self.sample_weight_train is None\n",
    "                    else {f\"{self.model_type}__sample_weight\": self.sample_weight_train.values}\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # æ­¥é©Ÿ5ï¼šå»ºç«‹æ¨™æº–åŒ–å™¨ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰\n",
    "        needs_scaling = self.model_type in ['lr', 'svm', 'mlp', 'knn', 'nb']\n",
    "        if needs_scaling:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.X_train)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "\n",
    "        print(\"\\nâœ… æ¨¡å‹è¨“ç·´å®Œæˆ\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        é æ¸¬\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : np.ndarray\n",
    "            é æ¸¬æ¨™ç±¤ï¼ˆç·¨ç¢¼å¾Œçš„ï¼‰\n",
    "        probabilities : np.ndarray\n",
    "            é æ¸¬æ¦‚ç‡\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆåŸ·è¡Œ fit() è¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        # é æ¸¬\n",
    "        predictions_encoded = self.model.predict(X)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        # å¦‚æœéœ€è¦ï¼Œå¯ä»¥å°‡ç·¨ç¢¼å¾Œçš„æ¨™ç±¤è½‰å›åŸå§‹æ¨™ç±¤\n",
    "        # ä½†é€šå¸¸æˆ‘å€‘ä¿æŒç·¨ç¢¼å¾Œçš„æ¨™ç±¤ï¼Œè®“èª¿ç”¨è€…è‡ªå·±è™•ç†æ˜ å°„\n",
    "        \n",
    "        return predictions_encoded, probabilities\n",
    "    \n",
    "    def evaluate(self,\n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight: Optional[pd.Series] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        è©•ä¼°æ¨¡å‹æ€§èƒ½\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            ç‰¹å¾µçŸ©é™£\n",
    "        y : pd.Series\n",
    "            çœŸå¯¦æ¨™ç±¤\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            è©•ä¼°æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import (accuracy_score, f1_score, \n",
    "                                   precision_score, recall_score,\n",
    "                                   roc_auc_score)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"è«‹å…ˆåŸ·è¡Œ fit() è¨“ç·´æ¨¡å‹\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # é æ¸¬\n",
    "        predictions, probabilities = self.predict(X)\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ¨™\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, predictions, sample_weight=sample_weight),\n",
    "            'precision': precision_score(y, predictions, average='weighted', \n",
    "                                        sample_weight=sample_weight, zero_division=0),\n",
    "            'recall': recall_score(y, predictions, average='weighted', \n",
    "                                  sample_weight=sample_weight, zero_division=0),\n",
    "            'f1': f1_score(y, predictions, average='weighted', \n",
    "                          sample_weight=sample_weight, zero_division=0),\n",
    "        }\n",
    "        \n",
    "        # å¦‚æœæ˜¯äºŒåˆ†é¡ï¼Œè¨ˆç®—æ›´å¤šæŒ‡æ¨™\n",
    "        if len(np.unique(y)) == 2:\n",
    "            metrics['f1_binary'] = f1_score(y, predictions, sample_weight=sample_weight, zero_division=0)\n",
    "            metrics['roc_auc'] = roc_auc_score(y, probabilities[:, 1], sample_weight=sample_weight)\n",
    "        \n",
    "        # æ‰“å°çµæœ\n",
    "        print(f\"\\nè©•ä¼°æŒ‡æ¨™:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # æ··æ·†çŸ©é™£\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y, predictions, sample_weight=sample_weight)\n",
    "        print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_results(self,\n",
    "                    X: pd.DataFrame = None,\n",
    "                    y: pd.Series = None,\n",
    "                    sample_weight: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        ç¹ªè£½çµæœï¼ˆäº¤å‰é©—è­‰åˆ†æ•¸ã€é æ¸¬çµæœç­‰ï¼‰\n",
    "        \n",
    "        åƒæ•¸:\n",
    "        -----------\n",
    "        X : pd.DataFrame, optional\n",
    "            ç‰¹å¾µçŸ©é™£ï¼ˆç”¨æ–¼é æ¸¬å’Œè©•ä¼°ï¼‰\n",
    "        y : pd.Series, optional\n",
    "            çœŸå¯¦æ¨™ç±¤ï¼ˆç”¨æ–¼è©•ä¼°ï¼‰\n",
    "        sample_weight : pd.Series, optional\n",
    "            æ¨£æœ¬æ¬Šé‡\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. äº¤å‰é©—è­‰åˆ†æ•¸å°æ¯”\n",
    "        if self.cv_scores is not None and self.best_cv_scores is not None:\n",
    "            axes[0, 0].bar(range(len(self.cv_scores)), self.cv_scores, alpha=0.6, \n",
    "                          label='Baseline Model', color='blue', width=0.4)\n",
    "            axes[0, 0].bar([i + 0.4 for i in range(len(self.best_cv_scores))], \n",
    "                          self.best_cv_scores, alpha=0.6, \n",
    "                          label='Best Model (After Tuning)', color='red', width=0.4)\n",
    "            axes[0, 0].axhline(self.cv_scores.mean(), color='blue', linestyle='--', \n",
    "                             label=f'Baseline Mean: {self.cv_scores.mean():.4f}', linewidth=1.5)\n",
    "            axes[0, 0].axhline(self.best_cv_scores.mean(), color='red', linestyle='--', \n",
    "                             label=f'Best Mean: {self.best_cv_scores.mean():.4f}', linewidth=1.5)\n",
    "            axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "            axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores: Baseline vs Best Model', fontsize=14)\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_xticks(range(len(self.cv_scores)))\n",
    "            axes[0, 0].set_xticklabels([f'Fold {i+1}' for i in range(len(self.cv_scores))])\n",
    "        elif self.cv_scores is not None:\n",
    "            axes[0, 0].bar(range(len(self.cv_scores)), self.cv_scores, alpha=0.7, color='blue')\n",
    "            axes[0, 0].axhline(self.cv_scores.mean(), color='red', linestyle='--', \n",
    "                             label=f'Mean: {self.cv_scores.mean():.4f}', linewidth=2)\n",
    "            axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "            axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores', fontsize=14)\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, 'No CV scores available', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 0].set_title('Cross-Validation Scores', fontsize=14)\n",
    "        \n",
    "        # 2. åˆ†æ•¸åˆ†å¸ƒå°æ¯”\n",
    "        if self.cv_scores is not None and self.best_cv_scores is not None:\n",
    "            axes[0, 1].boxplot([self.cv_scores, self.best_cv_scores], \n",
    "                             labels=['Baseline', 'Best Model'])\n",
    "            axes[0, 1].scatter([1] * len(self.cv_scores), self.cv_scores, \n",
    "                             alpha=0.3, s=30, color='blue')\n",
    "            axes[0, 1].scatter([2] * len(self.best_cv_scores), self.best_cv_scores, \n",
    "                             alpha=0.3, s=30, color='red')\n",
    "            axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution Comparison', fontsize=14)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        elif self.cv_scores is not None:\n",
    "            axes[0, 1].boxplot([self.cv_scores], labels=['Baseline'])\n",
    "            axes[0, 1].scatter([1] * len(self.cv_scores), self.cv_scores, \n",
    "                             alpha=0.3, s=30, color='blue')\n",
    "            axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution', fontsize=14)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No CV scores available', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[0, 1].set_title('Score Distribution', fontsize=14)\n",
    "        \n",
    "        # 3. é æ¸¬çµæœï¼ˆå¦‚æœæœ‰ X å’Œ yï¼‰\n",
    "        if X is not None and y is not None:\n",
    "            predictions, probabilities = self.predict(X)\n",
    "            \n",
    "            # æ··æ·†çŸ©é™£\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(y, predictions, sample_weight=sample_weight)\n",
    "            sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 0])  # ä½¿ç”¨æµ®é»æ•¸æ ¼å¼\n",
    "            axes[1, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "            axes[1, 0].set_ylabel('True Label', fontsize=12)\n",
    "            axes[1, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "            \n",
    "            # é æ¸¬åˆ†å¸ƒ\n",
    "            pred_counts = pd.Series(np.ravel(predictions)).value_counts().sort_index()\n",
    "            axes[1, 1].bar(pred_counts.index, pred_counts.values, alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('Predicted Label', fontsize=12)\n",
    "            axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "            axes[1, 1].set_title('Prediction Distribution', fontsize=14)\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No prediction data\\n(provide X and y)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 0].set_title('Confusion Matrix', fontsize=14)\n",
    "            axes[1, 1].text(0.5, 0.5, 'No prediction data\\n(provide X and y)', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[1, 1].set_title('Prediction Distribution', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8810024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹åŸ·è¡Œ MetaNewLow è‡ªå‹•åŒ–é æ¸¬...\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ AUS200 (MetaNewHigh / L1)\n",
      "============================================================\n",
      "ğŸ“¥ æˆåŠŸè¼‰å…¥å…¨å¸‚å ´ç‰¹å¾µ: 231115 ç­†\n",
      "ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸: 77\n",
      "\n",
      "============================================================\n",
      "ğŸ§¹ æ•¸æ“šæ¸…ç†...\n",
      "============================================================\n",
      "\n",
      "âœ… æ¸…ç†å®Œæˆ\n",
      "   æœ€çµ‚ç‰¹å¾µæ•¸: 77\n",
      "\n",
      "============================================================\n",
      "ğŸ”— æ•¸æ“šå°é½Š...\n",
      "============================================================\n",
      "============================================================\n",
      "ğŸ“Š æ¨£æœ¬æ¬Šé‡è¨ˆç®—é…ç½®\n",
      "============================================================\n",
      "ä½¿ç”¨å”¯ä¸€æ€§: True\n",
      "ä½¿ç”¨å ±é…¬æ­¸å› : False\n",
      "ä½¿ç”¨æ™‚é–“è¡°æ¸›: True\n",
      "ä½¿ç”¨é¡åˆ¥å¹³è¡¡: False\n",
      "æ™‚é–“è¡°æ¸›é¡å‹: linear\n",
      "============================================================\n",
      "============================================================\n",
      "ğŸ“Š ä¸¦ç™¼åº¦çµ±è¨ˆ\n",
      "============================================================\n",
      "ç¸½æ™‚é–“é»æ•¸: 221,764\n",
      "å¹³å‡ä¸¦ç™¼äº‹ä»¶æ•¸: 0.01\n",
      "æœ€å¤§ä¸¦ç™¼äº‹ä»¶æ•¸: 6\n",
      "\n",
      "èªªæ˜:\n",
      "  - ä¸¦ç™¼åº¦è¡¡é‡æ¯å€‹æ™‚é–“é»æœ‰å¤šå°‘å€‹æ¨™ç±¤åŒæ™‚å­˜æ´»\n",
      "  - é«˜ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ä¹‹é–“é‡ç–Šå¤šï¼Œè³‡è¨Šå†—é¤˜\n",
      "  - ä½ä¸¦ç™¼åº¦è¡¨ç¤ºæ¨™ç±¤ç›¸å°ç¨ç«‹\n",
      "ä¸¦ç™¼åº¦åˆ†å¸ƒ:\n",
      "count    221764.000000\n",
      "mean          0.013221\n",
      "std           0.134993\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           6.000000\n",
      "dtype: float64\n",
      "============================================================\n",
      "============================================================\n",
      "ğŸ“Š å”¯ä¸€æ€§çµ±è¨ˆ\n",
      "============================================================\n",
      "ç¸½äº‹ä»¶æ•¸: 1,260\n",
      "å¹³å‡å”¯ä¸€æ€§: 0.8153\n",
      "\n",
      "èªªæ˜:\n",
      "  - å”¯ä¸€æ€§è¡¡é‡æ¯å€‹æ¨™ç±¤çš„éé‡ç–Šç¨‹åº¦\n",
      "  - å”¯ä¸€æ€§ = 1 / å¹³å‡ä¸¦ç™¼åº¦\n",
      "  - å”¯ä¸€æ€§è¶Šé«˜ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤è¶Šç¨ç«‹ï¼Œæ¬Šé‡æ‡‰è©²è¶Šå¤§\n",
      "  - å”¯ä¸€æ€§è¶Šä½ï¼Œè¡¨ç¤ºè©²æ¨™ç±¤èˆ‡å…¶ä»–æ¨™ç±¤é‡ç–Šå¤šï¼Œæ¬Šé‡æ‡‰è©²è¶Šå°\n",
      "å”¯ä¸€æ€§åˆ†å¸ƒ:\n",
      "count    1260.000000\n",
      "mean        0.815338\n",
      "std         0.220004\n",
      "min         0.181818\n",
      "25%         0.666667\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "dtype: float64\n",
      "============================================================\n",
      "============================================================\n",
      "ğŸ“Š ç·šæ€§æ™‚é–“è¡°æ¸›\n",
      "============================================================\n",
      "æ™‚é–“è¡°æ¸›åƒæ•¸: const=1.0000, slope=0.000000\n",
      "æœ€èˆŠæ¨£æœ¬æ¬Šé‡ (c): 1.0000\n",
      "æœ€æ–°æ¨£æœ¬æ¬Šé‡: 1.0\n",
      "å¹³å‡æ™‚é–“è¡°æ¸›æ¬Šé‡: 1.0000\n",
      "\n",
      "èªªæ˜:\n",
      "  - c = 1: ç„¡æ™‚é–“è¡°æ¸›\n",
      "  - è¡°æ¸›åŸºæ–¼ç´¯ç©å”¯ä¸€æ€§ï¼Œè€Œéæ™‚é–“é †åº\n",
      "============================================================\n",
      "âš ï¸ æœªä½¿ç”¨é¡åˆ¥å¹³è¡¡ï¼Œè¨­ç‚º 1.0\n",
      "============================================================\n",
      "ğŸ“Š æ¨£æœ¬æ¬Šé‡çµ±è¨ˆï¼ˆæœ€çµ‚ï¼‰\n",
      "============================================================\n",
      "ç¸½æ¨£æœ¬æ•¸: 1,260\n",
      "å¹³å‡æ¬Šé‡: 0.8153\n",
      "æ¬Šé‡åˆ†å¸ƒ:\n",
      "count    1260.000000\n",
      "mean        0.815338\n",
      "std         0.220004\n",
      "min         0.181818\n",
      "25%         0.666667\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "dtype: float64\n",
      "============================================================\n",
      "âœ… å°é½Šå®Œæˆ\n",
      "   X: 1,260 å€‹æ¨£æœ¬, 77 å€‹ç‰¹å¾µ\n",
      "   y: 1,260 å€‹æ¨£æœ¬\n",
      "\n",
      "============================================================\n",
      "ğŸ” ç‰¹å¾µç¯©é¸ Pipeline\n",
      "============================================================\n",
      "âš ï¸ åªä½¿ç”¨ test_start (2019-01-01) ä¹‹å‰çš„æ•¸æ“šé€²è¡Œç¯©é¸\n",
      "\n",
      "ç¯©é¸ç”¨è¨“ç·´é›†:\n",
      "   æ¨£æœ¬æ•¸: 446\n",
      "   ç‰¹å¾µæ•¸: 77\n",
      "   æ™‚é–“ç¯„åœ: 2014-11-27 23:00:00 è‡³ 2018-12-27 13:00:00\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ é–‹å§‹åŸ·è¡Œç‰¹å¾µç¯©é¸ Pipeline\n",
      "================================================================================\n",
      "åŸå§‹ç‰¹å¾µæ•¸: 77\n",
      "åŸå§‹æ¨£æœ¬æ•¸: 446\n",
      "\n",
      "============================================================\n",
      "1ï¸âƒ£ Correlation Filtering\n",
      "============================================================\n",
      "   åŸå§‹ç‰¹å¾µæ•¸: 77\n",
      "   ç§»é™¤é«˜åº¦ç›¸é—œç‰¹å¾µ: 16\n",
      "   å‰©é¤˜ç‰¹å¾µæ•¸: 61\n",
      "   ç›¸é—œæ€§é–¾å€¼: 0.95\n",
      "\n",
      "============================================================\n",
      "2ï¸âƒ£ Variance Threshold\n",
      "============================================================\n",
      "   åŸå§‹ç‰¹å¾µæ•¸: 61\n",
      "   ç§»é™¤ä½è®Šç•°ç‰¹å¾µ: 11\n",
      "   å‰©é¤˜ç‰¹å¾µæ•¸: 50\n",
      "   è®Šç•°æ•¸é–¾å€¼: 0.005\n",
      "\n",
      "============================================================\n",
      "3ï¸âƒ£ IC (Information Coefficient) Ranking\n",
      "============================================================\n",
      "   åŸå§‹ç‰¹å¾µæ•¸: 50\n",
      "   ç§»é™¤ä½ |IC| ç‰¹å¾µ: 4\n",
      "   å‰©é¤˜ç‰¹å¾µæ•¸: 46\n",
      "   IC é–¾å€¼: |IC| >= 0.01\n",
      "\n",
      "   IC çµ±è¨ˆ:\n",
      "     å¹³å‡ IC: -0.0090\n",
      "     å¹³å‡ |IC|: 0.0378\n",
      "     ä¸­ä½æ•¸ |IC|: 0.0345\n",
      "     æœ€å¤§ IC: 0.0915\n",
      "     æœ€å° IC: -0.0941\n",
      "     æœ€å¤§ |IC|: 0.0941\n",
      "\n",
      "   IC åˆ†å¸ƒ:\n",
      "     æ­£ IC ç‰¹å¾µæ•¸: 18 (å¹³å‡: 0.0399)\n",
      "     è²  IC ç‰¹å¾µæ•¸: 32 (å¹³å‡: -0.0366)\n",
      "     é›¶ IC ç‰¹å¾µæ•¸: 0\n",
      "\n",
      "   Top 10 ç¯©é¸å¾Œç‰¹å¾µ (æŒ‰ |IC| æ’åº):\n",
      "      1. ESP35_d0.5: |IC|=0.0941 (IC=-0.0941, è² ç›¸é—œ)\n",
      "      2. NATR_L: |IC|=0.0915 (IC=0.0915, æ­£ç›¸é—œ)\n",
      "      3. XAUUSD_d1.0: |IC|=0.0863 (IC=0.0863, æ­£ç›¸é—œ)\n",
      "      4. AUS200_d0.75: |IC|=0.0790 (IC=0.0790, æ­£ç›¸é—œ)\n",
      "      5. ATR_SS: |IC|=0.0774 (IC=0.0774, æ­£ç›¸é—œ)\n",
      "      6. US2000_d0.5: |IC|=0.0723 (IC=-0.0723, è² ç›¸é—œ)\n",
      "      7. ESP35_d0.75: |IC|=0.0609 (IC=-0.0609, è² ç›¸é—œ)\n",
      "      8. MFI_SS: |IC|=0.0601 (IC=0.0601, æ­£ç›¸é—œ)\n",
      "      9. GER30_d0.5: |IC|=0.0581 (IC=-0.0581, è² ç›¸é—œ)\n",
      "     10. SPX500_d1.0: |IC|=0.0581 (IC=-0.0581, è² ç›¸é—œ)\n",
      "\n",
      "============================================================\n",
      "4ï¸âƒ£ ANOVA / t-test / Ï‡Â² Statistical Tests\n",
      "============================================================\n",
      "   åŸå§‹ç‰¹å¾µæ•¸: 46\n",
      "   ç§»é™¤é«˜ p-value ç‰¹å¾µ: 40\n",
      "   å‰©é¤˜ç‰¹å¾µæ•¸: 6\n",
      "   p-value é–¾å€¼: 0.2\n",
      "\n",
      "   Top 10 æœ€é¡¯è‘—ç‰¹å¾µ:\n",
      "      1. ESP35_d0.5: p=0.028129\n",
      "      2. AUS200_d0.75: p=0.055311\n",
      "      3. ATR_SS: p=0.087700\n",
      "      4. NATR_L: p=0.113249\n",
      "      5. Volume_Ratio_20: p=0.175420\n",
      "      6. MFI_SS: p=0.193642\n",
      "\n",
      "============================================================\n",
      "5ï¸âƒ£ MDA (Permutation Importance)\n",
      "============================================================\n",
      "   è¨“ç·´ Random Forest æ¨¡å‹...\n",
      "   Baseline Score: 0.7587\n",
      "   è¨ˆç®— Permutation Importance...\n",
      "   åŸå§‹ç‰¹å¾µæ•¸: 6\n",
      "   ç§»é™¤ä½é‡è¦æ€§ç‰¹å¾µ: 0\n",
      "   å‰©é¤˜ç‰¹å¾µæ•¸: 6\n",
      "\n",
      "   Top 10 æœ€é‡è¦ç‰¹å¾µ:\n",
      "      1. AUS200_d0.75: 0.080990\n",
      "      2. MFI_SS: 0.068564\n",
      "      3. ESP35_d0.5: 0.041895\n",
      "      4. NATR_L: 0.034415\n",
      "      5. Volume_Ratio_20: 0.033076\n",
      "      6. ATR_SS: 0.030790\n",
      "\n",
      "================================================================================\n",
      "âœ… ç‰¹å¾µç¯©é¸å®Œæˆ\n",
      "================================================================================\n",
      "æœ€çµ‚ç‰¹å¾µæ•¸: 6\n",
      "ç‰¹å¾µæ¸›å°‘æ¯”ä¾‹: 92.21%\n",
      "\n",
      "âœ… æœ€çµ‚ç‰¹å¾µæ•¸: 6 (å«å…¨å¸‚å ´ç‰¹å¾µ + ç¯©é¸å¾Œ)\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (RF)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 446\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.35874865 0.32137126 0.52922814 0.46059213 0.5139231 ]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— rf æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (RF)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 486\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30016402 0.33620486 0.51944977 0.50891686 0.55619911]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— rf æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (RF)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 781\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30887245 0.52245649 0.53688519 0.53087603 0.48023976]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— rf æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (RF)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 935\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.32378995 0.49016514 0.49301818 0.48186596 0.5611984 ]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— rf æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (RF)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,142\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.43249175 0.56750142 0.53451456 0.48875849 0.50965683]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— rf æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (RF)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,170\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.39926274 0.50314197 0.52301473 0.45882821 0.46599428]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— rf æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (RF)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,224\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.4012667  0.4958204  0.51734643 0.48487041 0.48092532]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— rf æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "   ğŸ’¾ rf: 814 rows saved\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (LIGHTGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 446\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30582026 0.32137126 0.47069012 0.52865957 0.51345251]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— lightgbm æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (LIGHTGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 486\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30016402 0.37360042 0.53768342 0.49787397 0.52621373]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— lightgbm æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (LIGHTGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 781\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30887245 0.40607452 0.49976738 0.49243383 0.44197675]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— lightgbm æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (LIGHTGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 935\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.32575669 0.44864517 0.55555139 0.54996329 0.55869883]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— lightgbm æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (LIGHTGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,142\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.38344933 0.50821443 0.50864091 0.49640298 0.49456136]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— lightgbm æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (LIGHTGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,170\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.40291236 0.51797742 0.52110744 0.49964504 0.47919763]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— lightgbm æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (LIGHTGBM)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,224\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.43751715 0.51572755 0.51306305 0.52205278 0.48549676]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— lightgbm æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 5\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "   ğŸ’¾ lightgbm: 814 rows saved\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (NB)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 446\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.35874865 0.32137126 0.5288247  0.46224906 0.45497648]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— nb æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 1\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (NB)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 486\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.36350075 0.3815714  0.47672183 0.4983436  0.4713747 ]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— nb æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 1\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (NB)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 781\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30887245 0.44834302 0.37820501 0.51724454 0.387145  ]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— nb æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 1\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (NB)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 935\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.3630139  0.40497814 0.517613   0.4885255  0.39433778]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— nb æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 1\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (NB)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,142\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.46455257 0.37468235 0.50467788 0.39373193 0.34235581]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— nb æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 1\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (NB)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,170\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.47637829 0.37065461 0.52721862 0.41985569 0.34878482]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— nb æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 1\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (NB)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,224\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.49968851 0.38482403 0.50850597 0.42674739 0.34158874]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— nb æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 1\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "   ğŸ’¾ nb: 814 rows saved\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (ET)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 446\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.35874865 0.32137126 0.49301206 0.5207553  0.48971254]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— et æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (ET)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 486\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.36350075 0.33374836 0.53828481 0.47009605 0.55874719]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— et æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (ET)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 781\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30887245 0.45530186 0.49768144 0.50155509 0.52722692]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— et æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (ET)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 935\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.33498383 0.53894166 0.4763622  0.55018913 0.51833289]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— et æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (ET)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,142\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.40527669 0.52906351 0.55121129 0.49972809 0.54534646]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— et æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (ET)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,170\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.39647964 0.54288421 0.51333215 0.53527934 0.49762544]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— et æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (ET)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,224\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.44168489 0.53953524 0.5860767  0.47186667 0.48436498]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— et æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "   ğŸ’¾ et: 814 rows saved\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (DT)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 446\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.35874865 0.32137126 0.49589659 0.49054454 0.49283725]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— dt æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (DT)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 486\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 255, 1: 231}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.36350075 0.31545405 0.51585276 0.50357485 0.51048095]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— dt æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (DT)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 781\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 416, 1: 365}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30887245 0.48864438 0.46377685 0.54800591 0.53802515]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— dt æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (DT)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 935\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 499, 1: 436}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.32575669 0.57544259 0.542125   0.50670192 0.51616149]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— dt æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (DT)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,142\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 601, 1: 541}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.397359   0.53326118 0.49692122 0.50325183 0.49473479]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— dt æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (DT)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,170\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 621, 1: 549}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.39692119 0.53913787 0.47578536 0.51103498 0.48462733]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— dt æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (DT)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 1,224\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 646, 1: 578}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.4320364  0.4777325  0.54081625 0.51184504 0.49568983]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— dt æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 4\n",
      "\n",
      "âœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\n",
      "\n",
      "âœ… æ¨¡å‹è¨“ç·´å®Œæˆ\n",
      "   ğŸ’¾ dt: 814 rows saved\n",
      "============================================================\n",
      "ğŸš€ PredictionPipeline é–‹å§‹è¨“ç·´ (XGBOOST)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ0ï¼šé‡æ–°ç·¨ç¢¼æ¨™ç±¤\n",
      "============================================================\n",
      "åŸå§‹æ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "åŸå§‹æ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤å”¯ä¸€å€¼: [0, 1]\n",
      "ç·¨ç¢¼å¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "ç·¨ç¢¼æ˜ å°„: {0: 0, 1: 1}\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 2\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ1ï¼šæ•¸æ“šå°é½Šèˆ‡é©—è­‰\n",
      "============================================================\n",
      "å°é½Šå¾Œæ¨£æœ¬æ•¸: 446\n",
      "å°é½Šå¾Œæ¨™ç±¤åˆ†å¸ƒ: {0: 233, 1: 213}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ2ï¼šå»ºç«‹åŸºæº–æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ3ï¼šåŸ·è¡Œ PurgedKFold äº¤å‰é©—è­‰\n",
      "============================================================\n",
      "äº¤å‰é©—è­‰é…ç½®:\n",
      "  K æŠ˜æ•¸: 5\n",
      "  ç¦åˆ¶æ¯”ä¾‹: 1.00%\n",
      "  è©•åˆ†æ–¹æ³•: roc_auc\n",
      "\n",
      "âœ… åŸºæº–æ¨¡å‹äº¤å‰é©—è­‰å®Œæˆ\n",
      "   åˆ†æ•¸: [0.30582026 0.32137126 0.48615734 0.48695145 0.55298343]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ­¥é©Ÿ4ï¼šè¶…åƒæ•¸æœå°‹\n",
      "============================================================\n",
      "ğŸš€ å–å¾— xgboost æ¨¡å‹åƒæ•¸ç¶²æ ¼\n",
      "è¶…åƒæ•¸æœå°‹æ–¹å¼: grid\n",
      "åƒæ•¸ç©ºé–“å¤§å°: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 330\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m ALL_MODELS:\n\u001b[1;32m--> 330\u001b[0m         \u001b[43mrun_walk_forward_prediction_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 281\u001b[0m, in \u001b[0;36mrun_walk_forward_prediction_binary\u001b[1;34m(comm, model_name, data)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# ç¢ºä¿ PredictionPipeline å·²å®šç¾©\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m PredictionPipeline(\n\u001b[0;32m    270\u001b[0m         model_type\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    271\u001b[0m         cv_scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    279\u001b[0m     )\n\u001b[1;32m--> 281\u001b[0m     \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msw_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     preds_mapped, probs \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    284\u001b[0m     preds_original \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(np\u001b[38;5;241m.\u001b[39mravel(preds_mapped), index\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mmap(rev)\n",
      "Cell \u001b[1;32mIn[9], line 593\u001b[0m, in \u001b[0;36mPredictionPipeline.fit\u001b[1;34m(self, X, y, t1, sample_weight, **model_kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124måƒæ•¸ç©ºé–“å¤§å°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(param_grid)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    592\u001b[0m     pipe_for_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_pipeline(n_classes\u001b[38;5;241m=\u001b[39mn_classes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mclfHyperFit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlbl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt1_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipe_clf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe_for_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbagging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbagging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrndSearchIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperopt_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandomized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpctEmbargo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpct_embargo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_scoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_weight_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_weight_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    608\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… è¶…åƒæ•¸æœå°‹å®Œæˆ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[8], line 198\u001b[0m, in \u001b[0;36mclfHyperFit\u001b[1;34m(feat, lbl, t1, pipe_clf, param_grid, cv, bagging, rndSearchIter, n_jobs, pctEmbargo, scoring, **fit_params)\u001b[0m\n\u001b[0;32m    188\u001b[0m     gs \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    189\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mpipe_clf,\n\u001b[0;32m    190\u001b[0m         param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39mrndSearchIter\n\u001b[0;32m    195\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# â­ ä½¿ç”¨æ¸…ç†å¾Œçš„ fit_paramsï¼ˆä¸åŒ…å« scoringï¼‰\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_clean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# 2) åœ¨å…¨éƒ¨æ•¸æ“šä¸Šæ“¬åˆé©—è­‰å¾Œçš„æ¨¡å‹\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bagging[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m bagging[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1605\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    994\u001b[0m         )\n\u001b[0;32m    995\u001b[0m     )\n\u001b[1;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1587\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1698\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1699\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ========================================================================================================\n",
    "# ğŸš€ MetaNewLow å…¨å•†å“è‡ªå‹•åŒ–é æ¸¬æµç¨‹ (äºŒå…ƒåˆ†é¡ + å®Œæ•´æ¬„ä½ + FracDiff å…¨å¸‚å ´ç‰¹å¾µ)\n",
    "# ========================================================================================================\n",
    "# é©ç”¨ç­–ç•¥ï¼šS1Strategy (æ–°ä½çªç ´)\n",
    "# æ¨™ç±¤å®šç¾©ï¼šClass 0 (Hit_SL), Class 1 (No_SL)\n",
    "# ç‰¹å¾µä¾†æºï¼šå€‹åˆ¥å•†å“ç‰¹å¾µ + å…¨å¸‚å ´æ—¥é »ç‰¹å¾µ\n",
    "# ========================================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import List, Optional\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å…¨å±€é…ç½®\n",
    "TRAIN_START = '2014-11-01'\n",
    "TEST_HORIZON_START_YEAR = 2019\n",
    "ROLLING_YEARS = 7\n",
    "\n",
    "ALL_COMMODITIES = [\"AUS200\", \"NAS100\", \"ESP35\", \"FRA40\", \"GER30\", \"HKG33\", \"JPN225\", \"SPX500\", \"US2000\"]\n",
    "ALL_MODELS = ['rf', 'lightgbm', 'nb', 'et', 'dt', 'xgboost', 'catboost', 'svm', 'lr']\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\a124a\\OneDrive\\æ¡Œé¢\\ç­–ç•¥é–‹ç™¼\\MetaLabelLearning\\MetaNewLow\")\n",
    "RAW_DATA_DIR = BASE_DIR / \"equity_prices\"\n",
    "FEATURE_DIR = Path(r\"C:\\Users\\a124a\\OneDrive\\æ¡Œé¢\\ç­–ç•¥é–‹ç™¼\\MetaLabelLearning\\MetaNewLow\\feature_data\")\n",
    "RESULTS_DIR = BASE_DIR / \"pred_results_binary\"\n",
    "\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# S1Strategy (æ–°ä½)\n",
    "# -------------------------------------------------------------------\n",
    "class S1Strategy:\n",
    "    def __init__(self, entry_param: float = 1.0, rolling_window: int = 96):\n",
    "        self.entry_param = entry_param\n",
    "        self.rolling_window = rolling_window\n",
    "        self.signals = None\n",
    "        \n",
    "    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df['HL115'] = (\n",
    "            df['High'].shift(1).rolling(self.rolling_window, min_periods=self.rolling_window).max() - \n",
    "            df['Low'].shift(1).rolling(self.rolling_window, min_periods=self.rolling_window).min()\n",
    "        )\n",
    "        df['çªç ´signalåƒ¹'] = (\n",
    "            df['High'].shift(1).rolling(self.rolling_window).max() - \n",
    "            (self.entry_param * df['HL115'])\n",
    "        )\n",
    "        df['signal'] = False\n",
    "        df['signal'] = (\n",
    "            (df['signal'].shift(1) == False) & \n",
    "            (df['Low'] < df['çªç ´signalåƒ¹']) & \n",
    "            (df['Open'] > df['çªç ´signalåƒ¹'])\n",
    "        )\n",
    "        df['side'] = np.nan\n",
    "        df.loc[df['signal'] == True, 'side'] = -1.0\n",
    "        df['side'] = df['side'].ffill()\n",
    "        self.signals = df\n",
    "        return df\n",
    "    \n",
    "    def get_signal_events(self) -> pd.DatetimeIndex:\n",
    "        if self.signals is None: return None\n",
    "        return self.signals[self.signals['signal'] == True].index\n",
    "\n",
    "# (NoiseFilter, MetaLabeling, SampleWeight, PredictionPipeline å‡è¨­å·²å®šç¾©)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# è³‡æ–™æº–å‚™ (å«å…¨å¸‚å ´ç‰¹å¾µåˆä½µ)\n",
    "# -------------------------------------------------------------------\n",
    "def prepare_commodity_data_binary(comm):\n",
    "    print(f\"\\n{'='*60}\\nğŸ“¦ {comm} (MetaNewHigh / L1)\\n{'='*60}\")\n",
    "    \n",
    "    # 1. è¼‰å…¥åƒ¹æ ¼\n",
    "    p = RAW_DATA_DIR / f\"{comm}.csv\"\n",
    "    if not p.exists(): \n",
    "        print(f\"âŒ æ‰¾ä¸åˆ°åƒ¹æ ¼æª”: {p}\")\n",
    "        return None\n",
    "    try: df = pd.read_csv(p)\n",
    "    except: return None\n",
    "    \n",
    "    d_col = next((c for c in df.columns if c.lower() in ['date','time','timestamp']), None)\n",
    "    if not d_col: return None\n",
    "    df[d_col] = pd.to_datetime(df[d_col])\n",
    "    df = df.sort_values(by=d_col).set_index(d_col)\n",
    "    \n",
    "    r_map = {'bidopen':'Open', 'open':'Open', 'bidhigh':'High', 'high':'High', \n",
    "             'bidlow':'Low', 'low':'Low', 'bidclose':'Close', 'close':'Close', 'volume':'Volume'}\n",
    "    df = df.rename(columns={c: r_map.get(c.lower(), c) for c in df.columns})\n",
    "    df = df.dropna().loc[df.index >= TRAIN_START]\n",
    "    \n",
    "    # âœ… ç§»é™¤ CUSUM éæ¿¾,ç›´æ¥ä½¿ç”¨åŸå§‹æ•¸æ“š\n",
    "    # è¨ˆç®— daily volatility ç”¨æ–¼ MetaLabeling\n",
    "    close_ret = df['Close'].pct_change()\n",
    "    daily_vol = close_ret.ewm(span=100).std()\n",
    "    \n",
    "    # Step 2: Strategy (L1)\n",
    "    strategy = S1Strategy(entry_param=0.5, rolling_window=500)\n",
    "    df_with_signals = strategy.calculate_indicators(df)\n",
    "    tEvents = strategy.get_signal_events()\n",
    "    if tEvents is None or len(tEvents) == 0:\n",
    "        print(\"âš ï¸ ç„¡ç­–ç•¥è¨Šè™Ÿ\")\n",
    "        return None\n",
    "        \n",
    "    # Step 3: Meta Labeling\n",
    "    meta = MetaLabeling(ptSl=[0.25, 0.25], numPeriods=32, minRet=0.001)\n",
    "    events = meta.get_events(df['Close'], tEvents, daily_vol, df_with_signals['side'])\n",
    "    bins = meta.get_bins(df['Close'])\n",
    "    \n",
    "    # Step 5: è¼‰å…¥ç‰¹å¾µ (å€‹åˆ¥ + å…¨å¸‚å ´)\n",
    "    fp = FEATURE_DIR / f\"{comm}_features.csv\"\n",
    "    if not fp.exists(): return None\n",
    "    try: feats_individual = pd.read_csv(fp, index_col=0, parse_dates=True)\n",
    "    except: feats_individual = pd.read_csv(fp, index_col=0, parse_dates=True, engine='python')\n",
    "    feats_individual = feats_individual[~feats_individual.index.duplicated(keep='first')]\n",
    "    \n",
    "    fp_all = FEATURE_DIR / \"fracdiff_features_all_products.csv\"\n",
    "    if fp_all.exists():\n",
    "        try:\n",
    "            feats_all = pd.read_csv(fp_all, index_col=0, parse_dates=True)\n",
    "            feats_all.index = pd.to_datetime(feats_all.index)\n",
    "            print(f\"ğŸ“¥ æˆåŠŸè¼‰å…¥å…¨å¸‚å ´ç‰¹å¾µ: {len(feats_all)} ç­†\")\n",
    "            feats_all_aligned = feats_all.reindex(df.index, method='ffill')\n",
    "            X = pd.concat([feats_individual, feats_all_aligned], axis=1)\n",
    "            X = X.loc[~X.index.duplicated(keep='first')]\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¼‰å…¥å…¨å¸‚å ´ç‰¹å¾µå¤±æ•—: {e},åƒ…ä½¿ç”¨å€‹åˆ¥ç‰¹å¾µ\")\n",
    "            X = feats_individual\n",
    "    else:\n",
    "        print(f\"âš ï¸ æ‰¾ä¸åˆ°å…¨å¸‚å ´ç‰¹å¾µæª”,åƒ…ä½¿ç”¨å€‹åˆ¥ç‰¹å¾µ\")\n",
    "        X = feats_individual\n",
    "    \n",
    "    print(f\"ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸: {len(X.columns)}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # Step 6: æ•¸æ“šæ¸…ç†\n",
    "    # =====================================================\n",
    "    print(f\"\\n{'='*60}\\nğŸ§¹ æ•¸æ“šæ¸…ç†...\\n{'='*60}\")\n",
    "    \n",
    "    X = X.fillna(method='ffill').fillna(method='bfill').fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # ç§»é™¤å®Œå…¨ç‚ºå¸¸æ•¸çš„ç‰¹å¾µ\n",
    "    constant_cols = []\n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            unique_count = int(X[col].nunique())\n",
    "            if unique_count <= 1:\n",
    "                constant_cols.append(col)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    if len(constant_cols) > 0:\n",
    "        print(f\"ç§»é™¤ {len(constant_cols)} å€‹å¸¸æ•¸ç‰¹å¾µ\")\n",
    "        X = X.drop(columns=constant_cols)\n",
    "    \n",
    "    # ç§»é™¤å®Œå…¨ç‚º NaN çš„ç‰¹å¾µ\n",
    "    nan_cols = [col for col in X.columns if X[col].isna().all()]\n",
    "    if len(nan_cols) > 0:\n",
    "        print(f\"ç§»é™¤ {len(nan_cols)} å€‹å…¨ NaN ç‰¹å¾µ\")\n",
    "        X = X.drop(columns=nan_cols)\n",
    "    \n",
    "    print(f\"\\nâœ… æ¸…ç†å®Œæˆ\")\n",
    "    print(f\"   æœ€çµ‚ç‰¹å¾µæ•¸: {len(X.columns):,}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # Step 7: æ•¸æ“šå°é½Š\n",
    "    # =====================================================\n",
    "    print(f\"\\n{'='*60}\\nğŸ”— æ•¸æ“šå°é½Š...\\n{'='*60}\")\n",
    "    \n",
    "    idx = bins.index.intersection(X.index)\n",
    "    X, y = X.loc[idx], bins.loc[idx, 'bin']\n",
    "    valid = X.dropna().index.intersection(y.dropna().index)\n",
    "    X, y = X.loc[valid], y.loc[valid]\n",
    "    events_valid = events.loc[valid]\n",
    "    \n",
    "    y_map = y.map({-1: 0, 0: 1, 1: 1})\n",
    "    rev_map = {0: 'Hit_StopLoss', 1: 'No_StopLoss'}\n",
    "    \n",
    "    try:\n",
    "        sw_calc = SampleWeight()\n",
    "        sw = sw_calc.compute_sample_weights(events_valid['t1'], df['Close'].index, y_map)\n",
    "    except:\n",
    "        sw = pd.Series(1., index=y.index)\n",
    "    \n",
    "    print(f\"âœ… å°é½Šå®Œæˆ\")\n",
    "    print(f\"   X: {len(X):,} å€‹æ¨£æœ¬, {len(X.columns):,} å€‹ç‰¹å¾µ\")\n",
    "    print(f\"   y: {len(y_map):,} å€‹æ¨£æœ¬\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # Step 6.5: ç‰¹å¾µç¯©é¸ Pipeline\n",
    "    # =====================================================\n",
    "    print(f\"\\n{'='*60}\\nğŸ” ç‰¹å¾µç¯©é¸ Pipeline\\n{'='*60}\")\n",
    "    print(f\"âš ï¸ åªä½¿ç”¨ test_start ({TEST_HORIZON_START_YEAR}-01-01) ä¹‹å‰çš„æ•¸æ“šé€²è¡Œç¯©é¸\")\n",
    "    \n",
    "    # åˆ‡åˆ†è¨“ç·´é›†ç”¨æ–¼ç¯©é¸\n",
    "    test_start = f'{TEST_HORIZON_START_YEAR}-01-01'\n",
    "    train_mask = (X.index >= TRAIN_START) & (X.index < test_start)\n",
    "    X_train_filter = X.loc[train_mask].copy()\n",
    "    y_train_filter = y_map.loc[train_mask].copy()\n",
    "    sw_train_filter = sw.loc[train_mask].copy()\n",
    "    \n",
    "    print(f\"\\nç¯©é¸ç”¨è¨“ç·´é›†:\")\n",
    "    print(f\"   æ¨£æœ¬æ•¸: {len(X_train_filter):,}\")\n",
    "    print(f\"   ç‰¹å¾µæ•¸: {len(X_train_filter.columns):,}\")\n",
    "    print(f\"   æ™‚é–“ç¯„åœ: {X_train_filter.index.min()} è‡³ {X_train_filter.index.max()}\")\n",
    "    \n",
    "    # åŸ·è¡Œç¯©é¸\n",
    "    filter_pipeline = FeatureFilteringPipeline(\n",
    "        corr_threshold=0.95,\n",
    "        variance_threshold=0.005,\n",
    "        ic_threshold=0.01,\n",
    "        anova_p_threshold=0.2,\n",
    "        mda_top_n=None,\n",
    "        stability_n_iter=5,\n",
    "        stability_threshold=0.1\n",
    "    )\n",
    "    \n",
    "    X_train_filtered = filter_pipeline.fit_transform(\n",
    "        X_train_filter,\n",
    "        y_train_filter,\n",
    "        sample_weight=sw_train_filter,\n",
    "        use_stability=False\n",
    "    )\n",
    "    \n",
    "    # ä½¿ç”¨ç¯©é¸å¾Œçš„ç‰¹å¾µ\n",
    "    selected_features = filter_pipeline.selected_features\n",
    "    X = X[selected_features]\n",
    "    \n",
    "    print(f\"\\nâœ… æœ€çµ‚ç‰¹å¾µæ•¸: {len(X.columns)} (å«å…¨å¸‚å ´ç‰¹å¾µ + ç¯©é¸å¾Œ)\")\n",
    "    \n",
    "    return {\n",
    "        'X': X, 'y': y_map, 'bins': bins.loc[valid], \n",
    "        'events': events_valid, 'sw': sw, 'rev_map': rev_map\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# ğŸƒâ€â™‚ï¸ æ»¾å‹•é æ¸¬ (å…±ç”¨é‚è¼¯)\n",
    "# ===================================================================\n",
    "# (æ­¤å‡½æ•¸èˆ‡ run_binary_prediction_full_format.py ä¸­çš„å®Œå…¨ç›¸åŒï¼Œå¯ç›´æ¥å…±ç”¨)\n",
    "def run_walk_forward_prediction_binary(comm, model_name, data):\n",
    "    X, y, bins, events, sw, rev = data['X'], data['y'], data['bins'], data['events'], data['sw'], data['rev_map']\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for yr in range(TEST_HORIZON_START_YEAR, TEST_HORIZON_START_YEAR + ROLLING_YEARS):\n",
    "        tz = X.index.tz\n",
    "        s_dt = pd.Timestamp(f'{yr}-01-01', tz=tz)\n",
    "        e_dt = s_dt + pd.DateOffset(years=1)\n",
    "        \n",
    "        tr_mask = (X.index >= TRAIN_START) & (X.index < s_dt)\n",
    "        te_mask = (X.index >= s_dt) & (X.index < e_dt)\n",
    "        \n",
    "        if tr_mask.sum() < 50 or te_mask.sum() == 0: continue\n",
    "            \n",
    "        X_train, y_train = X.loc[tr_mask], y.loc[tr_mask]\n",
    "        t1_train, sw_train = events.loc[tr_mask, 't1'], sw.loc[tr_mask]\n",
    "        X_test, y_test = X.loc[te_mask], y.loc[te_mask]\n",
    "        test_events = events.loc[te_mask]\n",
    "        \n",
    "        try:\n",
    "            # ç¢ºä¿ PredictionPipeline å·²å®šç¾©\n",
    "            pipeline = PredictionPipeline(\n",
    "                model_type=model_name,\n",
    "                cv_scoring='roc_auc',\n",
    "                use_hyperopt=True,\n",
    "                use_cv=True,\n",
    "                cv_splits=5,\n",
    "                pct_embargo=0.01,\n",
    "                hyperopt_method='grid',\n",
    "                n_iter=10,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            \n",
    "            pipeline.fit(X_train, y_train, t1_train, sw_train)\n",
    "            \n",
    "            preds_mapped, probs = pipeline.predict(X_test)\n",
    "            preds_original = pd.Series(np.ravel(preds_mapped), index=X_test.index).map(rev)\n",
    "            \n",
    "            d = pd.DataFrame(index=X_test.index)\n",
    "            d['t1'] = test_events['t1']\n",
    "            d['side'] = test_events['side']\n",
    "            d['true_label'] = y_test.map(rev)\n",
    "            d['predicted_label'] = preds_original\n",
    "            \n",
    "            if probs is not None:\n",
    "                for idx, class_name in rev.items():\n",
    "                    if idx < probs.shape[1]:\n",
    "                        d[f'prob_class_{idx}'] = probs[:, idx]\n",
    "            \n",
    "            d['prediction_confidence'] = np.max(probs, axis=1) if probs is not None else 0\n",
    "            d['is_correct'] = (d['predicted_label'] == d['true_label'])\n",
    "            d['model_type'] = model_name\n",
    "            d['test_year'] = yr\n",
    "            d['train_window_start'] = X_train.index.min()\n",
    "            d['train_window_end'] = X_train.index.max()\n",
    "            \n",
    "            dfs.append(d)\n",
    "        except Exception as e: \n",
    "            print(f\"      Err {yr}: {e}\")\n",
    "            \n",
    "    if not dfs: return\n",
    "    out = pd.concat(dfs).sort_index()\n",
    "    out.index.name = 'Date'\n",
    "    \n",
    "    out_filename = f\"{comm}_pred_{model_name}_{TEST_HORIZON_START_YEAR}_{TEST_HORIZON_START_YEAR+ROLLING_YEARS-1}.csv\"\n",
    "    out_path = RESULTS_DIR / out_filename\n",
    "    out.to_csv(out_path)\n",
    "    print(f\"   ğŸ’¾ {model_name}: {len(out)} rows saved\")\n",
    "\n",
    "# ===================================================================\n",
    "# ğŸš€ ä¸»åŸ·è¡Œ\n",
    "# ===================================================================\n",
    "if 'PredictionPipeline' not in globals():\n",
    "    print(\"âš ï¸ è«‹ç¢ºä¿ PredictionPipeline Class å·²è¼‰å…¥ï¼\")\n",
    "else:\n",
    "    print(f\"ğŸš€ é–‹å§‹åŸ·è¡Œ MetaNewLow è‡ªå‹•åŒ–é æ¸¬...\")\n",
    "    for comm in ALL_COMMODITIES:\n",
    "        try:\n",
    "            data = prepare_commodity_data_binary(comm) # ä½¿ç”¨ S1 æº–å‚™å‡½æ•¸\n",
    "            if data is None: continue\n",
    "            \n",
    "            for model in ALL_MODELS:\n",
    "                run_walk_forward_prediction_binary(comm, model, data)\n",
    "                \n",
    "        except Exception as e: \n",
    "            print(f\"âŒ {comm}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102e56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86720d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
